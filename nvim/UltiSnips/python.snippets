# General
# {{{1
#O_O infoj -- for head in jupyter notebooks
#O_O sec -- for a section commented
#O_O subsec -- for a subsection commented
# }}}
# jupyter
# {{{1
#O_O progress -- for a progress bar
#O_O ignoreWarnings -- for ignoring warnigns
# }}}
# ===== python =====
# {{{1
# {{{2
# System Interaction
# shOutput -- for getting sh output
# shCommand -- for executing sh command
# }}}
# ===== strings =====
# {{{2
# forString -- for loop a string
# doubleFor -- for for loop iterating two objects
# ins -- for incluising a string
# }}}
# ====== data structures ====
# {{{2
# dir -- for getting information about a  var type
# heap -- help on heaps
# bit_masquing -- for using bit masquing in python
# sort -- for sorting in python
# bit_count -- for counting the number of bits an int var has
# sublist -- sublist slicing
# set 
# tuple
# }}}
# ======= files ======
# {{{2
# file_open -- for file open
# file_write -- for writting into a file
# file_append -- for appending to a file
# forFolder -- create a for folder
# recurseFolder -- recurse a folder
# fmkdir -- create a directory and parent directories
# move_file -- moving a file into other location
# copy_file -- copy a file into a new location
# relPath -- create a relative path
# }}}
# ====== object oriented programming ===== 
# {{{2
# POOHelp -- help information with python
# inheritanceHelp -- for getting help on inheritance
# }}}
# ====== exception handling =====
# {{{2
# try -- try catch generinc
# raise -- for raising a custom exception
# }}}

# }}}
# for pandas dataframe
# {{{1
# data collection
# {{{2
#O_O encode -- for encoding a string
#O_O read_csv -- for reading a pandas csv
#O_O fileEncodingProblems -- for finding the file encoding
#O_O parseDate -- for parsing a date
#O_O rename -- for renaming the dataset
#O_O pcombinecsv -- for combining multiple csv files into one
#O_O create_dataframe -- for creating a pandas dataframe
#O_O create_series -- for creating a pandas series ( list of values)
#O_O export_csv -- for exporting a solution csv file

# }}}
# Data cleaning
# {{{2
#O_O Nan -- for managing Null values or Not a Number values
#O_O dataDrop -- for dropping a column of data
#O_O fillWithNextToItVal -- for fill NaN values with the value that comes directly after it
#O_O convertType -- for converting a data type of a column pandas
#O_O datatype -- for getting the data type of a column
#O_O conditional_selection -- for getting a dataframe with a condition
#O_O fillWithNextToItVal -- for fill NaN values with the value that comes directly after it
#O_O contractions -- for english contractions in a dictionary
#O_O feature_imputer -- for imputing missing values in a column

# }}}
# find insights
# {{{2
#O_O fuzzyMatching -- for searching and replacing inconsistencies 
#O_O contractions -- for creating a dictionary with english common word contractions
#O_O describe -- for getting a description of the data
#O_O columns -- for getting the column names in pandas
#O_O columnDescription -- for getting the description of a column (mean, describe, etc)
#O_O csv_len -- for getting the length of the csv
#O_O csv_head -- for getting the top five elements of the csv

# }}}
# data manipulation
# {{{2
#O_O combineDataframe -- for combining a dataframe
#O_O multiFunction -- for applying multiple functions to a dataframe
#O_O sort_values -- for sorting a pandas dataframe
#O_O group_by -- for grouping by a column
#O_O applyFunction -- for applying a function to a dataframe
#O_O loc_iloc -- for help on loc iloc
#O_O get_column -- for getting the column of a dataframe
#O_O get_val -- for getting the value of the column and row
#O_O get_row -- for getting the row of the pandas dataframe
#O_O getRange_col -- for getting a range in a column
#O_O getRange_row -- for getting a range in a row
#O_O getRange_row_index -- for setting the index column for the dataframe 
#O_O assignToColumn -- for assigning a value to a column
#O_O mapFunction -- for applying a basic function to all cells in a column
#O_O applyFunction -- for applying a function to all the rows in a dataset
#O_O replace_data -- for replacing data in a column

# }}}
# preparing data
# {{{2
#O_O normalizeData -- for normalizing the given data for a machine learning algoritm
#O_O scalingData -- for scaling the needed data
# }}}
# }}}1
# == graphs
# {{{1
#O_O globalTextSize -- for setting the global text size
#O_O graphs -- simple graphs
#O_O graphsSubplots -- for subplots
#O_O igraphs -- for interactive graphs
#O_O pairplot -- for creating a pairplot
#O_O pltannotate -- for annotation description with an arrow
#O_O matplotlib -- for help on matplotlib plt
#O_O folium -- for help on folium
#O_O animation -- for animating
#O_O plotText -- for plotting text
#O_O pltFormat -- for configuring the format of the plot
#O_O palettes -- for color palettes
#O_O multipie -- for plotting multiple pie plots

# === machine learning graphs
# {{{2
#O_O plot2dgaussianNB -- for gaussian naive bayes
# }}}

# }}}1
# ========== Machine Learning ======
# {{{1
#O_O ignoreWarnings -- for ignoring warnings
#O_O install_kaggle -- for installing kaggle
#O_O import_kaggle -- for importing the necessary for machine learning plotting and other things
#O_O model_selection -- for selecting a model
# ========= pyspark =======
# {{{2
# }}}
# ========== math =========
#{{{2
#O_O solve_eq -- for solving an equation
# }}}
# ========== statistics ===
# {{{2
#O_O zscores -- for comparing two normal distributions
#O_O pfunArea -- for finding a probability below the bell curve
#O_O norm_array -- for creating a normal distribution array
# O_O ttest -- for using a ttest
# O_O norm_test -- for testing if the data came from a normal distribution
# O_O anova -- for multiple multiple independent samples
# }}}
# === machine learning
# {{{2
#O_O naive_bayes -- for naive bayes
#O_O split_validation -- for split validating a model
#O_O model_descition_tree -- for using the model descition tree
#O_O model_random_forest -- for using the random forest model
#O_O getMAE -- for getting mean absolute error
#O_O ParamBuilderSqlearn -- basic code to loop through tree sizes
#O_O xgboostClassifier -- for using the sgboost classifier
#O_O xgboostRegressor -- for using the xgboost regressor
# }}}2
# ==== feature engineering
# {{{2
#O_O feature_selection -- for selecting features
#O_O mutual_info_regression -- for creating a mutual info regression
#O_O clustering -- for clustering the information
#O_O multiOnehotencoder -- for one hot encoding multiple columns
# }}}2
#O_O == intermediate machine learning -- for more refined machine learning
# {{{2
#O_O scoreModels -- for scoring the models
#O_O imputter -- for replacing missing values with a simple regression
#O_O completeImputter -- for using imputer and specifying that the imputter was performed in a column
#O_O ordinalEncoder -- for categorical values to encode them in integer colum
#O_O oneHotEncoding -- like the above but particullarly usefull when you have a column that encapsulates mo categories
#O_O splitOneHotEncoding -- for using ordinalEncoder on columns that have a lot of unique values
#O_O pipeLine -- for using a pipieline for better performance and less bugs
#O_O crossValidation -- for using crossvalidation with a pipeline and a regressionForest
#O_O xgboost -- for using the better xgboost
#O_O dataLeackage -- for looking and dropping data leackage
# }}}
# == sql data analysis big query
# {{{2
#O_O loadBigQuery -- for loading the python big query module
#O_O BQ_query -- for executing a sql command in big query
#O_O BQ_big_queries -- for executing a big query without exceding the limit
#O_O BQ_groupBy_having_count -- for executing thouse on a database big query
#O_O BQ_orderBy -- for ordering by
#O_O BQ_dates -- for working with dates on big query
#O_O BQ_with_as -- for nested queries better readability
#O_O BQ_join -- for join in big query
# }}}2
# == advanced big query
# {{{2
#O_O BQ_left_join -- for doing the big query left join
#O_O BQ_over_clause -- for using the over clause
#O_O BQ_partition_clause -- for using the partition clause
#O_O BQ_unest -- for unesting nested data or repeated data
#O_O BQ_speed_functions -- usable functions to see the efficiency of the queries
# }}}2
# == intro to deep learning
# {{{2
#O_O model1LinearUnit -- for creating a model with one linear unit
#O_O getWeightsAndBiases -- for getting weights and biases
#O_O plotOutputOfUntrained -- for plotting the output of an untrained linear model
#O_O loss_optimizer -- for adding a loss and the optimizer
#O_O train_dl -- for training a deep learning model
#O_O plotLosstrainingData -- for plotting loss training data
#O_O animateDeepLearning -- for animating a deep learnig project
#O_O earlyStopping -- for getting an early stopping
#O_O binaryClassification -- for classifying in bynary
# }}}2
# === computer vision
# {{{2
#O_O define_pretrained_base -- for defining the pre trained base
#O_O attachHeAD -- for attaching the head
#O_O CV_train -- for training a simple computer vision algoritm
#O_O CV_plot_acc -- for plotting the binary accuracy
#O_O filterWithConvulsion -- for filtering with convulsion
#O_O relu_activation -- for activating the relu with te conv2d
#O_O CV_tfKernel -- for defining a basic tensorflow kernel
#O_O condenseWithMaximumPooling -- adding condense with maximum pooling
#O_O imageCondensePool -- for image condensing a pool
#O_O globalAveragePooling -- for global average pooling
#O_O sliding_window -- for the sliding window tecniques padding and stride
#O_O customConvents -- for defining a custom convent
#O_O dataAugmentation -- for adding features to the cv model
# }}}2
# == reinforcement learning
#O_O definingAgents -- for defining an agent for machine learning
# ==  time series
# {{{2
#O_O fitLinearRegression -- for fitting a linear regression model from a forecasting task
#O_O lagFeature -- for adding a lag feature to a pandas dataframe
#O_O moving_average_plot -- for a moving average plot
#O_O splines -- an alternative to polinomials that is better
#O_O plotSeasonability -- for plotting seasonability
#O_O seasonalFeatures -- for creating seasonal features
#O_O hibrid_forecasting -- for creating hibrid models
# }}}2
# === geoespacial analisys
# {{{2
#O_O GP_setup -- for setting up a basic geopandas instance
#O_O interactive_map -- for creating an interactive map
#O_O choropeth_i_map -- for creating a choroplet map
#O_O geoencoder_map -- for creating locations in a map
#O_O measure_distance -- for measuring a distance in a map
#O_O create_a_buffer -- for creating a buffer in geopandas
# }}}2
# == machine learning explainability
# {{{2
#O_O test_score_features -- for testing the score of the features in your model
#O_O visualize_tree_sqlearn -- for visualizing a single in sqlearn
# }}}2
# notify-send
# execute sound
# newFolder-recursive
#O_O ---------------- organization
# sec
# subsec
#O_O ------------ python bases de datos
# {{{2
#O_O sqlite3 --- for creating a connection to sqlite3
#O_O sqlite3c -- for executing a command in sqlite3j
#O_O mysql -- for creating a connection with mysql or mariadb 
#O_O mysqlc -- for executing a command in the mysql connector
#O_O mysqlq -- for getting the information from a query
#O_O rowcount -- for getting the number of rows affected after  a commit
#O_O encryption -- for encripting a password
# }}}2
# }}}1
# ======= big data ==== 
# {{{1
# === pyspark ==
# {{{2
#O_O spark -- init spark and configurations
#O_O smergeDataset -- merge two datasets with the same columsn
#O_O sread_csv -- for reading a csv file
#O_O sinfo -- for getting the info of a spark dataframe
#O_O sread_json -- for reading a json file
#O_O sfilter -- for filtering a spark dataframe
#O_O createSquema -- for creating a squema
#O_O ssql -- for creating sql queries
#O_O sdatetime -- for creating a datetime
#O_O sgroupby -- for grouping by
#O_O sapplyFunction -- for applying a function in a dataframe
# }}}
# }}}
#O_O -------------- python django
# {{{1
#O_O crateProject -- for creating a django project from scratch
#O_O migrate -- for applying models.py file. create tables
#O_O commonPackages -- for a list of common packages for django
# {{{2
# }}}2
#O_O --- settings.py ------ MainProjectFolder
# {{{2
#O_O createApp -- for creating a django app
#O_O useMysql -- for using mysql
#O_O changeLanguage -- for setting the default language in the project
#O_O addContextProcessor -- these are used for global functions inside the html files
# }}}2
#O_O --- urls.py --- MainProjectFolder
# {{{2
#O_O importUrl -- for importing the url file from an other app into the main app
#O_O imageRoute -- for using an image from a route
# }}}2
#O_O --- views.py --- MainAppFolder
# {{{2
#O_O viewsHelp -- for creating a help file
#O_O login_page -- for creating a log in page
#O_O log_out -- for logging out form a page
#O_O register -- for registering user
#O_O getSqlObject -- for getting an sql object
# }}}2
#O_O --- apps.py --- MainAppFolder
#O_O appsHelp -- for help on the apps.py file
#O_O --- urls.py -- MainAppFolder
#O_O urlsHelp -- for help in importing directories
#O_O --- forms.py --- MainAppFolder
#O_O forms_help -- for registering a user in the database
#O_O --- layouts.html -- MainAppFolder/templates/layouts/
#O_O html_template -- for using templates
#O_O --- models.py --- SecondaryAppFolder
#O_O django_createTable -- for creating a table for the app
#O_O --- views.py --- SecondaryAppFolder
#O_O loginRequired -- require login to acces a page
#O_O --- admin.py -- SecondaryAppFolder
#O_O modClassDisplay -- for modifications on the django panel search bar and many others
# ========== BasesDeDatos ======
# sec - for creatign a new section
#O_O -------- sql basic syntax
# ======== queries ========
# {{{2
#O_O showUsers -- to show all users available in the server
#O_O showAll -- for showing everything in a table
#O_O describe -- for showing the rows of a table
#O_O showCurUser -- for showing the current user
#O_O bunion -- help for creating a union query
#O_O bjoin -- help for creating a join query
# }}}2
# =========== databases ============
# {{{2
#O_O createDatabase -- for creating a database
#O_O deleteDatabase -- for deleting a database
#O_O grantAll -- to grant all privileges to user over the database
#O_O showPrivileges -- to show the privileges
#O_O helpUsers -- to get help on the users
# }}}2
# ========== table modifications ========
# {{{2
#O_O foreign -- for  a foreign key
#O_O createTable -- for creating a table interactively
#O_O deltable -- for deleting a table
#O_O helpTable -- for help on the table creation and deletion
# }}}2
# ========== data modifications =========
# {{{2
#O_O insert -- for inserting a new value into the table
#O_O insertIfne -- for inserting a value if it doesn't exist
#O_O update -- for updating the database
# }}}2
# }}}1







































# ==========================
# ========== global functions ======
# ==========================
global !p
def create_matrix_placeholders(snip):
	# Create anonymous snippet body
	anon_snippet_body = ""
	# Get start and end line number of expanded snippet
	start = snip.snippet_start[0]
	end = snip.snippet_end[0]
  # Append current line into anonymous snippet
	for i in range(start, end + 1):
		anon_snippet_body += snip.buffer[i]
		anon_snippet_body += "" if i == end else "\n"
	# Delete expanded snippet line till second to last line
	for i in range(start, end):
		del snip.buffer[start]
	# Empty last expanded snippet line while preserving the line
	snip.buffer[start] = ''
	# Expand anonymous snippet
	snip.expand_anon(anon_snippet_body)
endglobal
post_jump "snip"
global !p
def returnCommandSplitted(command):
	cmd_arr = command.split('_')
	return_command = ""
	for a in cmd_arr:
		return_command += a + " "
	return_command = return_command[0:len(return_command)-1]
	return return_command
endglobal

post_jump "snip"
global !p

def parseStatement(i,j,k,stri):
	prev = ''
	twoBehind=''
	ans = ""
	for ch in stri:

		if ( ch == 'k' or ch == 'i' or ch == 'j' )  and prev == '$' and twoBehind == '\\':
			ans = ans[:-2]
			ans+=ch

		elif ch == 'i' and prev == '$' and twoBehind != '\\':
			ans = ans[:-1]
			ans+=str(i)
		elif ch == 'j' and prev == '$' and twoBehind != '\\':
			ans = ans[:-1]
			ans+=str(j)
		elif ch == 'k' and prev == '$' and twoBehind != '\\':
			ans = ans[:-1]
			ans+=str(k)
		elif ch == 'n' and prev == '\\':
			ans = ans[:-1]
			ans+="\n"

		else :
			ans+=ch

		twoBehind = prev
		prev = ch
	return ans

endglobal

global !p
def getMatchArr():
	cont=1
	arr = []
	while 1 :
		try :
			arr.append(match.group(cont))
			cont+=1
		except :
			break
	return arr
endglobal







# ==========================
# ========== gui's with tkinter ======
# ==========================

snippet tkinter

# snippet dropMenu "for creating a basic drop dow mwnu"
# \$3TypeLabel = Label(${4:frame},text="\$3:")
# \$3TypeLabel.pack(side=TOP)
# 
# $\{1:name of list} = [
# $\{2:"alarm",
# "alert",
# "zoom",
# "circadian-alert",
# "circadian-zoom",
# "circadian-alarm"}
# ]
# 
# $\{3:name} = tk.StringVar()
# \$3.set(\$1[0]) # default value
# 
# \$3DropMenu = OptionMenu(\$4, \$3, *\$1)
# \$3DropMenu.pack()
# 
# endsnippet
# 
# 
# # options im-> inword
# snippet figma "for automatically creating tkinter from figma"
# tkdesigner $FILE_URL $FIGMA_TOKEN
# endsnippet
# snippet s-gui
# # pack is for piling elements and grid is like a spread sheet
# root = Tk()
# \${1:frame} = Frame(root)
# \$1.\${2:pack|grid}()
# \$3
# root.mainloop()
# endsnippet
# 
# snippet gBut
# # opt parameters = fg="color",bg="color",command=lambda: function(params)
# \${1:name_of_button} = Button(\${2:frame},text="\$3").pack(side=\${4:BUTTON|TOP|LEFT|RIGHT})
# endsnippet
# 
# snippet gLab
# # opt parameters = fg="color",bg="color"
# \${1:name_of_label} = Label(\${2:frame},text="\$3").pack(side=\${4:BUTTON|TOP|LEFT|RIGHT})
# endsnippet
# 
# snippet gText
# \$1Label = Label(level_add_task,text="\$1:")
# \$1Label.pack(side=TOP)
# # start entering information
# \$1 = tk.StringVar()
# \$1TextBox = tk.Entry(level_add_task, width = 50, textvariable = \$1)
# \$1TextBox.pack(side=TOP)
# endsnippet
# 
# snippet notify-send
# os.system("notify-send \"\$1\"")
# endsnippet
# 
# # options im-> inword
# snippet gImage "for loading image into a tkinter application"
# # the render and the image variable may be lost in a function or a class,
# # so make sure you save them somewhere otherwise you will lose them
# 
# image = Image.open(\${1:path})
# hsize = int((float(image.size[1])*float(wpercent)))
# # resize the image acording to the witdh
# image = image.resize(({1:basewidth},hsize), Image.ANTIALIAS)
# 
# render = ImageTk.PhotoImage(image)
# self.img_arr.append((image,render))
# # create the tkinter label and resize
# img = Label(self.ventana,image=self.img_arr[-1][1])
# # now just pack or grid the image
# 
# endsnippet
# 
# 
# # options im-> inword
# snippet gGrid "for grid in tkinter"
# # row -- for the row position
# # column -- for the column position
# # rowspan -- for the number of rows it takes
# # columnspan -- for the number of columns it takes
# # sticky=N,S,W,E,NE,NW,SE,SW -- if the widget is smaller than the frame is says where it is going to stick to
# endsnippet
# # options im-> inword
# snippet gPack "for the pack information"
# # fill=X,Y -- to fill the entire window in the desired position
# # expand=YES -- the widget fills the space in the parent widget
# # side=BOTTOM,LEFT,RIGHT,UP -- to specify the side to be placed
# endsnippet
# # options im-> inword
# snippet gRadioButtons "for creating radio buttons"
# # \$4 = StringVar()
# \${6:rbn} = Radiobutton(
# 	\${1:ventana},
# 	text="\${2:text}",
# 	value="\${3:value}",
# 	variable=\${4:variable},
# 	command=lambda: \${5:function(params)},
# 	)
# # grid or pack
# endsnippet # options im-> inword
# snippet gCheckButtons "for check buttons"
# # \$3 = IntVar()
# \${5:cbn} = Checkbutton(
# 	\${1:ventana},
# 	text="\${2:text}",
# 	variable = \${3:variable},
# 	onvalue=1,
# 	offvalue=0,
# 	command=lambda: \${4:function(params)}
# 	)
# # grid or pack
# endsnippet
# 
# # options im-> inword
# snippet gOptionMenu "for the option menu"
# # \$3 = StringVar()
# \${1:optionMenu_name} = OptionMenu(
# 	\${2:ventana},
# 	\${3:stringVar},
# 	\${4:"opciones","separadas"}
# 	)
# # pack or grid
# endsnippet
# 
# # options im-> inword
# snippet gDeleteItem "for deleting an item "
# \${1:name}.grid_remove()
# endsnippet
# 
# # options im-> inword
# snippet gPackRemove "for deleting a pack item"
# \${2:name}.pack_forget()
# endsnippet
# 
# # options im-> inword
# snippet gMainMenu "for adding the main menu"
# \${1:name} = Menu(\${2:window})
# # name.add_command(label="",command=lamda: command(params))
# \$2.config(menu=\$1)
# endsnippet
# # options im-> inword
# snippet gShow "for showing all the pacekd widgets in window"
# \${1:window}.mainloop()
# endsnippet
# 
# # options im-> inword
# snippet gExitTkinter "for exiting the main loop"
# \${1:ventana}.exit
# endsnippet

endsnippet


# ==========================
# ========== Organization ======
# ==========================
snippet sk

# Date: `date +%d/%B/%Y\ -\ %A`
# Author: Virgilio Murillo Ochoa
# personal github: Virgilio-AI
# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
# contact: virgiliomurilloochoa1@gmail.com
# web: virgiliomurillo.com

from typing import *
import heapq as hp
from collections import deque
from collections import defaultdict
import sys

# dic = defaultdict(int)
# set = set() # .remove(val),.add(val),.discard(val),.pop(),.clear()
# dic = {} # .remove(id),dic[key] = val, dic.get(key,0)
# arr = [] # .append(val),.pop(),.remove(val),.sort(),.reverse(),.insert(pos,val),.clear()


# def input(f=open(".`!p snip.rv = snip.basename`_In1.txt")): return f.readline().rstrip() # uncomment for debugging


def main():
	$1


if __name__ == '__main__':
	main()

endsnippet

snippet infoj
# %% [markdown]
- Date: `date +%d/%B/%Y\ -\ %A`
- Author: Virgilio Murillo Ochoa
- personal github: Virgilio-AI
- linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
- contact: data_scientist@virgiliomurillo.com
- web: virgiliomurillo.com
endsnippet


snippet sec
# =============
# ==== $1 =====
# =============
endsnippet



snippet subsec
# ======== $1
endsnippet


# ==========================
# ========== jupyter ======
# ==========================
# options im-> inword
snippet progress "progress bar"
from tqdm import tqdm

for i in tqdm(range(100)):
	# do something
endsnippet

# ==========================
# ========== system interaction ======
# ==========================

snippet shOutput
${1:strinName}=str(subprocess.check_output([${2:list_commands}]).decode('utf-8'))
endsnippet

snippet shCommand
os.system('$1')
endsnippet

# ==========================
# ========== python ======
# ==========================

snippet printCommands "for pringting the commands available for a datatype or some other kind of object"
for i in dir(${1:dataType}):
	if i[0] != '_' and "${2:keyword}" in i:
		print(i)
endsnippet

snippet help "for getting help"
help($1)
endsnippet

# ========== strings ======

snippet forString
for element in range(0, len(${1:string_name})):
	print($1[element])
endsnippet


snippet doubleFor "for getting a for iterating two different objects"
for ob1,ob2 in zip(obar1,obarr2):
	print(ob1,ob2)
endsnippet


snippet ins "" im
"+str($1)+"
endsnippet

# ========== data structures ======

snippet dir "for getting information on a var type"
print(dir(rolls))
# where rolls is  a numpy.ndarray
endsnippet

snippet heap "for help on heaps"
test = [[]] * 4
# for pushing into a heap
# you can use a regular array
# hp.heappush(test[2],6)
# hp.heappop(heap)
endsnippet

snippet bit_masquing "for using bit masquing in python"
# example with 3 elements
# when you want to test 010 in the 6 position 110
# it returns true because there is a 1 in the middle both times
for i in range(1<<n):
	for j in range(n):
		if i & 1<<j:
			print("there is a one")

endsnippet

snippet sort "for sorting in python"
li = [[1,4,7],[3,6,9],[2,59,8]]
li.sort(key=lambda x: int(x[0]))
sorted(li, key = lambda x: int(x[0]))
endsnippet

snippet bit_count "for counting the number of bits an int has"
n = 12
n.bit_count()
endsnippet

snippet list
${1:temp} = [$2]
endsnippet

# options im-> inword
snippet sublist "for getting a sublist (slicing)"
# list[start:end+1:step]
endsnippet


snippet set
${1:temp} = {$2}
endsnippet


snippet tuple
${1:temp} = ($2)
endsnippet

# ========== files ======

snippet file_open

with open('${1:file_name}') as ${2:file_var}:
	for line in $2:
		print(line.rstrip())

endsnippet

snippet file_write

# from io import open
${1:name} = open("${2:file}","w")
$1.write(${2:string})
$1.close()

endsnippet
# options im-> inword
snippet file_append "for appending text into a file"
# from io import open
${1:name} = open("${2:file}","a+")
$1.write(${2:string})
$1.close()
endsnippet

# options im-> inword
snippet forFolder "iteration of files over folder"
# iterate over files in
# that directory
for filename in os.listdir(directory):
	f = os.path.join(directory, filename)
# checking if it is a file
	if os.path.isfile(f):
		print(f)
endsnippet
# options im-> inword
snippet recurseFolder "to recurse a folder iteractively"
def recurseFolder(path):
	for filename in os.listdir(directory):
		f = os.path.join(path,filename)
	if os.path.isfile(f):
		print(f)
	else:
		recurseFolder(minusOne
endsnippet
# options im-> inword
snippet fmkdir "for creating a directory and it's parent directories"
# import pathlib
pathlib.Path(${1:abs_path}).mkdir(parents=True,exist_ok=True)
endsnippet
# options im-> inword
snippet move_file "for moving a file into other location"
# import shutil
shutil.move(${1:path_original},${2:path_target})
endsnippet
# options im-> inword
snippet copy_file "for copying a file into a new location"
# import shutil
shutil.copy(${1:path_original},${2:path_target})
endsnippet

# options im-> inword
snippet relPath "create a relative path"

# import pathlib


# create the directory creating the parents
relPath = str(pathlib.Path().absolute()) + "/"

# pathlib.Path(relPath + "/holaa").mkdir(parents=True, exist_ok=True)

# abrir
ruta = relPath  +"${1:relarivePath}"
endsnippet


# ========== POO ======

snippet POOHelp "for help information on POO with python"
# create files with a single class inside of them and import them into the main file
# example:
# create the class Car inside the car.py file
# import into the main file using 
# from car import Car
#
# and use it
# car = Car()
endsnippet

snippet inheritanceHelp "how to use inheritance in python"
# class name(fatherName)
# inside the __init__(self)
# super().__init__()
endsnippet


# ========== exception handling ======
# options im-> inword
snippet try "try catch generic"
try:
	$1
except Exception as e:
	$2
fianlly:
	$3
endsnippet
# options im-> inword
snippet raise "for raising custom exception"
if $1:
	raise $2("${3:text for exception}")
endsnippet

# ==========================
# ========== pandas ======
# ==========================


snippet encode "for encoding a string"
# encode it to a different encoding, replacing characters that raise errors
after = before.encode("utf-8", errors="replace")
endsnippet



# options im-> inword
snippet fuzzyMatching "for searching and replacing inconsistencies "
# get the top 10 closest matches to "south korea"
matches = fuzzywuzzy.process.extract("south korea", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)

# take a look at them
matches
###################################
######################


# function to replace rows in the provided column of the provided dataframe
# that match the provided string above the provided ratio with the provided string
def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):
# get a list of unique strings
	strings = df[column].unique()

# get the top 10 closest matches to our input string
matches = fuzzywuzzy.process.extract(string_to_match, strings, 
		limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)

# only get matches with a ratio > 90
	close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]

# get the rows of all the close matches in our dataframe
rows_with_matches = df[column].isin(close_matches)

# replace all rows with close matches with the input matches 
	df.loc[rows_with_matches, column] = string_to_match

# let us know the function's done
	print("All done!")

# use the function we just wrote to replace close matches to "south korea" with "south korea"
replace_matches_in_column(df=professors, column='Country', string_to_match="south korea")

endsnippet

snippet fileEncodingProblems "for finding the file encoding"
# get the byte size of the file to predict the needed file encoding
def getSize(fileobject):
	fileobject.seek(0,2) # move the cursor to the end of the file
	size = fileobject.tell()
	return size

file = open('../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv', 'rb')
print( getSize(file))


# now replace with the number of bytes needed
with open("../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv", 'rb') as rawdata:
	result = chardet.detect(rawdata.read(${1:number_of_bytes}))

# check what the character encoding might be
print(result)

# now replace the needed file encoding
police_killings = pd.read_csv("../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv",encoding='Windows-1252')
endsnippet

snippet parseDate "for parsing a date"
# where pd is for pandas and landslides['date'] is the name of the column
landslides['date_parsed'] = pd.to_datetime(landslides['date'], format="%m/%d/%y")
endsnippet

snippet normalizeData "for normalizing the given data for a machine learning algoritm"
# normalize the exponential data with boxcox
normalized_data = stats.boxcox(original_data)
endsnippet

snippet scalingData "for scaling the needed data"
## this is used to place the data in a matter of 0 - 1
# generate 1000 data points randomly drawn from an exponential distribution
original_data = np.random.exponential(size=1000)

# mix-max scale the data between 0 and 1
# where the columns are the list of columns to be normalized
scaled_data = minmax_scaling(original_data, columns=[0])
endsnippet

snippet contractions "for creating a dictionary with english common word contractions"
#O_O replace_data -- for replacing data in a column

contractions = { 
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he'll've": "he will have",
"he's": "he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how does",
"i'd": "i would",
"i'd've": "i would have",
"i'll": "i will",
"i'll've": "i will have",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'd've": "it would have",
"it'll": "it will",
"it'll've": "it will have",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she would",
"she'd've": "she would have",
"she'll": "she will",
"she'll've": "she will have",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"so's": "so is",
"that'd": "that would",
"that'd've": "that would have",
"that's": "that is",
"there'd": "there would",
"there'd've": "there would have",
"there's": "there is",
"they'd": "they would",
"they'd've": "they would have",
"they'll": "they will",
"they'll've": "they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
" u ": " you ",
" ur ": " your ",
" n ": " and "}

endsnippet

snippet fillWithNextToItVal "for fill NaN values with the value that comes directly after it"
sf_permits_with_na_imputed = sf_permits.fillna(method='bfill', axis=0)
endsnippet

# options im-> inword
snippet feature_imputer "for feature imputter using sqlearn"
# %%
from sklearn.impute import SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer


# imputers
imp_embarked = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')
imp_age = IterativeImputer(max_iter = 100, random_state = 34, n_nearest_features = 2)

# impute embarked
train['Embarked'] = imp_embarked.fit_transform(train[['Embarked']])
test['Embarked'] = imp_embarked.transform(test[['Embarked']])

# impute age
train['Age'] = np.round(imp_age.fit_transform(train[['Age']]))
test['Age'] = np.round(imp_age.fit_transform(test[['Age']]))

# ==========================
# ========== miss forest ======
# ==========================

!pip install missingpy
!pip install miceforest

from missingpy import MissForest

r1 = random.sample(range(len(df2)),36)
r2 = random.sample(range(len(df2)),34)
r3 = random.sample(range(len(df2)),37)
r4 = random.sample(range(len(df2)),30)
df2['AveRooms'] = [val if i not in r1 else np.nan for i, val in enumerate(dt2['AveRooms'])]
df2['HouseAge'] = [val if i not in r2 else np.nan for i, val in enumerate(dt2['HouseAge'])]
df2['MedHouseVal'] = [val if i not in r3 else np.nan for i, val in enumerate(dt2['MedHouseVal'])]
df2['Latitude'] =    [val if i not in r4 else np.nan for i, val in enumerate(dt2['Latitude'])]


imputer = MissForest() #miss forest
X_imputed = imputer.fit_transform(df2)
X_imputed = pd.DataFrame(X_imputed, columns = df2.columns).round(1)

missF = np.sum(np.abs(X_imputed[df2.isnull().any(axis=1)] -         dt2[df2.isnull().any(axis=1)]))

fig,ax = plt.subplots(figsize=(29,12),nrows=2, ncols=2)
for col in ['AveRooms', 'HouseAge', 'MedHouseVal', 'Latitude']:
	sns.kdeplot(x = X_imputed[col][df2.isnull().any(axis=1)], label = 'MissForest' )
	sns.kdeplot(x = dt2[col][df2.isnull().any(axis=1)], label= 'Original', ax=ax[i][j] )


# for categorical data
X_train = train.iloc[:,1:4]
y_train = train.iloc[:,0]

# fit our logistic regression model
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train, y_train)

to_predict = values
lr.predict(to_predict)

endsnippet

snippet combineDataframe "for combining a dataframe"
# when you have different dataframes with the same columns
pd.concat([canadian_youtube, british_youtube])


# combine dataframes that have an index in common
# where MeetID is the index in common and can be different
powerlifting_combined = powerlifting_meets.set_index("MeetID").join(powerlifting_competitors.set_index("MeetID"))
endsnippet

snippet rename "for renaming the dataset"
reviews.rename(columns={'points': 'score'},inplace=True)
reviews.rename(index={0: 'firstEntry', 1: 'secondEntry'})
reviews.rename_axis("wines", axis='rows').rename_axis("fields", axis='columns')
endsnippet


snippet Nan "for managing Null values or Not a Number values"
reviews[pd.isnull(reviews.country)]
reviews[pd.notnull(reviews.country)]
reviews.region_2.fillna("Unknown")

# also for replacing the values
reviews.taster_twitter_handle.replace("@kerinokeefe", "@kerino")
endsnippet

snippet convertType "for converting a data type of a column pandas"
reviews.points.astype('float64')
endsnippet

snippet datatype "for getting the data type of a column"
reviews.price.dtype

# for getting all columns
reviews.dtypes
endsnippet


snippet sort_values "for sorting a pandas dataframe"
countries_reviewed.sort_values(by=['country', 'len'])
endsnippet

# options im-> inword
snippet group_by "for grouping a dataframe by a column name"
reviews.groupby(['col_name','col_name2']).apply(function,axis='columns')

# reset to normal index
# reviews.reset_index()


# example
price_extremes = reviews.groupby('variety').price.agg([min, max])

endsnippet

snippet applyFunction "for applying a function to all the rows in a dataset"
def function(row):
	row.col_name = row.col_name - 5
	return row

dataFrame.apply(function, axis='columns')
endsnippet

# options im-> inword
snippet replace_data "for replacing data in a columns"
dataframe['column'] = dataframe['column'].replace(['value1','value2'], 'value')
endsnippet

snippet dataDrop
${1:name_of_db}.drop(
labels=["name","example"],
axis=1, # 0 -> rows 1 -> columns
inplace=False # alter the dataFrame
)
endsnippet

# options im-> inword
snippet describe "for describing data"
${1:pandas_dataframe}.describe
endsnippet

# options im-> inword
snippet columns "for getting the column names in pandas"
${1:name}.columns
endsnippet

snippet pcombinecsv
${1:name_of_files} = glob.glob("${2:PATH/TO/FILES/file*_.csv}")
${1}_dt = pd.concat([pd.read_csv(f) for f in ${1} ])
endsnippet


# options im-> inword
snippet loc_iloc "for information on loc and iloc"
# the iloc if for using the integer position of the dataframe
# the loc is for using the label position on the dataframe
endsnippet


# options im-> inword
snippet create_dataframe "for creating a pandas dataframe"
# optionales
# index=['idx1','idx2']
pd.DataFrame({'${1:col_name}':[${2:vals,separated,by,comman|series}]})
endsnippet

# options im-> inword
snippet create_series "for creating a pandas series ( list of values)"
# optionals
# index=['index1','index2', ... , 'index5'] -- to assigning a name to the indices
# name = "name of series" -- for assigning a name to the whole series
pd.Series([1, 2, 3, 4, 5])
endsnippet


snippet read_csv "for reading a pandas csv"
# optionals
# set the index col manually
# index_col=num
dt_name = pd.read_csv("path/to/file.csv")


# for specific encoding
# kickstarter_2016 = pd.read_csv("../input/kickstarter-projects/ks-projects-201612.csv", encoding='Windows-1252')

# for loading dates as index
# museum_data = pd.read_csv(museum_filepath,index_col="Date",parse_dates=True)

endsnippet

# options im-> inword

# options im-> inword
snippet csv_len "for getting the length of the csv"
dt_name.shape
endsnippet

# options im-> inword
snippet csv_head "for getting the top 5 elements of the csv"
dt_name.head()
endsnippet

# options im-> inword
snippet get_column "for getting the column of a dataframe"
# dataframe.title
# dataframe["title"]
endsnippet

# options im-> inword
snippet get_val "for getting the value of the column and row"
# dataframe["title"][num]
endsnippet

# options im-> inword
snippet get_row "for getting the row of the pandas dataframe"
#dataframe.iloc[index_num]
endsnippet


# options im-> inword
snippet getRange_col "for getting a range in a column"
# dataframe.loc[row_start:row_end,column]
# you can pass a list instead to get only the needed indices
# dataframe.loc[[1,2,3,4],[col1,col2]]
endsnippet

# options im-> inword
snippet getRange_row "for getting a range in a row"
# dataframe.iloc[start:end-1]

# also you can do
# museum_data.loc[first_slice_q:second_slice_q]
# because for dates it tends to work because you use a specific index and cannot be -1
endsnippet

# options im-> inword
snippet getRange_row_index "for getting a range of rows using the index"
# remember that loc takes the last element of the range unlike python
dataframe.loc[row_start:row_end,'index_name_1','index_name_2']
# you can use a list [1,2,5] instead of a range 1:2
endsnippet

# options im-> inword
snippet set_index "for setting the index column for the dataframe"
dataframe.set_index("title")
endsnippet

# options im-> inword
snippet conditional_selection "for getting a dataframe with a condition"
dataframe.loc[dataframe.title == "Italy"]
# optional
# dataframe.loc[dataframe.title.isin(['Italy','France'])]
# dataframe.loc[dataframe.title.notnull())]
# use always parenthesis since it tends to fail
# use & and | instead of and or
endsnippet

# options im-> inword
snippet assignToColumn "for assigning a value to a column"
dataframe['col_title'] = 'constant'
dataframe['col_title'] = range(len(dataframe),0,-1)
endsnippet

# options im-> inword
snippet columnDescription "for getting the column description"
# for getting general information on the dataframe
# dt_dataframe['col_name'].describe()
# for getting the mean needed
# dt_dataframe['col_name'].mean()
# for getting the unique values
# dt_dataframe['col_name'].unique()
# for getting the times each element appears in the list
# dt_dataframe['col_name'].value_counts()

endsnippet


snippet mapFunction "for applying a basic function to all cells in a column"
# column_mean = dataFrame.col_name.mean()
# also you can put a function instead of a lambda function
# dataFrame.col_name.map(lambda p: p - column_mean)

# also you can apply a operand like
# and it will be the same
# dataFrame.col_name - column_mean

# also < > ==  etc
# example:
# dataFrame.col_name + " - " + dataFrame.second_col_name
endsnippet


# options im-> inword
snippet pltannotate "for annotation description with an arrow"

fig, ax = plt.subplots(figsize=(10,8))

# annotations blocks
bbox_args = dict(boxstyle='round',fc = "0.8")
arrow_args = dict(arrowstyle="->", color = 'black', linewidth = 2)

ax.annotate('global minimum\n$f(x) = 0$', xy = (0,0), xytext = (-2,8), bbox = bbox_args,arrowprops = arrow_args)

endsnippet


# options im-> inword
snippet matplotlib "for matplotlib information"

plt.xlabel('\${1:x_description}')
plt.xlabel('\${2:y_description}')


plt.title('\$1')


#pass a list and respect the order of the plots
# format (['f(x)','g(x)'])

plt.legend(['\${1:function_mathematical_representation}\$2'])




plt.xlim(\${1:left_lim},\${2:right_lim})
plt.ylim(\${3:left_lim},\${4:right_lim})


plt.grid(True)



def getClosestPointToLine(p1,p2,p3):
	x1, y1 = p1
	x2, y2 = p2
	x3, y3 = p3
	dx, dy = x2-x1, y2-y1
	det = dx*dx + dy*dy
	a = (dy*(y3-y1)+dx*(x3-x1))/det
	return x1+a*dx, y1+a*dy



\${1:x_var} = np.linspace(\${2:x_lower_bound},\${3:x_upper_bound},\${4:step})
\${5:y_var} = np.linspace(\${6:y_lower_bound},\${7:y_upper_bound},\${8:step})
X_, Y_ = np.meshgrid(\$1, \$5)
Z_ = mainFunction(X_, Y_)
plt.contour(X_,Y_,Z_,\${9:number_of_curves})


plt.scatter(\${1:x},\${2:y})




\${1:x_var} = np.arange(\${2:left_range},\${3:right_range},\${4:step})

plt.plot(\$1,\${5:f(x)})


# here you have to identify the lower function and the bigger function
# you can use something like below
# botton = np.maximum(y1,0)

plt.fill_between(\${1:function_1},\${2:function_2},\${3:function_3},where=\${4:x_conditions} )


endsnippet


snippet folium "for folium information"

m = folium.Map(location=[0,0], tiles = 'cartodbpositron',min_zoom = 1, max_zoom = 4, zoom_start=1)

for i in range(0,len(data)):
	tooltip = '<li><bold> Country:' + str(data.iloc[i]['Country']) +
	'<li><bold> Year:' + str(data.iloc[i]['Year'])
	folium.Circle(
			location=[data.iloc[i]['lat'], data.iloc[i]['lon']],
			popup=data.iloc[i]['name'],
			radius=100000,
			color='crimson',
			fill=True,
			fill_color='crimson'
			).add_to(m)

endsnippet


# options im-> inword
snippet animation "for animation"


# %%
def f(x):
	return np.sin(x) + 

def f_prime(x):
	return np.cos(x)

# %%



# %%
def animation():

	# generate a random name of 6 characters
	fps = 2
	letters = 'qwertyuiopasdfghjklzxcvbnm'
	images = []
	name = ""


	fig, ax = plt.subplots(figsize = (5,10))
	for i in range(10):
		name += random.choice(letters)

	name += ".gif"


	folder = 'gifs/'
	if not os.path.exists(folder):
		os.mkdir(folder)
	name = folder + name


	iterations = 40
	# function plot
	px = 0.0
	py = 1

	x = np.arange(0,30,0.01)
	fun = plt.plot(x,f(x), color = 'blue')
	for frame in range(iterations):

		px += 0.8
		py = f(px)


		tx = np.arange(px -2, px + 2, 0.1)
		slope = f_prime(px)
		y_intercept = py - slope * px
		ty = tx * slope + y_intercept




		tangent = plt.plot(tx,ty,color = 'red')

		point = plt.scatter(px,py,color = 'red')


		# plot point to track
		images.append([fun[0], tangent[0], point])



	anim = ArtistAnimation(fig,images)
	anim.save(name,writer = 'imagemagic',fps = fps)

	html = '<img src="'+name+'">'
	plt.close()
	return html,name


# %%
html,name = animation()
HTML(html)

endsnippet


# options im-> inword
snippet plotText "for plotting text"

# Plot text info
bbox_args = dict(boxstyle="round", fc="0.8")
arrow_args = dict(arrowstyle = '->', color = 'b', linewidth = 1)
text = f'Iteration: {frame}\nPoint: ({px:.2f}, {py:.2f})\nSlope: {slope:.2f}\nStep: {step:.4f}'
text = ax.annotate(text, xy = (px, py), xytext = (0.7, 0.9), textcoords = 'axes fraction', bbox = bbox_args, arrowprops=arrow_args, fontsize = 12)

endsnippet

# ==========================
# ========== Kaggle ======
# ==========================

# options im-> inword
snippet pltFormat "for configureing the format of the plot"
%config InlineBackend.figure_formats = ['${1:svg|png}']
endsnippet

# options im-> inword
snippet palettes "for color palettes"
# list all seaborn palettes 
# deep, muted, pastel, bright, dark, colorblind
endsnippet

# options im-> inword
snippet multipie "for plotting multilpe pie plots"
# %%
def plot1dpie(ax,labels_values, labels, title, x,y):
	ax[y].pie(labels_values, labels = labels, autopct='%1.1f%%', shadow = True, startangle = 90)
	ax[y].title.set_text(title)

def plot2dpie(ax,labels_values, labels, title,x,y):
	# ca
	ax[x,y].pie(labels_values, labels = labels, autopct='%1.1f%%', shadow = True, startangle = 90)
	ax[x,y].title.set_text(title)


def plotpie(x,y,ax,labels_values, labels, title):
	# check if ax is 2d or 1d
	if len(ax.shape) == 1:
		plot1dpie(ax,labels_values, labels, title, x,y)
	else:
		plot2dpie(ax,labels_values, labels, title,x,y)

def multipie(data, titles, target_col, labels, array_cols, icols = -1, irows = -1, pivot_column = None):
	"""
	%%
	data in the form of 
	
	|Survived|1  |2 |3  |
	|1       |136|87|119|
	|1       |80 |97|372|
	
	titles = ['1 survival rate','2 survival rate','3 survival rate']
	target_col = 'Survived'
	labels = ['Survived','Died']
	array_cols = ['1','2','3']
	icols, irows = 2,2
	pivot_column = 'Pclass' -- if Not None, then the data will be pivoted
	"""

	if pivot_column is not None:
		tmp = data.pivot_table(index = target_col, columns = pivot_column, aggfunc = 'size',fill_value = 0).reset_index()
		tmp = tmp.rename_axis(None, axis = 1)
		data = tmp


	display(data)
	input_cols = len(array_cols) + 1
	if icols == -1:
		ncols = 3
	else:
		ncols = icols

	if irows == -1:
		nrows = (input_cols - 1) // ncols  + 1
	else:
		nrows  = irows

	fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize=(8,2 * nrows))
	# first plot the sum of the target column
	labels_values = [0] * len(data)
	# get the sum of all the columns for each row and store it in a list
	# exclude the target_col column from the sums

	for i in range(len(data)):
		for j in range(len(array_cols)):
		
			column = array_cols[j]
			labels_values[i] += data.loc[i,column]


	plotpie(0,0,ax,labels_values, labels, title = 'Sum of ' + target_col)
	# now plot all the array_cols with the target_col individually
	for i in range(1, len(array_cols) + 1):
		labels_values = [0] * len(labels)
		for j in range(len(data)):
			labels_values[j] = data[array_cols[i-1]].iloc[j]
		row = (i) // ncols
		col = (i) % ncols
		print(row, col)
		plotpie(row,col,ax,labels_values, labels, title = titles[i-1])
	# delete unused plots axes
	for i in range(len(array_cols) + 1, ncols * nrows):
		row = i // ncols
		col = i % ncols
		fig.delaxes(ax[row,col])


	plt.tight_layout()
endsnippet


# options im-> inword
snippet plot2dgaussianNB "for plotting gaussian NB"

def plot_decision_surface(clf, features, labels, title):
	x_min = 0.0; x_max = 8.0
	y_min = 0.0; y_max = 8.0

	# Plot the decision boundary. For that, we will assign a color to each
	# point in the mesh [x_min, m_max]x[y_min, y_max].
	h = .01  # step size in the mesh
	xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
	Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])   
	z_min, z_max = -np.abs(Z).max(), np.abs(Z).max()

	# set title
	f, ax = plt.subplots()
	ax.set_title(title)

	# Put the result into a color plot
	Z = Z.reshape(xx.shape)
	plt.xlim(xx.min(), xx.max())
	plt.ylim(yy.min(), yy.max())
	plt.pcolormesh(xx, yy, Z, cmap='gray', vmin=.2, vmax=2.4)  # 

	# Plot the points
	class_1_x = [features[ii][0] for ii in range(0, len(features)) if labels[ii]==1]
	class_1_y = [features[ii][1] for ii in range(0, len(features)) if labels[ii]==1]
	class_2_x = [features[ii][0] for ii in range(0, len(features)) if labels[ii]==2]
	class_2_y = [features[ii][1] for ii in range(0, len(features)) if labels[ii]==2]

	plt.scatter(class_1_x, class_1_y, color = '#F5CA0C', label="1", s=40)
	plt.scatter(class_2_x, class_2_y, color = '#00A99D', label="2", s=40)

	plt.xlabel("feature_a")
	plt.ylabel("feature_b")
	plt.legend(('class 1', 'class 2'), frameon=True)


# Plot the decision surface and data point for training and tests data
# clf = GaussianNB() # already defined
plot_decision_surface(clf, train_features.values, train_labels.values, 'Training data')
plot_decision_surface(clf, test_features.values, test_labels.values,'Test data')
endsnippet

# options im-> inword
snippet ignoreWarnings "for ignoring warnings"
with warnings.catch_warnings():
	warnings.simplefilter('ignore')

# %%
%%javascript
(function(on) {
const e = $("Setup failed");
const ns = "js_jupyter_suppress_warnings";
var cssrules = $("#" + ns);
if(!cssrules.length)
cssrules = $("").appendTo("head");
e.click(function() {
		var s = 'Showing';
		cssrules.empty()
		if(on) {
		s = 'Hiding';
		cssrules.append("div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }");
		}
		e.text(s + ' warnings (click to toggle)');
		on = !on;
		}).click();
$(element).append(e);
})(true);

endsnippet

snippet install_kaggle

# upgrade pip
!pip install --upgrade pip
# dependencies
!pip install --upgrade chart_studio
# necessary

# ==========================
# ========== general ======
# ==========================
# !pip install gc
# !pip install os
#
# ==========================
# ========== for machine learning ======
# ==========================
# !pip install --upgrade sklearn
# !pip install --upgrade xgboost
# !pip install --upgrade lightgbm
# !pip install --upgrade catboost
# !pip install --upgrade keras
# !pip install --upgrade tensorflow

# ==========================
# ========== for data cleaning ======
# ==========================
# !pip install --upgrade textblob
# !pip install --upgrade pickle5
# !pip install --upgrade numpy
# !pip install --upgrade numpy
# !pip install --upgrade pandas


# ==========================
# ========== for plotting ======
# ==========================
# !pip install --upgrade folium
# !pip install --upgrade matplotlib
# !pip install --upgrade seaborn
# for visualization tools
# !pip install --upgrade cufflinks
# !pip install --upgrade plotly
# !pip install --upgrade ipython
# !pip install --upgrade pylab
# !pip install --upgrade mpl_toolkits

# ==========================
# ========== Big Data ======
# ==========================

# %%
# !pip install databricks
# !pip install pyspark



endsnippet

# options im-> inword
snippet import_kaggle "for importing the necessary for machine learning plotting and other things"
%config InlineBackend.figure_formats = ['svg']

# ========== for machine learning ======
# -----------------> imp_sqlearn
# -----------------> imp_xgboost
# -----------------> imp_pyspark
from sklearn.model_selection import cross_val_score # the cross validation module
from sklearn.naive_bayes import GaussianNB # gaussian naive bayes
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# ==========================
# ========== data cleaning ======
# ==========================
import pickle as pkl
from numpy.random import randint
import numpy as np # for importing numpy
import pandas as pd # for importing pandas
pd.set_option('display.expand_frame_repr', False)
pd.set_option('display.max_columns', None)
# ----------------> imp_excleaning -- for importing extended data cleaning


# ========== plotting ======
# -------> imp_interactive
import matplotlib.pyplot as plt # for importing matplotlib
from matplotlib.patches import Polygon
from matplotlib.animation import FuncAnimation
import seaborn as sns # seaborn is used to plot
from matplotlib.animation import ArtistAnimation
from mpl_toolkits.mplot3d import Axes3D
from pylab import meshgrid
from IPython.display import Image, display, HTML
# ========== general ======
import time
%matplotlib inline
import random
import gc
import os
# ========== Big Data ======
# -------------------> imp_bigdata
# ========== statistics ======
from scipy import stats
import statsmodels.api as sm
endsnippet

# options im-> inword
snippet imp_sqlearn "for importing sqlearn machine learning libraries"
from sklearn.ensemble import RandomForestRegressor # random forest regressor
endsnippet

# options im-> inword
snippet imp_interactive "for importing interactive plots"
import folium
import plotly.offline as py # for importing plotly
import cufflinks as cf
cf.go_offline()
import plotly.graph_objects as go
import plotly.express as px
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
endsnippet

# options im-> inword
snippet imp_sklearn "for importing sqlearn machine learning models"
# import random forest classifier
from sklearn.ensemble import RandomForestClassifier
# import random forest regressor
from sklearn.ensemble import RandomForestRegressor
# import gradient boosting classifier
from sklearn.ensemble import GradientBoostingClassifier
# import gradient boosting regressor
from sklearn.ensemble import GradientBoostingRegressor
# import support vector machine classifier
from sklearn.svm import SVC
# import support vector machine regressor
from sklearn.svm import SVR
# import linear regression
from sklearn.linear_model import LinearRegression
# import logistic regression
from sklearn.linear_model import LogisticRegression
# import k-nearest neighbors classifier
from sklearn.neighbors import KNeighborsClassifier
# import k-nearest neighbors regressor
from sklearn.neighbors import KNeighborsRegressor
# import naive bayes classifier
from sklearn.naive_bayes import GaussianNB
# import decision tree classifier
from sklearn.tree import DecisionTreeClassifier
# import decision tree regressor
from sklearn.tree import DecisionTreeRegressor
# import neural network classifier
from sklearn.neural_network import MLPClassifier
# import neural network regressor
from sklearn.neural_network import MLPRegressor
# import gaussian process classifier
from sklearn.gaussian_process import GaussianProcessClassifier
# import gaussian process regressor
from sklearn.gaussian_process import GaussianProcessRegressor
# import adaboost classifier
from sklearn.ensemble import AdaBoostClassifier
# import adaboost regressor
from sklearn.ensemble import AdaBoostRegressor
# import bagging classifier
from sklearn.ensemble import BaggingClassifier
# import bagging regressor
from sklearn.ensemble import BaggingRegressor
# import extra trees classifier
from sklearn.ensemble import ExtraTreesClassifier
# import extra trees regressor
from sklearn.ensemble import ExtraTreesRegressor
endsnippet

# options im-> inword
snippet imp_xgboost "for importing xgboost"
# import linear regression
from xgboost import XGBRegressor
# import logistic regression
from xgboost import XGBClassifier
# import decision tree

endsnippet

# options im-> inword
snippet imp_pyspark "for importing pyspark machine learning libraries"
from pyspark.ml.classification import LogisticRegression,RandomForestClassifier,GBTClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
endsnippet

# options im-> inword
snippet imp_bigdata "for importing big data"
import pyspark.pandas as ps
from pyspark.sql import SparkSession
from pyspark.sql.functions import * # count, mean, when, lit, create_map, col, split, regexp_extract
endsnippet


# options im-> inword
snippet imp_excleaning "for importing utilities for data cleaning"
from sklearn.utils import shuffle # for shuffling the data
from textblob import TextBlob
endsnippet


# options im-> inword
snippet model_selection "for selectin a machine learning model"

# based on data type
images & videos - cnn
text data or speech data - rnn
numerical data - svm, logisticRegression, Decision trees, etc.


# based on the task
classification - logistic regression, decision trees, random forest, etc.

endsnippet

# ==========================
# ========== math ======
# ==========================

# options im-> inword
snippet solve_eq "for solving an equation"
# We can solve by taking the inverse of matrix a times b
a.I * b

# A Faster and more stable way of doing the same is
# using the solve function
s = np.linalg.solve(a, b)
s

# Check the solution
x = s[0]
y = s[1]
z = s[2]

endsnippet
# ==========================
# ========== statistics ======
# ==========================
# options im-> inword
snippet zscores "for formula"
zscore = (x - x.mean()) / x.std()
endsnippet

# options im-> inword
snippet pfunArea "get probability of a range"
# you have to get the zscore
ffriends = 227
ffzscore = (ffriends - facebook_mu ) / facebook_sigma

# then you can plot for more convenience

x = np.linspace(-4, 4, 100)
y = stats.norm.pdf(x)
plt.plot(x, y)
plt.fill_between(x, y, where=(x > lzscore) & (x < rzscore), color='red')

# to get the probability of the area to the left of zscore you do
pl = stats.norm.cdf(ffzscore)
# this is usually is done via integration or via a table for lookup

# given a probability, like in this case alpha = 0.05 or alpha/2 = 0.025
# you can get the zscore
critical_value = stats.norm.ppf(alpha / 2) * -1.

endsnippet


# options im-> inword
snippet norm_array "for creating a norm array"
mu = 5
std = 2 
y = stats.norm.rvs(mu, std, size=1000)

endsnippet

snippet ttest "for using a ttest"




## perform a ttest on a sample
# given the mean of the population and a single sample
stats.ttest_1samp(a = minesota_ages, popmean = population_ages.mean())

# This returns 2 values, the t-statistic and the p-value
# the t-statistic is the number of standard deviations away from the mean
# the p-value is the probability of getting a t-statistic as extreme as the one we got
# usually if it is less than 0.05 we reject the null hypothesis


# now manually
# so now to test the null hypothesis we can do

stats.t.ppf(q=0.025, df = (len(sample_size) - 1))
stats.t.ppf(q=0.975, df = (len(sample_size) - 1))

# this gives us the critical values for the t-test
# so now we have our range, if the t-statistic is outside the range
# we reject the null hypothesis

# to get the probability we do
stats.t.cdf(x = (tstatistic), df = (len(sample_size) - 1)) * 2

# now to capture the true mean on the t-test we do

sigma = sample.std() / math.sqrt(len(sample))
stats.t.interval(alpha = 0.99,
	df = (len(sample_size) - 1),
	loc = population_mean,
	scale = sigma)

# ==========================
# ========== two population t-test ======
# ==========================
# this function is used when using two independent samples
# for example
# the difference in between baby weight from smoker mothers and non-smoker mothers
# you have to different samples of DIFFERENT MOTHERS in each sample
stats.ttest_ind(a = sample1, b = sample2, equal_var = False)

# pvalue < 0.05 reject null hypothesis


# ==========================
# ========== paired t-test ======
# ==========================

# This ttest is applied when the two samples are related
# ex:
# from the SAME 72 books look for the mean in amazon and the mean in alibaba
stats.ttest_rel(a = amazon_price, b = alibaba_price)
# pvalue < 0.05 -> there is a significant difference

# ==========================
# ========== Type I and Type II Error ======
# ==========================
# Type I error -> reject null hypothesis when it is true
# Type II error -> accept null hypothesis when it is false


endsnippet


# options im-> inword
snippet norm_test "for testing a normal test"
w, p = stats.shapiro(data)

# if p > 0.05 -> normal
# if p < 0.05 -> not normal 
endsnippet

# options im-> inword
snippet anova "for testing multiple independent sampes"
# One-way ANOVA the convenient way
f_value, p_value = stats.f_oneway(olixir, ernies, fendor, daddar)
print(f_value)
print(p_value)

# Null hypothesis test at alpha level .05
p_value > .05

# Construct single dataset for Tukey-kramer HSD test
x1 = pd.DataFrame(olixir, columns=['observation'])
x1['grouplabel'] = 'olixer'
x2 = pd.DataFrame(ernies, columns=['observation'])
x2['grouplabel'] = 'ernies'
x3 = pd.DataFrame(fendor, columns=['observation'])
x3['grouplabel'] = 'fendor'
x4 = pd.DataFrame(daddar, columns=['observation'])
x4['grouplabel'] = 'daddar'

data = x1.append(x2).append(x3).append(x4)
data.head()

result = sm.stats.multicomp.pairwise_tukeyhsd(data.observation, data.grouplabel)
print(result.summary())



endsnippet
# options im-> inword
# ==========================
# ========== machine learning ======
# ==========================

# options im-> inword
snippet naive_bayes "for a naive bayes"
clf = GaussianNB()
clf.fit(train_features, train_labels)
clf.predict([[4.8,4.2]])
endsnippet
# options im-> inword
snippet split_validation "for setting the split validation"
from sklearn.model_selection import train_test_split

# split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
# X is the parameters given and y is the target
# random state is the seed for the train -- use the same seed for same results

train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)
# Define model
melbourne_model = DecisionTreeRegressor()
# Fit model
melbourne_model.fit(train_X, train_y)

# get predicted prices on validation data
val_predictions = melbourne_model.predict(val_X)
print(mean_absolute_error(val_y, val_predictions))
endsnippet

# options im-> inword
snippet model_descition_tree "for a basic model"
from sklearn.tree import DecisionTreeRegressor

# Define model. Specify a number for random_state to ensure same results each run
melbourne_model = DecisionTreeRegressor(random_state=1)
# X is the parameters and y is the target
# Fit model
melbourne_model.fit(X, y)

print("Making predictions for the following 5 houses:")
print(X.head())
print("The predictions are")
print(melbourne_model.predict(X.head()))
endsnippet

# options im-> inword
snippet model_random_forest "is the descition tree with randomness"
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# X is the parameters and y is the target
forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
endsnippet

# options im-> inword
snippet getMAE "for getting the mean absolute error of a basic model"
# this is to get the error testing different size of tree (tree nodes)
# is meant to be used in a loop

def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
	model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
	model.fit(train_X, train_y)
	preds_val = model.predict(val_X)
	mae = mean_absolute_error(val_y, preds_val)
	return(mae)
endsnippet

# options im-> inword
snippet ParamBuilderSqlearn "param builder for sklearn"
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report

# Initialize a RandomForestClassifier
rf = RandomForestClassifier(random_state=34)

params = {'n_estimators': [50, 100, 200, 300, 350],
          'max_depth': [3,4,5,7, 10,15,20],
          'criterion':['entropy', 'gini'],
          'min_samples_leaf' : [1, 2, 3, 4, 5, 10],
          'max_features':['sqrt'],
          'min_samples_split': [3, 5, 10, 15, 20],
          'max_leaf_nodes':[2,3,4,5],
          }

clf = GridSearchCV(estimator=rf,param_grid=params,cv=10, n_jobs=-1)

clf.fit(X_train, y_train.ravel())

print(clf.best_estimator_)
print(clf.best_score_)

rf_best = clf.best_estimator_

# Predict from the test set
y_pred = clf.predict(X_test)

# Print the accuracy with accuracy_score function
print("Accuracy: ", accuracy_score(y_test, y_pred))

endsnippet

# options im-> inword
snippet xgboostClassifier "for the classifier"
import xgboost as xgb
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# %%
# create the model
model = xgb.XGBClassifier( tree_method='gpu_hist')

# %%
# train the model
model.fit(X_train, y_train)

# %%
# show the accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

endsnippet

# options im-> inword
snippet xgboostRegressor "regressor"
import xgboost as xgb
# %%
linear = xgb.XGBRegressor(tree_method='gpu_hist')

# %%
# train the model
linear.fit(X_train, y_train)

# %%
# predict the test data
y_pred = linear.predict(X_test)

# %%
# show the accuracy
accuracy = accuracy_score(y_test, y_pred.round())
print("Accuracy: %.2f%%" % (accuracy * 100.0))

# %%
# predict the test data
preds2 = linear.predict(test)

endsnippet

# options im-> inword
snippet export_csv "for exporting a csv file"
# Run the code to save predictions in the format used for competition scoring

output = pd.DataFrame({'Id': test_data.Id,
	'SalePrice': test_preds})
output.to_csv('submission.csv', index=False)
endsnippet

# ================================
# ========== graphs ==============
# ================================

# options im-> inword
snippet globalTextSize "for setting the global text size"

# %%
SMALL_SIZE = 8
MEDIUM_SIZE = 10
BIGGER_SIZE = 12

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)


endsnippet

snippet graphs "for importing the necessary for machine learning plotting and other things"
sns.pie(data, labels = labels, autopct='%1.1f%%')
sns.lineplot(data=fifa_data)
sns.barplot(x=flight_data.index, y=flight_data['NK'])
# must of the arguments here apply to all the graphs
sns.heatmap(
			df.corr(),
			cmap = colormap,
			square=True,
			cbar_kws={'shrink':.9},
			ax=ax,
			annot=True,
			linewidths=0.1,vmax=1.0, linecolor='white',
			annot_kws={'fontsize':12 },
			col = 'value',
			col_wrap = 3
)
sns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'])
sns.regplot(x=insurance_data['bmi'], y=insurance_data['charges'])
sns.distplot(a=iris_vir_data['Petal Length (cm)'], label="Iris-virginica", kde=False)
sns.kdeplot(data=iris_ver_data['Petal Length (cm)'], label="Iris-versicolor", shade=True)
sns.jointplot(x=iris_data['Petal Length (cm)'], y=iris_data['Sepal Width (cm)'], kind="kde")
sns.set_style("dark")
sns.countplot(x = 'pclass', data = titanic, hue = 'survived')

np.random.seed(0)
N = 20
theta = np.linspace(0.0, np.pi * 2 , N , endpoint = False)
radii = 10 * np.random.rand(N)
width = np.pi / 4 * np.random.rand(N)
colors = plt.cm.plasma(radii / 10)

plt.figure(dpi = 100)
ax = plt.subplot(1,1,1, projection = 'polar')
ax.bar(theta,radii,width = width, bottom = 0, color = colors, alpha = 0.7)
plt.show()
endsnippet

# options im-> inword
snippet graphsSubplots "for plotting subplots"
rows = 4
cols = 4

fig, ax = plt.subplots(rows, cols, figsize = (20,20))

	col = data.columns
	index = 0
	limit = 14

	for i in range(rows):
		for j in range(cols):

			if index >= limit:
			continue
			sns.scatterplot(x = 'Price', y = col[index], data = data, ax = ax[i][j])
			ax[i][j].tick_params('x', labelrotation=45)
# change font size of y axis label
ax[i][j].yaxis.label.set_size(20)
# change font size of x axis label
ax[i][j].xaxis.label.set_size(20)
	index = index + 1


	plt.tight_layout()
plt.show()

# =========================
# ====== graphs over graphs ===========
# =========================


# %%
# create a subplot with a small subplot inside it
fig, ax = plt.subplots(1,1,figsize = (10,10))
sns.lineplot( y = 'arr1', x = df.index, data = df, ax = ax)
ax2 = fig.add_axes([0.8,0.8,0.2,0.2])
sns.scatterplot(x = 'arr2', y = 'arr3', data = df, ax = ax2)
plt.show()

# ==== overlap joinplot
g = sns.jointplot(x = data['RM'], y = data['Price'], kind = 'kde', color = 'm')
g.plot_joint(plt.scatter, c = 'r', s = 40, linewidth = 1, marker = '+')
g.ax_joint.collections[0].set_alpha(0.3)


endsnippet

# options im-> inword
snippet igraphs "for plotting interactive graphs"
# ============================ treemap and example for plotting
fig = px.treemap(full_latest.sort_values(by = 'Confirmed', ascending = False).reset_index(drop = True),
		path = ['Country', 'Province/State'], values = 'Confirmed', height = 700,
		title = 'Number of Confirmed Cases',
		color_discrete_sequence = px.colors.qualitative.Dark2)

#  ===================== line graph with hue as a columns
fig = px.line(country_daywise, x = 'Date', y = 'Confirmed', color = 'Country', height = 600,
		title='Confirmed', color_discrete_sequence = px.colors.cyclical.mygbm)
fig.show()

#  ======================= choropleth map animation
	fig = px.choropleth(country_daywise, locations= 'Country', locationmode='country names', color = np.log(country_daywise['Confirmed']),
			hover_name = 'Country', animation_frame=country_daywise['Date'].dt.strftime('%Y-%m-%d'),
			title='Cases over time', color_continuous_scale=px.colors.sequential.Inferno)

fig.show()


# ======================== heatmap
	fig = px.density_mapbox(df, lat = 'Lat', lon = 'Long', hover_name = 'Country', hover_data = ['Confirmed', 'Recovered', 'Deaths'], animation_frame='Date', color_continuous_scale='Portland', radius = 7, zoom = 0, height= 700)
	fig.update_layout(title = 'Worldwide Covid-19 Cases with Time Laps')
	fig.update_layout(mapbox_style = 'open-street-map', mapbox_center_lon = 0)
fig.show()




	top = 15
	fig = px.scatter(countywise.sort_values('Deaths', ascending = False).head(top),
			x = 'Confirmed', y = 'Deaths', color = 'Country', size = 'Confirmed', height = 600,
			text = 'Country', log_x = True, log_y = True, title='Deaths vs Confirmed Cases (Cases are on log10 scale)')

	fig.update_traces(textposition = 'top center')
fig.update_layout(showlegend = False)
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()

# =================== timeline

first_date = df[df['Confirmed'] > 0 ]
first_date = first_date.groupby('Country')['Date'].agg(['min']).reset_index()
last_date = df.groupby('Country')['Date'].agg(['max']).reset_index()
last_date = last_date.drop(columns = ['Country'],axis = 1)
first_last = pd.concat([first_date,last_date],axis = 1)
first_last = first_last.rename(columns = {"max":"Finish","min":"Start","Country":"Task"})
fig = ff.create_gantt(first_last,height = 3000)
fig.show()



# =========================== folium

m = folium.Map(location=[0,0], tiles = 'cartodbpositron',min_zoom = 1, max_zoom = 4, zoom_start=1)

for i in range(0,len(data)):
	tooltip = '<li><bold> Country:' + str(data.iloc[i]['Country']) +
	'<li><bold> Year:' + str(data.iloc[i]['Year'])
	folium.Circle(
			location=[data.iloc[i]['lat'], data.iloc[i]['lon']],
			popup=data.iloc[i]['name'],
			radius=100000,
			color='crimson',
			fill=True,
			fill_color='crimson'
			).add_to(m)

endsnippet

snippet pairplot "for creating a pair plot"

# %%



def PairGrid(data, hue = None, regplot = False):

	# measure the time it takes to run the code
	prev = time.time()
	column_names = data.columns.to_list()

	# plot the data using seaborn
	if hue == None:
		g = sns.PairGrid(data)
	else:
		g = sns.PairGrid(data, hue = hue,palette = ["blue","orange","green"])

	g.map_upper(sns.scatterplot, s = 6)
	g.map_lower(sns.kdeplot, alpha = 0.5)
	g.map_diag(sns.histplot, kde = True, alpha = 0.5)

	xlabels,ylabels = [],[]

	# get the x labels
	for ax in g.axes[-1,:]:
		xlabel = ax.xaxis.get_label_text()
		xlabels.append(xlabel)

	# get the y labels
	for ax in g.axes[:,0]:
		ylabel = ax.yaxis.get_label_text()
		ylabels.append(ylabel)

	# set all the axes
	for i in range(len(xlabels)):
		for j in range(len(ylabels)):
			g.axes[j,i].xaxis.set_label_text(xlabels[i], visible = True)
			g.axes[j,i].yaxis.set_label_text(ylabels[j],visible = True)

	for ax in g.axes.flat:
		# get the ax position
		pos = ax.get_position()
		# check if the ax is below the main diagonal
		# example
		# 0 1 2
		# 3 4 5
		# 6 7 8
		# if the ax is 3, 6, 7 then it is below the main diagonal


		xlabel = ax.xaxis.get_label_text()
		ylabel = ax.yaxis.get_label_text()
		xindex = column_names.index(xlabel)
		yindex = column_names.index(ylabel)

		below = False
		if xindex < yindex:
			below = True


		# set xlabel and y label text visible
		ax.tick_params(labelleft = True, labelbottom = True)
		ax.xaxis.label.set_visible(True)
		ax.yaxis.label.set_visible(True)
		ax.grid(True)

		color_palette = ["blue","orange","green"]
		# create a regplot for the current ax
		if hue != None and regplot == True and below == False:
			colors = {}
			for i in range(len(data[hue].unique())):
				colors[data[hue].unique()[i]] = color_palette[i%len(color_palette)]
			if ax.get_xlabel() != ax.get_ylabel():
				tcounter = 0
				for cat in data[hue].unique():
					tmp_data = data[data[hue] == cat]
					sns.regplot(x = ax.get_xlabel(), y = ax.get_ylabel(), data = tmp_data, ax = ax, color = colors[cat], scatter = False, line_kws = {"alpha":0.5})
					tcounter+=1

	g.tight_layout()
	# show hue legend
	g.add_legend()
	print( "time(s) : "  + str(time.time() - prev))

endsnippet


# ================================
# ==  feature engineering ========
# ================================

# options im-> inword
snippet feature_selection "for selecting a feature"

import simpleml as sml
# Kendals rank coefficient -> kr
# Spearman rank coefficient -> sr
# Pearson correlation coefficient -> pc
# Chi square -> chi
# ANOVA -> anova
# mutual information -> mi

             |numerical    |  anova, kr    | pc, sr
   OUTPUT    |-------------+---------------+-------------
             |categorical  |  chi, mi      | anova, kr
             |-------------+---------------+-------------
             |             |categorical    |   numerical
				 +--------------------------------------------
				               INPUT

# ================ filter methods
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# feature extraction
chi_best = SelectKBest(score_func = chi2, k=4)
k_best = chi_best.fit(X,Y)

# summarize scores
np.set_printoptions(precision = 3)
print(k_best.scores_)

k_features = k_best.transform(X)
# summarize selected features
print(k_features[0:5,:])

# use personalized kendall function to get the best features
best_columns = sml.get_kendall(data, target = 'target', proportion = 0.3)


# ===================== wrapper methods
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model_lr = LogisticRegression()
recur_fe = RFE(model_lr, 3)
feature = recur_fe.fit(X,Y)
print("Number of Features: %s" (feature.n_features_))
print("Selected Features: %s" (feature.support_))
print("Feature Ranking: %s" (feature.ranking_))

# ===================== embedded methods
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha = 1.0)
ridge_reg.fit(X,Y)

# %%
def print_coefs(coef, names = None, sort = False):
	if names == None:
	names = ['X%s' % x for x in range(len(coef))]
	lst = zip(coef, names)
	if sort:
		lst = sorted(lst, key = lambda x: -np.abs(x[0]))
	return ' + '.join('%s * %s' % (round(coef, 3), name) for coef, name in lst)

print(print_coefs(ridge_reg.coef_))

endsnippet


snippet mutual_info_regression "for creating a mutual info regression"
from sklearn.feature_selection import mutual_info_regression

# where X is the whole dataset and the y is the target
# the discrete features are the integer values
def make_mi_scores(X, y, discrete_features):
	mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
	mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
	mi_scores = mi_scores.sort_values(ascending=False)
	return mi_scores

mi_scores = make_mi_scores(X, y, discrete_features)
mi_scores[::3]  # show a few features with their MI scores
endsnippet


snippet clustering "for clustering information"
# Create cluster feature
kmeans = KMeans(n_clusters=6)
X["Cluster"] = kmeans.fit_predict(X)
X["Cluster"] = X["Cluster"].astype("category")

X.head()


sns.relplot(
	x="Longitude", y="Latitude", hue="Cluster", data=X, height=6,
);
endsnippet

# options im-> inword
snippet multiOnehotencoder "for one hot encoding on multiple columns"
# the drop first parameter can be deleted for convenience and a faster model
test = pd.get_dummies(columns = ['Pclass','Embarked','Ticket','Cabin','Title_category','Fsize'], data = test, drop_first = True)
endsnippet


# ==================================
# == intermediate machine learning =
# ==================================

# options im-> inword
snippet scoreModels "for scoring the models"
from sklearn.ensemble import RandomForestRegressor

# Define the models
model_1 = RandomForestRegressor(n_estimators=50, random_state=0)
model_2 = RandomForestRegressor(n_estimators=100, random_state=0)
model_3 = RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)
model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)
model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)

models = [model_1, model_2, model_3, model_4, model_5]

from sklearn.metrics import mean_absolute_error

# Function for comparing different models
def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):
	model.fit(X_t, y_t)
	preds = model.predict(X_v)
	return mean_absolute_error(y_v, preds)

for i in range(0, len(models)):
	mae = score_model(models[i])
	print("Model %d MAE: %d" % (i+1, mae))


endsnippet

# options im-> inword
snippet imputter "for replacing missing values with a simple regression"
from sklearn.impute import SimpleImputer

# Imputation
my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

# Imputation removed column names; put them back
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns

print("MAE from Approach 2 (Imputation):")
print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))
endsnippet

# options im-> inword
snippet completeImputter "for using imputer and specifying that the imputter was performed in a column"
# Make copy to avoid changing original data (when imputing)
X_train_plus = X_train.copy()
X_valid_plus = X_valid.copy()

# Make new columns indicating what will be imputed
for col in cols_with_missing:
	X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()
	X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()

# Imputation
my_imputer = SimpleImputer()
imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))
imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))

# Imputation removed column names; put them back
imputed_X_train_plus.columns = X_train_plus.columns
imputed_X_valid_plus.columns = X_valid_plus.columns

print("MAE from Approach 3 (An Extension to Imputation):")
print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))
endsnippet

# options im-> inword
snippet ordinalEncoder "for categorical values to encode them in integer colum"
from sklearn.preprocessing import OrdinalEncoder

# Make copy to avoid changing original data 
label_X_train = X_train.copy()
label_X_valid = X_valid.copy()

# Apply ordinal encoder to each column with categorical data
ordinal_encoder = OrdinalEncoder()
label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])

print("MAE from Approach 2 (Ordinal Encoding):") 
print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))
endsnippet


# options im-> inword
snippet oneHotEncoding "like the above but particullarly usefull when you have a column that encapsulates mo categories"
from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = X_train.drop(object_cols, axis=1)
num_X_valid = X_valid.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)

print("MAE from Approach 3 (One-Hot Encoding):") 
print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))

# ================= binary one hot encoding
# do a binary one hot encoding for the Attended column
encoded = OneHotEncoder(sparse=False).fit_transform(df[['Attended']])
df = pd.concat([df, pd.DataFrame(encoded, columns = ['Attended(no)','Attended(yes)'])],axis = 1)
endsnippet

# options im-> inword
snippet splitOneHotEncoding "for using ordinalEncoder on columns that have a lot of unique values"
## for separating the high unique values columns
# Columns that will be one-hot encoded
low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]

# Columns that will be dropped from the dataset
high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))

print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)
print('\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)

# now one hot encoding and cardinality encoding


from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = X_train.drop(object_cols, axis=1)
num_X_valid = X_valid.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)
endsnippet

# options im-> inword
snippet pipeLine "for using a pipieline for better performance and less bugs"
# Step 1: Define Preprocessing Steps
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Preprocessing for numerical data
numerical_transformer = SimpleImputer(strategy='constant')

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
	('imputer', SimpleImputer(strategy='most_frequent')),
	('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
	transformers=[
		('num', numerical_transformer, numerical_cols),
		('cat', categorical_transformer, categorical_cols)
	])
# Step 2: Define the Model

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=0)




# Step 3: Create and Evaluate the Pipeline
from sklearn.metrics import mean_absolute_error

# Bundle preprocessing and modeling code in a pipeline
my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', model)])

# Preprocessing of training data, fit model 
my_pipeline.fit(X_train, y_train)

# Preprocessing of validation data, get predictions
preds = my_pipeline.predict(X_valid)

# Evaluate the model
score = mean_absolute_error(y_valid, preds)
print('MAE:', score)
endsnippet


# options im-> inword
snippet crossValidation "for using crossvalidation with a pipeline and a regressionForest"
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),('model',RandomForestRegressor(n_estimators=50,random_state=0))])

from sklearn.model_selection import cross_val_score

# Multiply by -1 since sklearn calculates *negative* MAE
scores = -1 * cross_val_score(my_pipeline, X, y,cv=5,scoring='neg_mean_absolute_error')

print("MAE scores:\n", scores)

endsnippet

# options im-> inword
snippet xgboost "for using the better xgboost"
from xgboost import XGBRegressor

my_model = XGBRegressor()
my_model.fit(X_train, y_train)

from sklearn.metrics import mean_absolute_error

predictions = my_model.predict(X_valid)
print("Mean Absolute Error: " + str(mean_absolute_error(predictions, y_valid)))

endsnippet






# options im-> inword
snippet dataLeackage "for looking and dropping data leackage"


## first we test the model and see that it has 98% of accuracy
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!)
my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))
cv_scores = cross_val_score(my_pipeline, X, y, cv=5,scoring='accuracy')

print("Cross-validation accuracy: %f" % cv_scores.mean())

## then we can see that the expenditure is creating the leackage
expenditures_cardholders = X.expenditure[y]
expenditures_noncardholders = X.expenditure[~y]

print('Fraction of those who did not receive a card and had no expenditures: %.2f' \
	%((expenditures_noncardholders == 0).mean()))
print('Fraction of those who received a card and had no expenditures: %.2f' \
	%(( expenditures_cardholders == 0).mean()))


## so we fix the leackage
# Drop leaky predictors from dataset
potential_leaks = ['expenditure', 'share', 'active', 'majorcards']
X2 = X.drop(potential_leaks, axis=1)

# Evaluate the model with leaky predictors removed
cv_scores = cross_val_score(my_pipeline, X2, y, cv=5,scoring='accuracy')

print("Cross-val accuracy: %f" % cv_scores.mean())
endsnippet




# options im-> inword
snippet loadBigQuery "for loading the python big query module"
from google.cloud import bigquery


# Create a "Client" object
client = bigquery.Client()


# Construct a reference to the "hacker_news" dataset
dataset_ref = client.dataset("hacker_news", project="bigquery-public-data")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

# List all the tables in the "hacker_news" dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there are four!)
for table in tables:  
	print(table.table_id)


# Construct a reference to the "full" table
table_ref = dataset_ref.table("full")

# API request - fetch the table
table = client.get_table(table_ref)

# Print information on all the columns in the "full" table in the "hacker_news" dataset
table.schema


# Preview the first five lines of the "full" table
client.list_rows(table, max_results=5).to_dataframe()

# Preview the first five entries in the "by" column of the "full" table
client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()

endsnippet

# options im-> inword
snippet BQ_query "for executing a sql command in big query"

from google.cloud import bigquery

# Create a "Client" object
client = bigquery.Client()

# Construct a reference to the "openaq" dataset
dataset_ref = client.dataset("openaq", project="bigquery-public-data")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

# List all the tables in the "openaq" dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there's only one!)
for table in tables:  
	print(table.table_id)

# Construct a reference to the "global_air_quality" table
table_ref = dataset_ref.table("global_air_quality")

# API request - fetch the table
table = client.get_table(table_ref)

# Preview the first five lines of the "global_air_quality" table
client.list_rows(table, max_results=5).to_dataframe()



#### now actually the query

# Query to select all the items from the "city" column where the "country" column is 'US'
query = """
	SELECT city
	FROM `bigquery-public-data.openaq.global_air_quality`
	WHERE country = 'US'
	"""

# Create a "Client" object
client = bigquery.Client()

# Set up the query
query_job = client.query(query)


# API request - run the query, and return a pandas DataFrame
us_cities = query_job.to_dataframe()

# What five cities have the most measurements?
us_cities.city.value_counts().head()




endsnippet

# options im-> inword
snippet BQ_big_queries "for executing a big query without exceding the limit"
### you can test wheter the query is going to be very huge


# Query to get the score column from every row where the type column has value "job"
query = """
	SELECT score, title
	FROM `bigquery-public-data.hacker_news.full`
	WHERE type = "job" 
	"""

# Create a QueryJobConfig object to estimate size of query without running it
dry_run_config = bigquery.QueryJobConfig(dry_run=True)

# API request - dry run query to estimate costs
dry_run_query_job = client.query(query, job_config=dry_run_config)

print("This query will process {} bytes.".format(dry_run_query_job.total_bytes_processed))



### especify the limit and execut the query
# Only run the query if it's less than 1 MB
ONE_MB = 1000*1000
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)

# Set up the query (will only run if it's less than 1 MB)
safe_query_job = client.query(query, job_config=safe_config)

# API request - try to run the query, and return a pandas DataFrame
safe_query_job.to_dataframe()

endsnippet


# options im-> inword
snippet BQ_groupBy_having_count ""
from google.cloud import bigquery

# Create a "Client" object
client = bigquery.Client()

# Construct a reference to the "hacker_news" dataset
dataset_ref = client.dataset("hacker_news", project="bigquery-public-data")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

# Construct a reference to the "comments" table
table_ref = dataset_ref.table("comments")

# API request - fetch the table
table = client.get_table(table_ref)

# Preview the first five lines of the "comments" table
client.list_rows(table, max_results=5).to_dataframe()


### this is where the code starts


# Query to select comments that received more than 10 replies
query_popular = """
	SELECT parent, COUNT(id)
	FROM `bigquery-public-data.hacker_news.comments`
	GROUP BY parent
	HAVING COUNT(id) > 10
	"""

# Set up the query (cancel the query if it would use too much of 
# your quota, with the limit set to 10 GB)
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
query_job = client.query(query_popular, job_config=safe_config)

# API request - run the query, and convert the results to a pandas DataFrame
popular_comments = query_job.to_dataframe()

# Print the first five rows of the DataFrame
popular_comments.head()



endsnippet


# options im-> inword
snippet BQ_orderBy "for working with the order by"
query = """
select id,name,animal
from `bigquery-public-data.pet_records.pets`
order by id
"""
endsnippet

# options im-> inword
snippet BQ_dates "for working with dates on big query"
# Query to find out the number of accidents for each day of the week
query = """
	SELECT COUNT(consecutive_number) AS num_accidents, 
		EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week
	FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`
	GROUP BY day_of_week
	ORDER BY num_accidents DESC
	"""
# also check the documentation for extract method

#########
##### convert the datetime to date
######
query_with_CTE = """ 
	WITH time AS 
	(
		SELECT DATE(block_timestamp) AS trans_date
		FROM `bigquery-public-data.crypto_bitcoin.transactions`
	)
	SELECT COUNT(1) AS transactions,
		trans_date
	FROM time
	GROUP BY trans_date
	ORDER BY trans_date
	"""

# Set up the query (cancel the query if it would use too much of 
# your quota, with the limit set to 10 GB)
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
query_job = client.query(query_with_CTE, job_config=safe_config)

# API request - run the query, and convert the results to a pandas DataFrame
transactions_by_date = query_job.to_dataframe()

# Print the first five rows
transactions_by_date.head()
endsnippet


# options im-> inword
snippet BQ_with_as "for nested queries better readability"

query = """
with seniors as 
(
	select id,name
	from `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`
	where years_old > 5
)
"""

select id
from seniors
endsnippet

# options im-> inword
snippet BQ_join "for join in big query"
# Query to determine the number of files per license, sorted by number of files
query = """
	SELECT L.license, COUNT(1) AS number_of_files
	FROM `bigquery-public-data.github_repos.sample_files` AS sf
	INNER JOIN `bigquery-public-data.github_repos.licenses` AS L 
		ON sf.repo_name = L.repo_name
	GROUP BY L.license
	ORDER BY number_of_files DESC
	"""

# Set up the query (cancel the query if it would use too much of 
# your quota, with the limit set to 10 GB)
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
query_job = client.query(query, job_config=safe_config)

# API request - run the query, and convert the results to a pandas DataFrame
file_count_by_license = query_job.to_dataframe()
endsnippet



# == advanced BQ
# options im-> inword
snippet BQ_left_join "for doing the big query left join"
# Query to select all stories posted on January 1, 2012, with number of comments
join_query = """
	WITH c AS
	(
	SELECT parent, COUNT(*) as num_comments
	FROM `bigquery-public-data.hacker_news.comments` 
	GROUP BY parent
	)
	SELECT s.id as story_id, s.by, s.title, c.num_comments
	FROM `bigquery-public-data.hacker_news.stories` AS s
	LEFT JOIN c
	ON s.id = c.parent
	WHERE EXTRACT(DATE FROM s.time_ts) = '2012-01-01'
	ORDER BY c.num_comments DESC
	"""

# Run the query, and return a pandas DataFrame
join_result = client.query(join_query).result().to_dataframe()
join_result.head()
endsnippet


# options im-> inword
snippet BQ_over_clause ""
# Query to count the (cumulative) number of trips per day
num_trips_query = """
	WITH trips_by_day AS
	(
	SELECT DATE(start_date) AS trip_date,
		COUNT(*) as num_trips
	FROM `bigquery-public-data.san_francisco.bikeshare_trips`
	WHERE EXTRACT(YEAR FROM start_date) = 2015
	GROUP BY trip_date
	)
	SELECT *,
		SUM(num_trips) 
			OVER (
				ORDER BY trip_date
				ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
				) AS cumulative_trips
		FROM trips_by_day
	"""

# Run the query, and return a pandas DataFrame
num_trips_result = client.query(num_trips_query).result().to_dataframe()
num_trips_result.head()


endsnippet

# options im-> inword
snippet  partition_by "for using the partition clause"
# Query to track beginning and ending stations on October 25, 2015, for each bike
start_end_query = """
	SELECT bike_number,
		TIME(start_date) AS trip_time,
		FIRST_VALUE(start_station_id)
			OVER (
				PARTITION BY bike_number
				ORDER BY start_date
				ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
				) AS first_station_id,
				E(end_station_id)
				(
				PARTITION BY bike_number
				ORDER BY start_date
				ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
				) AS last_station_id,
		start_station_id,
		end_station_id
	FROM `bigquery-public-data.san_francisco.bikeshare_trips`
	WHERE DATE(start_date) = '2015-10-25' 
	"""

# Run the query, and return a pandas DataFrame
start_end_result = client.query(start_end_query).result().to_dataframe()
start_end_result.head()
endsnippet


# options im-> inword
snippet BQ_unest "for unesting nested data or repeated data"

# Your code here
all_langs_query = """
	SELECT l.name, l.bytes
	FROM `bigquery-public-data.github_repos.languages`,
		UNNEST(language) as l
	WHERE repo_name = 'polyrabbit/polyglot'
	ORDER BY l.bytes DESC
	"""
endsnippet





# options im-> inword
snippet BQ_speed_functions "usable functions to see the efficiency of the queries"

from google.cloud import bigquery
from time import time

client = bigquery.Client()

def show_amount_of_data_scanned(query):
	# dry_run lets us see how much data the query uses without running it
	dry_run_config = bigquery.QueryJobConfig(dry_run=True)
	query_job = client.query(query, job_config=dry_run_config)
	print('Data processed: {} GB'.format(round(query_job.total_bytes_processed / 10**9, 3)))

def show_time_to_run(query):
	time_config = bigquery.QueryJobConfig(use_query_cache=False)
	start = time()
	query_result = client.query(query, job_config=time_config).result()
	end = time()
	print('Time to run: {} seconds'.format(round(end-start, 3)))

endsnippet

# === intro to deep learning

# options im-> inword
snippet model1LinearUnit "for creating a model with one linear unit"
from tensorflow import keras
from tensorflow.keras import layers

# Create a network with 1 linear unit
model = keras.Sequential([
	layers.Dense(units=1, input_shape=[3])
])
endsnippet


# options im-> inword
snippet getWeightsAndBiases "for getting weights and biases"
w, b = model.weights

print("Weights\n{}\n\nBias\n{}".format(w, b)
endsnippet

# options im-> inword
snippet plotOutputOfUntrained "for plotting the output of an untrained linear model"
import tensorflow as tf
import matplotlib.pyplot as plt

model = keras.Sequential([
	layers.Dense(1, input_shape=[1]),
])

x = tf.linspace(-1.0, 1.0, 100)
y = model.predict(x)

plt.figure(dpi=100)
plt.plot(x, y, 'k')
plt.xlim(-1, 1)
plt.ylim(-1, 1)
plt.xlabel("Input: x")
plt.ylabel("Target y")
w, b = model.weights # you could also use model.get_weights() here
plt.title("Weight: {:0.2f}\nBias: {:0.2f}".format(w[0][0], b[0]))
plt.show()
endsnippet

# options im-> inword
snippet buildSequencialModel "this is for creating a secuencial model"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	# the hidden ReLU layers
	layers.Dense(units=4, activation='relu', input_shape=[2]),
	layers.Dense(units=3, activation='relu'),
	# the linear output layer 
	layers.Dense(units=1),
])


endsnippet

# options im-> inword
snippet loss_optimizer "for adding a loss and the optimizer"
model.compile(
	optimizer="adam",
	loss="mae",
)

endsnippet

# options im-> inword
snippet train_dl "for training a deep learning model"
history = model.fit(
	X_train, y_train,
	validation_data=(X_valid, y_valid),
	batch_size=256,
	epochs=10,
)

endsnippet

# options im-> inword
snippet plotLosstrainingData "for plotting loss training data"
import pandas as pd

# convert the training history to a dataframe
history_df = pd.DataFrame(history.history)
# use Pandas native plot method
history_df['loss'].plot();
endsnippet

# options im-> inword
snippet animateDeepLearning "for animating a deep learnig project"
from learntools.deep_learning_intro.dltools import animate_sgd


# YOUR CODE HERE: Experiment with different values for the learning rate, batch size, and number of examples
learning_rate = 0.05
batch_size = 32
num_examples = 256

animate_sgd(
	learning_rate=learning_rate,
	batch_size=batch_size,
	num_examples=num_examples,
	# You can also change these, if you like
	steps=50, # total training steps (batches seen)
	true_w=3.0, # the slope of the data
	true_b=2.0, # the bias of the data
)
endsnippet


# options im-> inword
snippet earlyStopping "for getting an early stopping"

from tensorflow import keras
from tensorflow.keras import layers, callbacks

early_stopping = callbacks.EarlyStopping(
	min_delta=0.001, # minimium amount of change to count as an improvement
	patience=20, # how many epochs to wait before stopping
	restore_best_weights=True,
	
	
	l = keras.Sequential([
	layers.Dense(512, activation='relu', input_shape=[11]),
	layers.Dense(512, activation='relu'),
	layers.Dense(512, activation='relu'),
	layers.Dense(1),
	
	l.compile(
	optimizer='adam',
	loss='mae',
)


history = model.fit(
	X_train, y_train,
	validation_data=(X_valid, y_valid),
	batch_size=256,
	epochs=500,
	callbacks=[early_stopping], # put your callbacks in a list
	verbose=0,  # turn off training log
)

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
print("Minimum validation loss: {}".format(history_df['val_loss'].min()))

endsnippet




# options im-> inword
snippet dropout_batchNormalization "for using dropout and batch normalization"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Dense(1024, activation='relu', input_shape=[11]),
	layers.Dropout(0.3),
	layers.BatchNormalization(),
	layers.Dense(1024, activation='relu'),
	layers.Dropout(0.3),
	layers.BatchNormalization(),
	layers.Dense(1024, activation='relu'),
	layers.Dropout(0.3),
	layers.BatchNormalization(),
	layers.Dense(1),
])
endsnippet



# options im-> inword
snippet binaryClassification "for classifying in bynary"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Dense(4, activation='relu', input_shape=[33]),
	layers.Dense(4, activation='relu'),    
	layers.Dense(1, activation='sigmoid'),
])

model.compile(
	optimizer='adam',
	loss='binary_crossentropy',
	metrics=['binary_accuracy'],
)
# include early stopping
early_stopping = keras.callbacks.EarlyStopping(
	patience=10,
	min_delta=0.001,
	restore_best_weights=True,
)

history = model.fit(
	X_train, y_train,
	validation_data=(X_valid, y_valid),
	batch_size=512,
	epochs=1000,
	callbacks=[early_stopping],
	verbose=0, # hide the output because we have so many epochs
)



endsnippet


# options im-> inword
snippet define_pretrained_base "for defining the pre trained base"
pretrained_base = tf.keras.models.load_model(
	'../input/cv-course-models/cv-course-models/vgg16-pretrained-base',
)
pretrained_base.trainable = False
endsnippet

# options im-> inword
snippet attachHeAD "for attaching the head"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	pretrained_base,
	layers.Flatten(),
	layers.Dense(6, activation='relu'),
	layers.Dense(1, activation='sigmoid'),
])
endsnippet


# options im-> inword
snippet CV_train "for training a simple computer vision algoritm"
model.compile(
	optimizer='adam',
	loss='binary_crossentropy',
	metrics=['binary_accuracy'],
)

history = model.fit(
	ds_train,
	validation_data=ds_valid,
	epochs=30,
	verbose=0,
)
endsnippet

# options im-> inword
snippet CV_plot_acc "for plotting the binary accuracy"
import pandas as pd

history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();
endsnippet



# options im-> inword
snippet filterWithConvulsion "for filtering with convulsion"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Conv2D(filters=64, kernel_size=3), # activation is None
	# More layers follow
])
endsnippet


# options im-> inword
snippet relu_activation "for activating the relu with te conv2d"
model = keras.Sequential([
	layers.Conv2D(filters=64, kernel_size=3, activation='relu')
	# More layers follow
])
endsnippet

# options im-> inword
snippet CV_tfKernel "for defining a basic tensorflow kernel"
kernel = tf.constant([
	[-2, -1, 0],
	[-1, 1, 1],
	[0, 1, 2],
])
endsnippet

# options im-> inword
snippet condenseWithMaximumPooling "adding condense with maximum pooling"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Conv2D(filters=64, kernel_size=3), # activation is None
	layers.MaxPool2D(pool_size=2),
	# More layers follow
])
endsnippet


# options im-> inword
snippet imageCondensePool "for image condensing a pool"
import tensorflow as tf

image_condense = tf.nn.pool(
	input=image_detect, # image in the Detect step above
	window_shape=(2, 2),
	pooling_type='MAX',
	# we'll see what these do in the next lesson!
	strides=(2, 2),
	padding='SAME',
)

plt.figure(figsize=(6, 6))
plt.imshow(tf.squeeze(image_condense))
plt.axis('off')
plt.show();
endsnippet


# options im-> inword
snippet globalAveragePooling ""
feature_maps = [visiontools.random_map([5, 5], scale=0.1, decay_power=4) for _ in range(8)]

gs = gridspec.GridSpec(1, 8, wspace=0.01, hspace=0.01)
plt.figure(figsize=(18, 2))
for i, feature_map in enumerate(feature_maps):
	plt.subplot(gs[i])
	plt.imshow(feature_map, vmin=0, vmax=1)
	plt.axis('off')
plt.suptitle('Feature Maps', size=18, weight='bold', y=1.1)
plt.show()

# reformat for TensorFlow
feature_maps_tf = [tf.reshape(feature_map, [1, *feature_map.shape, 1])
	for feature_map in feature_maps]

global_avg_pool = tf.keras.layers.GlobalAvgPool2D()
pooled_maps = [global_avg_pool(feature_map) for feature_map in feature_maps_tf]
img = np.array(pooled_maps)[:,:,0].T

plt.imshow(img, vmin=0, vmax=1)
plt.axis('off')
plt.title('Pooled Feature Maps')
plt.show();
endsnippet


# options im-> inword
snippet sliding_window "for the sliding window tecniques padding and stride"
show_extraction(
	image, kernel,
	
	# Window parameters
	conv_stride=1,
	pool_size=2,
	pool_stride=2,
	
	subplot_shape=(1, 4),
	figsize=(14, 6),
)
endsnippet

# options im-> inword
snippet customConvents "for defining a custom convent"
### define the model
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([

	# First Convolutional Block
	layers.Conv2D(filters=32, kernel_size=5, activation="relu", padding='same',
		# give the input dimensions in the first layer
		# [height, width, color channels(RGB)]
		input_shape=[128, 128, 3]),
	layers.MaxPool2D(),
	
	# Second Convolutional Block
	layers.Conv2D(filters=64, kernel_size=3, activation="relu", padding='same'),
	layers.MaxPool2D(),
	
	# Third Convolutional Block
	layers.Conv2D(filters=128, kernel_size=3, activation="relu", padding='same'),
	layers.MaxPool2D(),
	
	# Classifier Head
	layers.Flatten(),
	layers.Dense(units=6, activation="relu"),
	layers.Dense(units=1, activation="sigmoid"),
])
model.summary()

#### train the model
model.compile(
	optimizer=tf.keras.optimizers.Adam(epsilon=0.01),
	loss='binary_crossentropy',
	metrics=['binary_accuracy']
)

history = model.fit(
	ds_train,
	validation_data=ds_valid,
	epochs=40,
	verbose=0,
)

#### test the model
import pandas as pd

history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();
endsnippet



# options im-> inword
snippet dataAugmentation "for adding features to the cv model"
from tensorflow import keras
from tensorflow.keras import layers
# these are a new feature in TF 2.2
from tensorflow.keras.layers.experimental import preprocessing


pretrained_base = tf.keras.models.load_model(
	'../input/cv-course-models/cv-course-models/vgg16-pretrained-base',
)
pretrained_base.trainable = False

model = keras.Sequential([
	# Preprocessing
	preprocessing.RandomFlip('horizontal'), # flip left-to-right
	preprocessing.RandomContrast(0.5), # contrast change by up to 50%
	# Base
	pretrained_base,
	# Head
	layers.Flatten(),
	layers.Dense(6, activation='relu'),
	layers.Dense(1, activation='sigmoid'),
])
endsnippet


# options im-> inword
snippet RL_setup "for the setup on reinforcement learnig"
from kaggle_environments import make, evaluate

# Create the game environment
# Set debug=True to see the errors if your agent refuses to run
env = make("connectx", debug=True)

# List of available default agents
print(list(env.agents))
endsnippet

# options im-> inword
snippet definingAgents "for defining agents for the competition"
# Selects random valid column
def agent_random(obs, config):
	valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]
	return random.choice(valid_moves)

# Selects middle column
def agent_middle(obs, config):
	return config.columns//2

# Selects leftmost valid column
def agent_leftmost(obs, config):
	valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]
	return valid_moves[0]
endsnippet



# options im-> inword
snippet fitLinearRegression "for fitting a linear regression model from a forecasting task"
from sklearn.linear_model import LinearRegression

# Training data
X = df.loc[:, ['Time']]  # features
y = df.loc[:, 'NumVehicles']  # target

# Train the model
model = LinearRegression()
model.fit(X, y)

# Store the fitted values as a time series with the same time index as
# the training data
y_pred = pd.Series(model.predict(X), index=X.index)
endsnippet

# options im-> inword
snippet lagFeature "for adding a lag feature to a pandas dataframe"
df['Lag_1'] = df['NumVehicles'].shift(1)
df.head()
endsnippet

# options im-> inword
snippet moving_average_plot "for a moving average plot"
moving_average = tunnel.rolling(
	window=365,       # 365-day window
	center=True,      # puts the average at the center of the window
	min_periods=183,  # choose about half the window size
).mean()              # compute the mean (could also do median, std, min, max, ...)

ax = tunnel.plot(style=".", color="0.5")
moving_average.plot(
	ax=ax, linewidth=3, title="Tunnel Traffic - 365-Day Moving Average", legend=False,
);
endsnippet

# options im-> inword
snippet splines "an alternative to polinomials that is better"
from pyearth import Earth

# Target and features are the same as before
y = average_sales.copy()
dp = DeterministicProcess(index=y.index, order=1)
X = dp.in_sample()

# Fit a MARS model with `Earth`
model = Earth()
model.fit(X, y)

y_pred = pd.Series(model.predict(X), index=X.index)

ax = y.plot(**plot_params, title="Average Sales", ylabel="items sold")
ax = y_pred.plot(ax=ax, linewidth=3, label="Trend")
endsnippet

# options im-> inword
snippet plotSeasonability "for plotting seasonability"
X = tunnel.copy()

# days within a week
X["day"] = X.index.dayofweek  # the x-axis (freq)
X["week"] = X.index.week  # the seasonal period (period)

# days within a year
X["dayofyear"] = X.index.dayofyear
X["year"] = X.index.year
fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))
seasonal_plot(X, y="NumVehicles", period="week", freq="day", ax=ax0)
seasonal_plot(X, y="NumVehicles", period="year", freq="dayofyear", ax=ax1);

plot_periodogram(tunnel.NumVehicles);

endsnippet




# options im-> inword
snippet seasonalFeatures "for creating seasonal features"
from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess

fourier = CalendarFourier(freq="A", order=10)  # 10 sin/cos pairs for "A"nnual seasonality

dp = DeterministicProcess(
	index=tunnel.index,
	constant=True,               # dummy feature for bias (y-intercept)
	order=1,                     # trend (order 1 means linear)
	seasonal=True,               # weekly seasonality (indicators)
	additional_terms=[fourier],  # annual seasonality (fourier)
	drop=True,                   # drop terms to avoid collinearity
)

X = dp.in_sample()  # create features for dates in tunnel.index

endsnippet


# options im-> inword
snippet hibrid_forecasting "for creating hibrid models"
# 1. Train and predict with first model
model_1.fit(X_train_1, y_train)
y_pred_1 = model_1.predict(X_train)

# 2. Train and predict with second model on residuals
model_2.fit(X_train_2, y_train - y_pred_1)
y_pred_2 = model_2.predict(X_train_2)

# 3. Add to get overall predictions
y_pred = y_pred_1 + y_pred_2


# The `stack` method converts column labels to row labels, pivoting from wide format to long
X = retail.stack()  # pivot dataset wide to long
display(X.head())
y = X.pop('Sales')  # grab target series


# Turn row labels into categorical feature columns with a label encoding
X = X.reset_index('Industries')
# Label encoding for 'Industries' feature
for colname in X.select_dtypes(["object", "category"]):
	X[colname], _ = X[colname].factorize()

# Label encoding for annual seasonality
X["Month"] = X.index.month  # values are 1, 2, ..., 12

# Create splits
X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]
y_train, y_test = y.loc[idx_train], y.loc[idx_test]


# Pivot wide to long (stack) and convert DataFrame to Series (squeeze)
y_fit = y_fit.stack().squeeze()    # trend from training set
y_pred = y_pred.stack().squeeze()  # trend from test set

# Create residuals (the collection of detrended series) from the training set
y_resid = y_train - y_fit

# Train XGBoost on the residuals
xgb = XGBRegressor()
xgb.fit(X_train, y_resid)

# Add the predicted residuals onto the predicted trends
y_fit_boosted = xgb.predict(X_train) + y_fit
y_pred_boosted = xgb.predict(X_test) + y_pred
endsnippet

# options im-> inword
snippet GP_setup "for setting up a basic geopandas instance"
# Read in the data
full_data = gpd.read_file("../input/geospatial-learn-course-data/DEC_lands/DEC_lands/DEC_lands.shp")

# View the first five rows of the data
full_data.head()

type(full_data)

data = full_data.loc[:, ["CLASS", "COUNTY", "geometry"]].copy()

# How many lands of each type are there?
data.CLASS.value_counts()

# Select lands that fall under the "WILD FOREST" or "WILDERNESS" category
wild_lands = data.loc[data.CLASS.isin(['WILD FOREST', 'WILDERNESS'])].copy()
wild_lands.head()


wild_lands.plot()


# View the first five entries in the "geometry" column
wild_lands.geometry.head()


# Campsites in New York state (Point)
POI_data = gpd.read_file("../input/geospatial-learn-course-data/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp")
campsites = POI_data.loc[POI_data.ASSET=='PRIMITIVE CAMPSITE'].copy()

# Foot trails in New York state (LineString)
roads_trails = gpd.read_file("../input/geospatial-learn-course-data/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp")
trails = roads_trails.loc[roads_trails.ASSET=='FOOT TRAIL'].copy()

# County boundaries in New York state (Polygon)
counties = gpd.read_file("../input/geospatial-learn-course-data/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp")

# Define a base map with county boundaries
ax = counties.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)

# Add wild lands, campsites, and foot trails to the base map
wild_lands.plot(color='lightgreen', ax=ax)
campsites.plot(color='maroon', markersize=2, ax=ax)
trails.plot(color='black', markersize=1, ax=ax)

endsnippet

# options im-> inword
snippet interactive_map "for creating an interactive map"
import folium
from folium import Choropleth, Circle, Marker
from folium.plugins import HeatMap, MarkerCluster

# Create a map
m_1 = folium.Map(location=[42.32,-71.0589], tiles='openstreetmap', zoom_start=10)

# Display the map
m_1

## plotting points

daytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == 'Robbery') & \(crimes.HOUR.isin(range(9,18))))]




# add the marquers
# Create a map
m_2 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)

# Add points to the map
for idx, row in daytime_robberies.iterrows():
	Marker([row['Lat'], row['Long']]).add_to(m_2)

# Display the map
m_2

###
## for adding a marker cluster instead
##
# Create the map
m_3 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)

# Add points to the map
mc = MarkerCluster()
for idx, row in daytime_robberies.iterrows():
	if not math.isnan(row['Long']) and not math.isnan(row['Lat']):
		mc.add_child(Marker([row['Lat'], row['Long']]))
m_3.add_child(mc)

# Display the map
m_3

##
# for adding classic points
##

# Create a base map
m_4 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)

def color_producer(val):
	if val <= 12:
		return 'forestgreen'
	else:
		return 'darkred'

# Add a bubble map to the base map
for i in range(0,len(daytime_robberies)):
	Circle(
		location=[daytime_robberies.iloc[i]['Lat'], daytime_robberies.iloc[i]['Long']],
		radius=20,
		color=color_producer(daytime_robberies.iloc[i]['HOUR'])).add_to(m_4)

# Display the map
m_4


##
# for adding a heat map
##

# Create a base map
m_5 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)

# Add a heatmap to the base map
HeatMap(data=crimes[['Lat', 'Long']], radius=10).add_to(m_5)

# Display the map
m_5



endsnippet


# options im-> inword
snippet choropeth_i_map "for creating a choroplet map"
# GeoDataFrame with geographical boundaries of Boston police districts
districts_full = gpd.read_file('../input/geospatial-learn-course-data/Police_Districts/Police_Districts/Police_Districts.shp')
districts = districts_full[["DISTRICT", "geometry"]].set_index("DISTRICT")
districts.head()


# Number of crimes in each police district
plot_dict = crimes.DISTRICT.value_counts()
plot_dict.head()

# Create a base map
m_6 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)

# Add a choropleth map to the base map
Choropleth(geo_data=districts.__geo_interface__, 
	data=plot_dict, 
	key_on="feature.id", 
	fill_color='YlGnBu', 
	legend_name='Major criminal incidents (Jan-Aug 2018)'
	).add_to(m_6)

# Display the map
m_6
endsnippet



# options im-> inword
snippet circles_choropeth_i_map "for creating a choroplet map that has empty circles of differenct sizes pointing at density results"
# Create a base map
m_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)

# Create a map
def color_producer(magnitude):
	if magnitude > 6.5:
		return 'red'
	else:
		return 'green'

Choropleth(
	geo_data=prefectures['geometry'].__geo_interface__,
	data=stats['density'],
	key_on="feature.id",
	fill_color='BuPu',
	legend_name='Population density (per square kilometer)').add_to(m_4)

for i in range(0,len(earthquakes)):
	folium.Circle(
		location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],
		popup=("{} ({})").format(
			earthquakes.iloc[i]['Magnitude'],
			earthquakes.iloc[i]['DateTime'].year),
		radius=earthquakes.iloc[i]['Magnitude']**5.5,
		color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)

# Uncomment to see a hint
#q_4.a.hint()

# View the map
embed_map(m_4, 'q_4.html')
endsnippet



# options im-> inword
snippet geoencoder_map "for creating locations in a map"
from geopy.geocoders import Nominatim


geolocator = Nominatim(user_agent="kaggle_learn")
location = geolocator.geocode("Pyramid of Khufu")

print(location.point)
print(location.address)

point = location.point
print("Latitude:", point.latitude)
print("Longitude:", point.longitude)

universities = pd.read_csv("../input/geospatial-learn-course-data/top_universities.csv")
universities.head()

def my_geocoder(row):
	try:
		point = geolocator.geocode(row).point
		return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})
	except:
		return None

universities[['Latitude', 'Longitude']] = universities.apply(lambda x: my_geocoder(x['Name']), axis=1)

print("{}% of addresses were geocoded!".format(
	(1 - sum(np.isnan(universities["Latitude"])) / len(universities)) * 100))

# Drop universities that were not successfully geocoded
universities = universities.loc[~np.isnan(universities["Latitude"])]
universities = gpd.GeoDataFrame(
	universities, geometry=gpd.points_from_xy(universities.Longitude, universities.Latitude))
universities.crs = {'init': 'epsg:4326'}
universities.head()



##
# Create a map
##
m = folium.Map(location=[54, 15], tiles='openstreetmap', zoom_start=2)

# Add points to the map
for idx, row in universities.iterrows():
	Marker([row['Latitude'], row['Longitude']], popup=row['Name']).add_to(m)

# Display the map
m
endsnippet

# options im-> inword
snippet measure_distance "for measuring a distance in a map"
# measure the distance incident and every station
# Select one release incident in particular
recent_release = releases.iloc[360]

# Measure distance from release to each station
distances = stations.geometry.distance(recent_release.geometry)
distances
endsnippet


# options im-> inword
snippet create_a_buffer "for creating a buffer in geopandas"
two_mile_buffer = stations.geometry.buffer(2*5280)
two_mile_buffer.head()

# Create map with release incidents and monitoring stations
m = folium.Map(location=[39.9526,-75.1652], zoom_start=11)
HeatMap(data=releases[['LATITUDE', 'LONGITUDE']], radius=15).add_to(m)
for idx, row in stations.iterrows():
	Marker([row['LATITUDE'], row['LONGITUDE']]).add_to(m)

# Plot each polygon on the map
GeoJson(two_mile_buffer.to_crs(epsg=4326)).add_to(m)

# Show the map
m
endsnippet




# options im-> inword
snippet test_score_features "for testing the score of the features in your model"
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')
y = (data['Man of the Match'] == "Yes")  # Convert from string "Yes"/"No" to binary
feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]
X = data[feature_names]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
my_model = RandomForestClassifier(n_estimators=100,random_state=0).fit(train_X, train_y)

import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)
eli5.show_weights(perm, feature_names = val_X.columns.tolist())



endsnippet

# options im-> inword
snippet visualize_tree_sqlearn "for visualizing a tree in sqlearn"
# %%
# Extract single tree
estimator = last_clf.estimators_[5]

from sklearn.tree import export_graphviz
# Export as dot file
export_graphviz(estimator, out_file='tree.dot', 
                rounded = True, proportion = False, 
                precision = 2, filled = True)

# Convert to png using system command (requires Graphviz)
from subprocess import call
call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])

# %%
Image(filename = 'tree.png')
endsnippet



# options im-> inword
snippet sk_tree_lc "for leetcode trees"

# Date: `date +%d/%B/%Y\ -\ %A`
# Author: Virgilio Murillo Ochoa
# personal github: Virgilio-AI
# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
# contact: virgiliomurilloochoa1@gmail.com
# web: virgiliomurillo.com

from typing import *
import heapq as hp
from collections import deque
from collections import defaultdict
import sys

# dic = defaultdict(int)
# set = set() # .remove(val),.add(val)
# dic = {} # .remove(id),dic[key] = val
# arr = []

class TreeNode:
	def __init__(self, x):
		self.val = x
		self.left = None
		self.right = None
# main part


# build the tree
def tree_builder(values):
	if not values:
		return None
	root = TreeNode(values[0])
	queue = collections.deque([root])
	leng = len(values)
	nums = 1
	while nums < leng:
		node = queue.popleft()
		if node:
			node.left = TreeNode(values[nums]) if values[nums] else None
			queue.append(node.left)
			if nums + 1 < leng:
				node.right = TreeNode(values[nums+1]) if values[nums+1] else None
				queue.append(node.right)
				nums += 1
			nums += 1
	return root

# def input(f=open(".`!p snip.rv = snip.basename`_In1.txt")): return f.readline().rstrip() # uncomment for debugging


def main():
	arr = [1,2,3,None]
	tree = tree_builder(arr)
	# this lines are for testing the solution locally
	# Example:
	# a = Solution()
	# b = a.Function(tree)
	# print(b)


	$1


if __name__ == '__main__':
	main()
endsnippet

snippet info_firenvim



# Date: `date +%d/%B/%Y\ -\ %A`
# Author: Virgilio Murillo Ochoa
# personal github: Virgilio-AI
# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
# contact: data_scientist@virgiliomurillo.com
# web: virgiliomurillo.com


# dic = defaultdict(int)
# set = set() # .remove(val),.add(val),.discard(val),.pop(),.clear()
# dic = {} # .remove(id),dic[key] = val, dic.get(key,0)
# arr = [] # .append(val),.pop(),.remove(val),.sort(),.reverse(),.insert(pos,val),.clear()


endsnippet

snippet info
# Date: `date +%d/%B/%Y\ -\ %A`
# Author: Virgilio Murillo Ochoa
# personal github: Virgilio-AI
# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
# contact: data_scientist@virgiliomurillo.com
# web: virgiliomurillo.com
endsnippet



# ==========================
# ========== python bases de datos======
# ==========================
# options im-> inword
snippet sqlite3 "for connection into a sqlite3 database"
conexion = sqlite3.connect('pruebas.db')
endsnippet
# options im-> inword
snippet sqlite3c "for executing a command in sqlite3"

cursor = conexion.cursor()
command = """
"""
# execute command

cursor.execute(command)
# if query
# rows = cursor.fetchall()
# for row in rows():
#	print(row)
endsnippet


# options im-> inword
snippet mysql "for creating a connection to sql"
import mysql.connector

# conexion

database =  mysql.connector.connect(
		host="${1:localhost}",
		user = "$2",
		passwd="$3",
		database="${4:automation_suite}",
		)

endsnippet
# options im-> inword
snippet mysqlc "for executing a command"
cursor = database.cursor(buffered,True)

cursor.execute(${1:command})
# cursor.execute("string for formatted %s",tuple of values)
endsnippet
# options im-> inword
snippet mysqlq "for a query in mysql"
cursor.execute("${1:select * from Event ;}")
${2:rows} = cursor.fetchall()
endsnippet
# options im-> inword
snippet rowcount "for counting the affected rows by the commit"
#database.commit()
num = cursor.rowcount()
endsnippet

# options im-> inword
snippet encryption "for encripting a password"
# import hashlib
cifrado = hashlib.sha256()
cifrado.update(${1:string}.encode('utf8'))
endsnippet



# ==========================
# ========== BIG DATA ======
# ==========================
# options im-> inword
snippet spark "init spark configurations"
spark = SparkSession.builder.appName("Operations").getOrCreate()
endsnippet

# options im-> inword
snippet smergeDataset "merging two datasets with the same columns"

endsnippet

# options im-> inword
snippet sinfo "for getting info on spark dataframe"
df.printSchema()
endsnippet

# options im-> inword
snippet sread_csv "for reading a csv file"
df = spark.read.csv('appl_stock.csv',inferSchema=True,header=True)
endsnippet

# options im-> inword
snippet sread_json "for reading a json file"
df = spark.read.json('people.json')
endsnippet

# options im-> inword
snippet sfilter "for filtering a dataframe"
# python syntaxt
# MAKE SURE TO ADD IN THE PARENTHESIS SEPARATING THE STATEMENTS!
df.filter( (df["Close"] < 200) & (df['Open'] > 200) ).show()

# sql syntax
# Using SQL
df.filter("Close<500").show()
# %%
# Using SQL with .select()
df.filter("Close<500").select('Open').show()
endsnippet


# options im-> inword
snippet createSquema "for creating a squema"
from pyspark.sql.types import StructField,StringType,IntegerType,StructType

# %%
data_schema = [StructField("age", IntegerType(), True),StructField("name", StringType(), True)]

# %%
final_struc = StructType(fields=data_schema)

# %%
df = spark.read.json('people.json', schema=final_struc)

# %%
df.printSchema()
endsnippet

# options im-> inword
snippet ssql "for sql queries on the table"
# Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView("people")

# %%
sql_results = spark.sql("SELECT * FROM people")

# %%
sql_results

# %%
sql_results.show()
endsnippet

# options im-> inword
snippet sdatetime "for datetime functions"
from pyspark.sql.functions import format_number,dayofmonth,hour,dayofyear,month,year,weekofyear,date_format

# %%
df.select(dayofmonth(df['Date'])).show()

# %%
df.select(hour(df['Date'])).show()

# %%
df.select(dayofyear(df['Date'])).show()

# %%
df.select(month(df['Date'])).show()

# %%
df.select(year(df['Date'])).show()

# %%
df.withColumn("Year",year(df['Date'])).show()

# %%
newdf = df.withColumn("Year",year(df['Date']))
newdf.groupBy("Year").mean()[['avg(Year)','avg(Close)']].show()

endsnippet


# options im-> inword
snippet sgroupby "groupby"

df.groupBy("Company").mean().show()

# %%
# Count
df.groupBy("Company").count().show()

# %%
# Max
df.groupBy("Company").max().show()

# %%
# Min
df.groupBy("Company").min().show()

# %%
# Sum
df.groupBy("Company").sum().show()
endsnippet

# options im-> inword
snippet smissing_data "for missing data"
# You can use the .na functions for missing data. The drop command has the following parameters:
#
#     df.na.drop(how='any', thresh=None, subset=None)
#     
#     * param how: 'any' or 'all'.
#     
#         If 'any', drop a row if it contains any nulls.
#         If 'all', drop a row only if all its values are null.
#     
#     * param thresh: int, default None
#     
#         If specified, drop rows that have less than `thresh` non-null values.
#         This overwrites the `how` parameter.
#         
#     * param subset: 
#         optional list of column names to consider.
df.na.drop().show()
# Has to have at least 2 NON-null values
df.na.drop(thresh=2).show()
df.na.drop(subset=["Sales"]).show()
df.na.drop(how='any').show()
df.na.drop(how='all').show()
df.na.fill('NEW VALUE').show()
df.na.fill(0).show()
df.na.fill('No Name',subset=['Name']).show()
# A very common practice is to fill values with the mean value for the column, for example:
from pyspark.sql.functions import mean
mean_val = df.select(mean(df['Sales'])).collect()
mean_val[0][0]
mean_sales = mean_val[0][0]
df.na.fill(mean_sales,["Sales"]).show()
# One (very ugly) one-liner
df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()
endsnippet


# options im-> inword
snippet sapplyFunction "for applying function to dataframe"

# Create custom function
def upperCase(str):
	return str.upper()

# Convert function to udf
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType
upperCaseUDF = udf(lambda x:upperCase(x),StringType()) 

# Custom UDF with withColumn()
df.withColumn("Cureated Name", upperCaseUDF(col("Name"))) \
.show(truncate=False)

# Custom UDF with select()  
df.select(col("Seqno"), \
upperCaseUDF(col("Name")).alias("Name") ) \
.show(truncate=False)

# Custom UDF with sql()
spark.udf.register("upperCaseUDF", upperCaseUDF)
df.createOrReplaceTempView("TAB")
spark.sql("select Seqno, Name, upperCaseUDF(Name) from TAB") \
.show()  

endsnippet
# ==========================
# ========== python django ======
# ==========================

# options im-> inword
snippet createProject "for creating a django project"
# django-admin startproject mysite

endsnippet
# options im-> inword
snippet migrate "for creating a migration with databases"
# python manage.py makemigrations
# python manage.py sqlmigrate ${1:appName} ${2:migration_number ex:0001}
# python manage.py migrate

endsnippet
# options im-> inword
snippet commonPackages "for a list of common package"
django-ckeditor
mysqlclient
pillow
endsnippet
# options im-> inword
snippet createApp "for creating a django app"
# type in the terminal
python manage.py startapp polls

# in the main file settings.py add the name of the app in installed apps
# if apps.py is modified use the name in default_auto_field instead
INSTALLED_APPS = [
'$1'
]
endsnippet


# options im-> inword
snippet useMysql "for using mysql as default in django"
DATABASES = {
		'default': {
			'ENGINE' : 'django.db.backends.mysql',
			'NAME' : "django_course_second_project",
			'USER' : 'test_django',
			'PASSWORD' : '' ,
			'HOST' : 'localhost',
			'PORT' : ''
			}
		}
endsnippet

# options im-> inword
snippet changeLanguage "for changing the default language for the app"
# in the project settings
# LANGUAGE_CODE = 'en-en'
# LANGUAGE_CODE = 'en-es'

endsnippet

# options im-> inword
snippet addContextProcessor "these are used for global funcitons inside the html files"
# inside settings.py use the following
# inside the main folder add in the TEMPLATES=[
{
	'OPTIONS':{
		'context_processors':[
			'django.template.context_processors.debug',
			'django.template.context_processors.request',
			'django.contrib.auth.context_processors.auth',
			'django.contrib.messages.context_processors.messages',
			+++++++++++++++++++++++++++++++++
			'${1:appName}.${2:nameOfContext}.${3:name_of_function}'
		],
		}]

# now inside the context_processors.py add the following
from .models import Page


# and example would be

# def get_pages(request):
# 
# 	temp = Page.objects.values_list('id','title','slug','visible').order_by('order')
# 	pages = []
# 	for i in range(len(temp)):
# 		if temp[i][3] == True:
# 			pages.append(temp[i])
# 
# 
# 	return {
# 			'pages' : pages
# 			}

endsnippet




# options im-> inword
snippet importUrl "for importing the url file from an other app into the main app"
# from django.contrib import admin
# from django.urls import path,include
# from django.conf import settings
# 
# urlpatterns = [
# 	path('',include('${1:appName}.urls')),
# ]
endsnippet


# options im-> inword
snippet viewsHelp "for help in the views file"

# # for rendering html and to redirect into an other page
# from django.shortcuts import render,redirect
# # for poping messages
# from django.contrib import messages
# # for creating a user fast and easy
# from django.contrib.auth.forms import UserCreationForm
# # for creating a register form
# from MainApp.forms import RegisterForm
# # for loggin in and logging out
# from django.contrib.auth import authenticate,login,logout

# to create a view just add
def name_of_pages(request):
param1 = "param1"
return render(request,'path/inside/templates/folder',
{
	'param1_title':param1,
	'param2_title':"param2"
})

# for python tag interpolation use {{ var|safe }}
endsnippet

# options im-> inword
snippet login_page "for creating a basic login page"

# check the log in snippet for html

#from django.shortcuts import render, redirect
#from django.contrib.auth.forms import UserCreationForm,AuthenticationForm
#
#from django.contrib.auth.models import User
#from django.contrib.auth import login,logout,authenticate
#
#from django.db import IntegrityError
#
#def loginuser(request):
#	dic = {}
#	dic['form'] = AuthenticationForm()
#	if request.method == 'GET':
#		return render(request,'todo/loginuser.html',dic)
#	else:
#		user = authenticate(request,username =request.POST['username'],password=request.POST['password'])
#		if user is None:
#			dic['error'] = "username and password did not match"
#		else:
#			login(request,user)
#			return redirect('home')
#
#		return render(request,'todo/loginuser.html',dic)


endsnippet


# options im-> inword
snippet log_out "for logging out from a page"

#from django.shortcuts import render, redirect
#from django.contrib.auth.forms import UserCreationForm,AuthenticationForm
#
#from django.contrib.auth.models import User
#from django.contrib.auth import login,logout,authenticate
#
#from django.db import IntegrityError

#def logoutuser(request):
#	if request.method == 'POST':
#		logout(request)
#		return redirect('home')

endsnippet

# options im-> inword
snippet appsHelp "for help on the MainAppFolder"
# from django.apps import AppConfig
# 
# 
# class MainappConfig(AppConfig):
# 	default_auto_field = 'django.db.models.BigAutoField'
# 	name = 'MainApp'
# then on the INSTALLED_APPS use the default_auto_field name
endsnippet

# options im-> inword
snippet urlsHelp "for help in importing directories"
# first create the file urls in the app

# import the views from the same directorie

# from . import views
# from django.urls import path
#
# urlpatterns = [
#		path('inicio/',views.index,name='index'),
#		]

endsnippet

# options im-> inword
snippet forms_help "for registering a user in the database"
# from django import forms
# 
# from django.core import validators
# 
# from django.contrib.auth.forms import UserCreationForm
# from django.contrib.auth.models import User
# 
# class RegisterForm(UserCreationForm):
# 	class Meta:
# 		model = User
# 		fields = ['username','email','first_name','last_name','password1','password2']
endsnippet

# options im-> inword
snippet html_template "for creating an html template"
# inside the MainAppFolder
# create the folder templates/layouts/
# create the file layout.html and use the snippet html_template
# create the file static/css for the right design in the page


# create the pages app and inside it create the context_processors.py file
inside it place 

# from pages.models import Page
# 
# def get_pages(request):
# 
# 	temp = Page.objects.values_list('id','title','slug','visible').order_by('order')
# 	pages = []
# 	for i in range(len(temp)):
# 		if temp[i][3] == True:
# 			pages.append(temp[i])
# 
# 
# 	return {
# 			'pages' : pages
# 			}

to get the pages glbally for the whole project inside html files
endsnippet

# options im-> inword
snippet register "for registering "

##signupuser Create your views here.
#def signupuser(request):
#	dic = {}
#	dic['form'] = UserCreationForm()
#	if request.method == 'GET':
#		return render(request,'todo/signupuser.html',dic)
#	else:
#		if request.POST['password1'] == request.POST['password2']:
#			try:
#				user = User.objects.create_user(request.POST['username'],password=request.POST['password1'])
#				user.save()
#				login(request,user)
#				return redirect('currenttodos')
#
#			except IntegrityError:
#				dic['error'] = "this user already exists in the database"
#				return render(request,'todo/signupuser.html',dic)
#		else:
#			dic['error'] = "Passwords didn't match"
#			return render(request,'todo/signupuser.html',dic)
#
# use the html snippet for registering
endsnippet

# options im-> inword
snippet getSqlObject "for getting an sql object"

# Create your views here.
from .models import ${1:the_table_class}

# inside a view
	${2:objectName} = $1.objects.get(${3:condition})
endsnippet

# options im-> inword
snippet django_createTable "for creating a table"

from ckeditor.fields import RichTextField
# import the user table
from django.contrib.auth.models import User

# # table named Category 
# class Category(models.Model):
# 	name = models.CharField(max_length=100,verbose_name="Nombre")
# 	description = models.CharField(max_length=255,verbose_name = "Description")
# 	created_at = models.DateTimeField(auto_now_add=True,verbose_name = "Descripción")
# 
# 	# the displayed information in django admin page
# 	class Meta:
# 		verbose_name = "Categoria"
# 		verbose_name_plural = "categorias"
# 	
# 	# the name to be printed
# 	def __str__(self):
# 		return self.name
# 
# class Article(models.Model):
# 	title = models.CharField(max_length=150,verbose_name="Título")
# 	content = RichTextField(verbose_name="Contenido")
# 	image = models.ImageField(default='null',verbose_name="Imagen",upload_to="articles")
# 	public = models.BooleanField(verbose_name="publicado?")
# 	# foreign key to create relationship many to one
# 	user = models.ForeignKey(User,editable=False,verbose_name="Usuario",on_delete=models.CASCADE)
# 	# many to many relationship between Category and Article
# 	categories = models.ManyToManyField(Category,verbose_name='Categorias',blank=True)
# 
# 	created_at = models.DateTimeField(auto_now_add=True,verbose_name="Creado el")
# 	updated_at = models.DateTimeField(auto_now=True,verbose_name="última actualización")
# 
# 
# 	class Meta:
# 		verbose_name = "Articulo"
# 		verbose_name_plural = "Artículos"
# 	
# 	def __str__(self):
# 		return self.title
endsnippet

# options im-> inword
snippet loginRequired "require a log in for a page"

from django.contrib.auth.decorators import login_required

@login_required(login_url="login")
# def page(request,slug):
# 	page = Page.objects.get(slug=slug)
# 	return render(request,"pages/page.html",{
# 		"page": page
# 		}
# )

endsnippet

# options im-> inword
snippet modClassDisplay "for modifications on the django panel search and many others"
# from .models import Category
# class CategoryAdmin(admin.ModelAdmin):
# 	readonly_fieds = ('created_at','updated_at')
# 	search_fields = ('name','description')
# 	list_display = ('name','created_at')
# class ArticleAdmin(admin.ModelAdmin):
# 	readonly_fieds = ('user','created_at','updated_at')
# 	search_fields = ('title','content','user__username','categories__name')
# 	list_display = ('title','user','public','created_at')
# 	list_filter = ('public','user__username','categories__name')

#

# 	def save_model(self,request,obj,form,change):
# 		if not obj.user_id:
# 			obj.user_id = request.user.id
# 		obj.save()

#

# admin.site.register(Category,CategoryAdmin)
# admin.site.register(Article,ArticleAdmin)



endsnippet



# ==========================
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ==========================

snippet insertIfne
-- insert query
insert into ${1:tableName}(${2:name,actionDescription}) values(${3:"alert","gives a personalized voice alert for the system"})
SELECT ${4:row_to_compare}
FROM $1
WHERE NOT($4 IN (SELECT $4 FROM $1));
endsnippet







snippet describe
describe $1 ;
endsnippet







# ==========================
# ========== sql basic syntax ======
# ==========================
snippet compHelp
-- <,>,<=,>=,=,
-- != equal <>
-- and,or
endsnippet

snippet wildCards
-- % -> A substitude for 0 or more characters
-- _ -> a substitude for a single character
-- [charlist] -> sets and ranges for characters to match
endsnippet

snippet bjoin
select ${1:table.col,table.col}
from table
join ${2:second_table}
on employee.id == branch.id;
-- left join if you want to see all
endsnippet
# ==========================
# ========== queries ======
# ==========================

snippet showTables
show tables ;
endsnippet
snippet showUsers
SELECT User FROM mysql.user ;
endsnippet

snippet showAll
select * from $1 ;
endsnippet

# options im-> inword
snippet showCurUser ""
select user() ;
endsnippet
# ==========================
# ========== databases ======
# ==========================

# options im-> inword
snippet createDatabase "for creating a database"
create database ${1:name_of_database} ;
endsnippet
# options im-> inword
snippet deleteDatabase "for deleting a database"
drop database ${1:is exists} ${2:name_of_database}
endsnippet
# options im-> inword
snippet grantAll "grant all privileges to user over database"
-- use it on the terminal as sudo
-- sudo mariadb
grant all privileges on ${1:database_name}.* to '${2:username}'@${3:localhost} ;
endsnippet
# options im-> inword
snippet showPrivileges "to show the privileges"
show grants for '${1:username}'@'${2:localhost} ;
endsnippet
# options im-> inword
snippet showColumn "for showing the specified column"
select ${1:list,of,columns}
from student
${2:[ order by (list of columns)]}
;
endsnippet

snippet columnConditions
-- select * 
-- from ${column}
-- where name in ('name1','name2')
-- limit 5;
endsnippet

snippet countVal
select count(${1:column})
from ${2:table}
-- [where conditions]
;
endsnippet

snippet average
select avg(${1:column})
from ${2:table}
-- [ where conditions]
-- [ group by column ]
endsnippet


# options im-> inword
snippet bunion "merge two queries into one"
-- select column
-- from employee
-- union
-- select branch_name
-- from branch
endsnippet
# options im-> inword
snippet helpUsers "for getting help on the users"
# the user that is been used when login normally is the '' user
endsnippet
# ==========================
# ========== table modifications ======
# ==========================

snippet foreign
foreign key (${1:name_of_key_in_this_table}) references ${2:foreign_table}(${3:value_in_foreing_table})
endsnippet

snippet createTable

create table if not exists ${1:table_name}(
	$2
);
endsnippet
# options im-> inword
snippet helpTable "for help on the table creating and deletion"

-- data types -> varchar(n),Int,DECIMAL(M,N),BLOB,DATE,TIMESTAMP,float
-- float is variable and decimal is fixed
-- use float always
-- constrains -> not null,unique,auto_increment,default 'val'
-- on delete -> on delete set null, on delete cascade
-- snippets -> foreign,

-- EXAMPLE
-- student_id INT,
-- name VARCHAR(20) not null,
-- major VARCHAR(20) unique,
-- primary key(student_id)
-- foreign key(student_id) references branch(branch_id) on delete cascade
endsnippet

snippet deltable
drop table ${1:table_name}
endsnippet

snippet addRow
alter table ${1:name_of_table} add ${2:name varchar(100)}
endsnippet

snippet modTable
-- alter table name_of_table
-- add foreign key(branch_id)
-- references branch(branch_id)
-- on delete set null;
endsnippet
# ==========================
# ========== data modification ======
# ==========================

snippet insert
insert into ${1:table_name}(${2:info,action,name,weeks,action_time,hour,minute}) values(${3:"test info","test action","test name",1,10,12,30}) ;
endsnippet

# options im-> inword
snippet update "update the data" 
-- see compHelp
update ${1:table_name}
set ${2:column_name} = ${3:value}
-- you can separate this by commas
-- ex: name='tom',name='jon'
where ${4:condition} ;
endsnippet

snippet deleteRow
-- to delete a row from the table
delete from ${1:name_of_table} where ${2:condition} ;
endsnippet
# ==========================
# ========== triggers ======
# ==========================

snippet trigger
create table ${1:trigger test} (
	message varchar(100)
	);
endsnippet


