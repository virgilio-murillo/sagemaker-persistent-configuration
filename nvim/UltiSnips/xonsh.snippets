# General
# {{{1
# infoj -- for head in jupyter notebooks
# sec -- for a section commented
# subsec -- for a subsection commented
# }}}
# Python
# {{{1
# basic_sh_usage -- for basic shell usage
# forString -- for loop over a string
# doubleFor -- for loop using zip
# dir -- for listing the loop
# heap -- for heap
# bit_masquing -- for bit masquing
# sort -- for sorting
# bit_count -- for counting bits
# sublists -- for a sublist
# file_open -- for opening a file
# file_write -- for writing a file
# file_append -- for appending to a file
# forFolder -- for a for loop in a folder
# recurseFolder -- for a recursive for loop in a folder
# relPath -- for a relative path
# ====== POOHelp ======
# POOHelp -- for help on POO
# inheritance -- for inheritance
# raise -- for raising an exception
# }}}
# for pandas dataframe
# {{{1
# encode -- for encoding a string
# lookAndFix -- for looking and fixing the needed inconsistencies
# fuzzyMatching -- for searching and replacing inconsistencies 
# fileEncodingProblems -- for finding the file encoding
# parseDate -- for parsing a date
# normalizeData -- for normalizing the given data for a machine learning algoritm
# scalingData -- for scaling the needed data
# contractions -- for creating a dictionary with english common word contractions
# fillWithNextToItVal -- for fill NaN values with the value that comes directly after it
# combineDataframe -- for combining a dataframe
# rename -- for renaming the dataset
# Nan -- for managing Null values or Not a Number values
# convertType -- for converting a data type of a column pandas
# datatype -- for getting the data type of a column
# multiFunction -- for applying multiple functions to a dataframe
# sort_values -- for sorting a pandas dataframe
# group_by -- for grouping by a column
# applyFunction -- for applying a function to a dataframe
# dataDrop -- for dropping a column of data
# describe -- for getting a description of the data
# columns -- for getting the column names in pandas
# pcombinecsv -- for combining multiple csv files into one
# loc_iloc -- for help on loc iloc
# create_dataframe -- for creating a pandas dataframe
# create_series -- for creating a pandas series ( list of values)
# read_csv -- for reading a pandas csv
# csv_len -- for getting the length of the csv
# csv_head -- for getting the top five elements of the csv
# get_column -- for getting the column of a dataframe
# get_val -- for getting the value of the column and row
# get_row -- for getting the row of the pandas dataframe
# getRange_col -- for getting a range in a column
# getRange_row -- for getting a range in a row
# getRange_row_index -- for setting the index column for the dataframe 
# conditional_selection -- for getting a dataframe with a condition
# assignToColumn -- for assigning a value to a column
# columnDescription -- for getting the description of a column (mean, describe, etc)
# mapFunction -- for applying a function to a column
# }}}1
# == graphs
# {{{1
# graphs -- simple graphs
# graphsSubplots -- for subplots
# igraphs -- for interactive graphs
# pairplot -- for creating a pairplot
# matplotlib -- for help on matplotlib plt
# folium -- for help on folium
# }}}1
# ========== Kaggle ======
# {{{1
# {{{2
# pltFormat -- for configuring the format of the plot
# ignoreWarnings -- for ignoring warnings
# install_kaggle -- for installing kaggle
# import_kaggle -- for importing the necessary for machine learning plotting and other things
# }}}2
# === machine learning
# {{{2
# split_validation -- for split validating a model
# model_descition_tree -- for using the model descition tree
# model_random_forest -- for using the random forest model
# getMAE -- for getting mean absolute error
# getBsetTreeSize -- basic code to loop through tree sizes
# export_csv -- for exporting a solution csv file
# }}}2
# == Sumary functions and maps
# {{{2
# columnDescription --for getting the description of a column (mean, describe, etc)
# mapFunction -- for applying a basic function to all cells in a column
# applyFunction -- for applying a function to all the rows in a dataset
# }}}2
# == grouping and sorting
# {{{2
# group_by -- for grouping a dataframe by a column name
# sort_values -- to sort the values by a column name
# multiFunction -- for apply multiple functions to the group
# }}}2
# == data types and missing values
# {{{2
# datatype -- for getting the datatype of a column
# convertType -- for converting a data type of a column pandas
# NaN -- for managing Null values or Not a Number values
# rename -- for renaming the dataset
# combineDataframe -- for combining a dataframe
# }}}2
# == data cleaning
# {{{2
# fillWithNextToItVal -- for fill NaN values with the value that comes directly after it
# contractions -- for english contractions in a dictionary
# }}}2
# == scaling and normalization
# {{{2
# scaleData -- for scaling the used data
# normalizeData -- for normalizing the given data for a machine learning algoritm
# }}}2
# == parse dates in dataset
# parseDate -- for parsing a date
# == char encoding
# {{{2
# fileEncodingProblems -- for finding the file encoding
# encode -- for encoding a string
# }}}2
# == inconsistencies
# {{{2
# lookAndFix -- for looking and fixing inconsistencies
# fuuzyMatching -- for searching and replacing inconsistencies
# }}}2
# ==== feature engineering
# {{{2
# mutual_info_regression -- for creating a mutual info regression
# clustering -- for clustering the information
# }}}2
# == intermediate machine learning -- for more refined machine learning
# {{{2
# scoreModels -- for scoring the models
# imputter -- for replacing missing values with a simple regression
# completeImputter -- for using imputer and specifying that the imputter was performed in a column
# ordinalEncoder -- for categorical values to encode them in integer colum
# oneHotEncoding -- like the above but particullarly usefull when you have a column that encapsulates mo categories
# splitOneHotEncoding -- for using ordinalEncoder on columns that have a lot of unique values
# pipeLine -- for using a pipieline for better performance and less bugs
# crossValidation -- for using crossvalidation with a pipeline and a regressionForest
# xgboost -- for using the better xgboost
# dataLeackage -- for looking and dropping data leackage
# }}}2
# == sql data analysis big query
# {{{2
# loadBigQuery -- for loading the python big query module
# BQ_query -- for executing a sql command in big query
# BQ_big_queries -- for executing a big query without exceding the limit
# BQ_groupBy_having_count -- for executing thouse on a database big query
# BQ_orderBy -- for ordering by
# BQ_dates -- for working with dates on big query
# BQ_with_as -- for nested queries better readability
# BQ_join -- for join in big query
# }}}2
# == advanced big query
# {{{2
# BQ_left_join -- for doing the big query left join
# BQ_over_clause -- for using the over clause
# BQ_partition_clause -- for using the partition clause
# BQ_unest -- for unesting nested data or repeated data
# BQ_speed_functions -- usable functions to see the efficiency of the queries
# }}}2
# == intro to deep learning
# {{{2
# model1LinearUnit -- for creating a model with one linear unit
# getWeightsAndBiases -- for getting weights and biases
# plotOutputOfUntrained -- for plotting the output of an untrained linear model
# loss_optimizer -- for adding a loss and the optimizer
# train_dl -- for training a deep learning model
# plotLosstrainingData -- for plotting loss training data
# animateDeepLearning -- for animating a deep learnig project
# earlyStopping -- for getting an early stopping
# binaryClassification -- for classifying in bynary
# }}}2
# === computer vision
# {{{2
# define_pretrained_base -- for defining the pre trained base
# attachHeAD -- for attaching the head
# CV_train -- for training a simple computer vision algoritm
# CV_plot_acc -- for plotting the binary accuracy
# filterWithConvulsion -- for filtering with convulsion
# relu_activation -- for activating the relu with te conv2d
# CV_tfKernel -- for defining a basic tensorflow kernel
# condenseWithMaximumPooling -- adding condense with maximum pooling
# imageCondensePool -- for image condensing a pool
# globalAveragePooling -- for global average pooling
# sliding_window -- for the sliding window tecniques padding and stride
# customConvents -- for defining a custom convent
# dataAugmentation -- for adding features to the cv model
# }}}2
# == reinforcement learning
# definingAgents -- for defining an agent for machine learning
# ==  time series
# {{{2
# fitLinearRegression -- for fitting a linear regression model from a forecasting task
# lagFeature -- for adding a lag feature to a pandas dataframe
# moving_average_plot -- for a moving average plot
# splines -- an alternative to polinomials that is better
# plotSeasonability -- for plotting seasonability
# seasonalFeatures -- for creating seasonal features
# hibrid_forecasting -- for creating hibrid models
# }}}2
# === geoespacial analisys
# {{{2
# GP_setup -- for setting up a basic geopandas instance
# interactive_map -- for creating an interactive map
# choropeth_i_map -- for creating a choroplet map
# geoencoder_map -- for creating locations in a map
# measure_distance -- for measuring a distance in a map
# create_a_buffer -- for creating a buffer in geopandas
# }}}2
# == machine learning explainability
# {{{2
# test_score_features -- for testing the score of the features in your model
# notify-send
# execute sound
# newFolder-recursive
# }}}2
# ---------------- organization
# sec
# subsec
# ------------ python bases de datos
# {{{2
# sqlite3 --- for creating a connection to sqlite3
# sqlite3c -- for executing a command in sqlite3j
# mysql -- for creating a connection with mysql or mariadb 
# mysqlc -- for executing a command in the mysql connector
# mysqlq -- for getting the information from a query
# rowcount -- for getting the number of rows affected after  a commit
# encryption -- for encripting a password
# }}}2
# }}}1
# -------------- python django
# {{{1
# {{{2
# crateProject -- for creating a django project from scratch
# migrate -- for applying models.py file. create tables
# commonPackages -- for a list of common packages for django
# }}}2
# --- settings.py ------ MainProjectFolder
# {{{2
# createApp -- for creating a django app
# useMysql -- for using mysql
# changeLanguage -- for setting the default language in the project
# addContextProcessor -- these are used for global functions inside the html files
# }}}2
# --- urls.py --- MainProjectFolder
# {{{2
# importUrl -- for importing the url file from an other app into the main app
# imageRoute -- for using an image from a route
# }}}2
# --- views.py --- MainAppFolder
# {{{2
# viewsHelp -- for creating a help file
# login_page -- for creating a log in page
# log_out -- for logging out form a page
# register -- for registering user
# getSqlObject -- for getting an sql object
# }}}2
# --- apps.py --- MainAppFolder
# appsHelp -- for help on the apps.py file
# --- urls.py -- MainAppFolder
# urlsHelp -- for help in importing directories
# --- forms.py --- MainAppFolder
# forms_help -- for registering a user in the database
# --- layouts.html -- MainAppFolder/templates/layouts/
# html_template -- for using templates
# --- models.py --- SecondaryAppFolder
# django_createTable -- for creating a table for the app
# --- views.py --- SecondaryAppFolder
# loginRequired -- require login to acces a page
# --- admin.py -- SecondaryAppFolder
# modClassDisplay -- for modifications on the django panel search bar and many others
# ========== BasesDeDatos ======
# sec - for creatign a new section
# -------- sql basic syntax
# ======== queries ========
# {{{2
# showUsers -- to show all users available in the server
# showAll -- for showing everything in a table
# describe -- for showing the rows of a table
# showCurUser -- for showing the current user
# bunion -- help for creating a union query
# bjoin -- help for creating a join query
# }}}2
# =========== databases ============
# {{{2
# createDatabase -- for creating a database
# deleteDatabase -- for deleting a database
# grantAll -- to grant all privileges to user over the database
# showPrivileges -- to show the privileges
# helpUsers -- to get help on the users
# }}}2
# ========== table modifications ========
# {{{2
# foreign -- for  a foreign key
# createTable -- for creating a table interactively
# deltable -- for deleting a table
# helpTable -- for help on the table creation and deletion
# }}}2
# ========== data modifications =========
# {{{2
# insert -- for inserting a new value into the table
# insertIfne -- for inserting a value if it doesn't exist
# update -- for updating the database
# }}}2
# }}}1





































# ==========================
# ========== global functions ======
# ==========================
global !p
def create_matrix_placeholders(snip):
	# Create anonymous snippet body
	anon_snippet_body = ""
	# Get start and end line number of expanded snippet
	start = snip.snippet_start[0]
	end = snip.snippet_end[0]
  # Append current line into anonymous snippet
	for i in range(start, end + 1):
		anon_snippet_body += snip.buffer[i]
		anon_snippet_body += "" if i == end else "\n"
	# Delete expanded snippet line till second to last line
	for i in range(start, end):
		del snip.buffer[start]
	# Empty last expanded snippet line while preserving the line
	snip.buffer[start] = ''
	# Expand anonymous snippet
	snip.expand_anon(anon_snippet_body)
endglobal
post_jump "snip"
global !p
def returnCommandSplitted(command):
	cmd_arr = command.split('_')
	return_command = ""
	for a in cmd_arr:
		return_command += a + " "
	return_command = return_command[0:len(return_command)-1]
	return return_command
endglobal

post_jump "snip"
global !p

def parseStatement(i,j,k,stri):
	prev = ''
	twoBehind=''
	ans = ""
	for ch in stri:

		if ( ch == 'k' or ch == 'i' or ch == 'j' )  and prev == '$' and twoBehind == '\\':
			ans = ans[:-2]
			ans+=ch

		elif ch == 'i' and prev == '$' and twoBehind != '\\':
			ans = ans[:-1]
			ans+=str(i)
		elif ch == 'j' and prev == '$' and twoBehind != '\\':
			ans = ans[:-1]
			ans+=str(j)
		elif ch == 'k' and prev == '$' and twoBehind != '\\':
			ans = ans[:-1]
			ans+=str(k)
		elif ch == 'n' and prev == '\\':
			ans = ans[:-1]
			ans+="\n"

		else :
			ans+=ch

		twoBehind = prev
		prev = ch
	return ans

endglobal

global !p
def getMatchArr():
	cont=1
	arr = []
	while 1 :
		try :
			arr.append(match.group(cont))
			cont+=1
		except :
			break
	return arr
endglobal







# ==========================
# ========== gui's with tkinter ======
# ==========================

snippet tkinter

# snippet dropMenu "for creating a basic drop dow mwnu"
# \$3TypeLabel = Label(${4:frame},text="\$3:")
# \$3TypeLabel.pack(side=TOP)
# 
# $\{1:name of list} = [
# $\{2:"alarm",
# "alert",
# "zoom",
# "circadian-alert",
# "circadian-zoom",
# "circadian-alarm"}
# ]
# 
# $\{3:name} = tk.StringVar()
# \$3.set(\$1[0]) # default value
# 
# \$3DropMenu = OptionMenu(\$4, \$3, *\$1)
# \$3DropMenu.pack()
# 
# endsnippet
# 
# 
# # options im-> inword
# snippet figma "for automatically creating tkinter from figma"
# tkdesigner $FILE_URL $FIGMA_TOKEN
# endsnippet
# snippet s-gui
# # pack is for piling elements and grid is like a spread sheet
# root = Tk()
# \${1:frame} = Frame(root)
# \$1.\${2:pack|grid}()
# \$3
# root.mainloop()
# endsnippet
# 
# snippet gBut
# # opt parameters = fg="color",bg="color",command=lambda: function(params)
# \${1:name_of_button} = Button(\${2:frame},text="\$3").pack(side=\${4:BUTTON|TOP|LEFT|RIGHT})
# endsnippet
# 
# snippet gLab
# # opt parameters = fg="color",bg="color"
# \${1:name_of_label} = Label(\${2:frame},text="\$3").pack(side=\${4:BUTTON|TOP|LEFT|RIGHT})
# endsnippet
# 
# snippet gText
# \$1Label = Label(level_add_task,text="\$1:")
# \$1Label.pack(side=TOP)
# # start entering information
# \$1 = tk.StringVar()
# \$1TextBox = tk.Entry(level_add_task, width = 50, textvariable = \$1)
# \$1TextBox.pack(side=TOP)
# endsnippet
# 
# snippet notify-send
# os.system("notify-send \"\$1\"")
# endsnippet
# 
# # options im-> inword
# snippet gImage "for loading image into a tkinter application"
# # the render and the image variable may be lost in a function or a class,
# # so make sure you save them somewhere otherwise you will lose them
# 
# image = Image.open(\${1:path})
# hsize = int((float(image.size[1])*float(wpercent)))
# # resize the image acording to the witdh
# image = image.resize(({1:basewidth},hsize), Image.ANTIALIAS)
# 
# render = ImageTk.PhotoImage(image)
# self.img_arr.append((image,render))
# # create the tkinter label and resize
# img = Label(self.ventana,image=self.img_arr[-1][1])
# # now just pack or grid the image
# 
# endsnippet
# 
# 
# # options im-> inword
# snippet gGrid "for grid in tkinter"
# # row -- for the row position
# # column -- for the column position
# # rowspan -- for the number of rows it takes
# # columnspan -- for the number of columns it takes
# # sticky=N,S,W,E,NE,NW,SE,SW -- if the widget is smaller than the frame is says where it is going to stick to
# endsnippet
# # options im-> inword
# snippet gPack "for the pack information"
# # fill=X,Y -- to fill the entire window in the desired position
# # expand=YES -- the widget fills the space in the parent widget
# # side=BOTTOM,LEFT,RIGHT,UP -- to specify the side to be placed
# endsnippet
# # options im-> inword
# snippet gRadioButtons "for creating radio buttons"
# # \$4 = StringVar()
# \${6:rbn} = Radiobutton(
# 	\${1:ventana},
# 	text="\${2:text}",
# 	value="\${3:value}",
# 	variable=\${4:variable},
# 	command=lambda: \${5:function(params)},
# 	)
# # grid or pack
# endsnippet # options im-> inword
# snippet gCheckButtons "for check buttons"
# # \$3 = IntVar()
# \${5:cbn} = Checkbutton(
# 	\${1:ventana},
# 	text="\${2:text}",
# 	variable = \${3:variable},
# 	onvalue=1,
# 	offvalue=0,
# 	command=lambda: \${4:function(params)}
# 	)
# # grid or pack
# endsnippet
# 
# # options im-> inword
# snippet gOptionMenu "for the option menu"
# # \$3 = StringVar()
# \${1:optionMenu_name} = OptionMenu(
# 	\${2:ventana},
# 	\${3:stringVar},
# 	\${4:"opciones","separadas"}
# 	)
# # pack or grid
# endsnippet
# 
# # options im-> inword
# snippet gDeleteItem "for deleting an item "
# \${1:name}.grid_remove()
# endsnippet
# 
# # options im-> inword
# snippet gPackRemove "for deleting a pack item"
# \${2:name}.pack_forget()
# endsnippet
# 
# # options im-> inword
# snippet gMainMenu "for adding the main menu"
# \${1:name} = Menu(\${2:window})
# # name.add_command(label="",command=lamda: command(params))
# \$2.config(menu=\$1)
# endsnippet
# # options im-> inword
# snippet gShow "for showing all the pacekd widgets in window"
# \${1:window}.mainloop()
# endsnippet
# 
# # options im-> inword
# snippet gExitTkinter "for exiting the main loop"
# \${1:ventana}.exit
# endsnippet

endsnippet


# ==========================
# ========== Organization ======
# ==========================
snippet sk

# Date: `date +%d/%B/%Y\ -\ %A`
# Author: Virgilio Murillo Ochoa
# personal github: Virgilio-AI
# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
# contact: virgiliomurilloochoa1@gmail.com
# web: virgiliomurillo.com

from typing import *
import heapq as hp
from collections import deque
from collections import defaultdict
import sys
import argparse
import argcomplete  # Tab completion support with xontrib-argcomplete
from argcomplete.completers import ChoicesCompleter
import os

sdic = {}


# import colored for terminal color
from termcolor import colored

# dic = defaultdict(int)
# set = set() # .remove(val),.add(val),.discard(val),.pop(),.clear()
# dic = {} # .remove(id),dic[key] = val, dic.get(key,0)
# arr = [] # .append(val),.pop(),.remove(val),.sort(),.reverse(),.insert(pos,val),.clear()


# def input(f=open(".`!p snip.rv = snip.basename`_In1.txt")): return f.readline().rstrip() # uncomment for debugging





arguments = {
	'path':{
		'type':str,
	}, # the - pass to _ on the dictionary
	'dry_run':{
		'type':bool,
	},
	'number':{
		'type':int,
	},
	"Name":{
		'type':str,
		},
	}




def main():
	print(sdic)
	$1



































# The arguments
# {{{1

arguments = {
	'search_text':{
		'short':'-s',
		'long':'--search-text',
		'completer':'path',
		'help':'The file containing the text to search',
		'required':True,
		'type':str,
		},
	'replace_text':{
		'short':'-r',
		'long':'--replace-text',
		'completer':'path',
		'help':'the file containing the text to be replaced',
		'required':True,
		'type':str,
		},
	'target_file':{
		'short':'-t',
		'long':'--target-file',
		'help':'the target file to search and replace',
		'required':True,
		'type':str,
		},
	'dry-run':{
		'long':'--dry-run',
		'completer':['True','False'],
		'help':'Dry run',
		'required':False,
		'type':bool,
		},
	"verbose":{
		'short':'-v',
		'long':'--verbose',
		'help':'for verbose output',
		'required':False,
		'type':bool,
		},
	"silent":{
		'long':'--silent',
		'help':'for no output',
		'required':False,
		'type':bool,
		},
	"no_confirm":{
		'long':'--no-confirm',
		'help':'for confirmation of changes',
		'required':False,
		'type':bool,
		},
	}

if __name__ == '__main__':
	# list arguments

	parser = argparse.ArgumentParser()

	for arg in arguments:
		arg = arguments[arg]
		required = arg['required']
		type = arg['type']
		if type == bool:
			if 'short' in arg:
				parser.add_argument(arg['short'],arg['long'],help=arg['help'],required=required,action="store_true")
			else:
				parser.add_argument(arg['long'],help=arg['help'],required=required,action="store_true")
			continue



		if 'short' in arg:
			if	'default' in arg:
				parser.add_argument(arg['short'],arg['long'],help=arg['help'],required=required,type=type,default=arg['default'])
			else:
				parser.add_argument(arg['short'],arg['long'],help=arg['help'],required=required,type=type)
		else:
			if 'default' in arg:
				parser.add_argument(arg['long'],help=arg['help'],required=required,type=type,default=arg['default'])
			else:
				parser.add_argument(arg['long'],help=arg['help'],required=required,type=type)
		# add completer

	argcomplete.autocomplete(parser)
	args = parser.parse_args()

	sdic = args.__dict__

	#sdic['path']
	#sdic['Name']
	# add parameters that require no input

	main()

# }}}

endsnippet

snippet infoj
# %% [markdown]
- Date: `date +%d/%B/%Y\ -\ %A`
- Author: Virgilio Murillo Ochoa
- personal github: Virgilio-AI
- linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
- contact: data_scientist@virgiliomurillo.com
- web: virgiliomurillo.com
endsnippet


snippet sec
# =============
# ==== $1 =====
# =============
endsnippet



snippet subsec
# ======== $1
endsnippet



# ==========================
# ========== Python ======
# ==========================

# options im-> inword
snippet basic_sh_usage "for a basic sh usage"
1 + 1

d = {'xonsh': True}
d.get('bash', False)


$HOME
'/home/snail'

$GOAL = 'Become the Lord of the Files'
print($GOAL)
Become the Lord of the Files
del $GOAL

${'HO' + 'ME'}
'/home/snail'

$(ls -l)

x = 'xonsh'
y = 'party'
echo @(x + ' ' + y)

out = $(echo @(x + ' ' + y))
out
'xonsh party\n'
@("ech" + "o") "hey"
hey
endsnippet
# ==========================
# ========== system interaction ======
# ==========================



snippet shOutput
${1:strinName}=str(subprocess.check_output([${2:list_commands}]).decode('utf-8'))
endsnippet

snippet shCommand
os.system('$1')
endsnippet

# ==========================
# ========== python ======
# ==========================

snippet printCommands "for pringting the commands available for a datatype or some other kind of object"
for i in dir(${1:dataType}):
	if i[0] != '_' and "${2:keyword}" in i:
		print(i)
endsnippet

snippet help "for getting help"
help($1)
endsnippet

# ========== strings ======

snippet forString
for element in range(0, len(${1:string_name})):
	print($1[element])
endsnippet


snippet doubleFor "for getting a for iterating two different objects"
for ob1,ob2 in zip(obar1,obarr2):
	print(ob1,ob2)
endsnippet


snippet ins "" im
"+str($1)+"
endsnippet

# ========== data structures ======

snippet dir "for getting information on a var type"
print(dir(rolls))
# where rolls is  a numpy.ndarray
endsnippet

snippet heap "for help on heaps"
test = [[]] * 4
# for pushing into a heap
# you can use a regular array
# hp.heappush(test[2],6)
# hp.heappop(heap)
endsnippet

snippet bit_masquing "for using bit masquing in python"
# example with 3 elements
# when you want to test 010 in the 6 position 110
# it returns true because there is a 1 in the middle both times
for i in range(1<<n):
	for j in range(n):
		if i & 1<<j:
			print("there is a one")

endsnippet

snippet sort "for sorting in python"
li = [[1,4,7],[3,6,9],[2,59,8]]
li.sort(key=lambda x: int(x[0]))
sorted(li, key = lambda x: int(x[0]))
endsnippet

snippet bit_count "for counting the number of bits an int has"
n = 12
n.bit_count()
endsnippet

snippet list
${1:temp} = [$2]
endsnippet

# options im-> inword
snippet sublist "for getting a sublist (slicing)"
# list[start:end+1:step]
endsnippet


snippet set
${1:temp} = {$2}
endsnippet


snippet tuple
${1:temp} = ($2)
endsnippet

# ========== files ======

snippet file_open

with open('${1:file_name}') as ${2:file_var}:
	for line in $2:
		print(line.rstrip())

endsnippet

snippet file_write

# from io import open
${1:name} = open("${2:file}","w")
$1.write(${2:string})
$1.close()

endsnippet
# options im-> inword
snippet file_append "for appending text into a file"
# from io import open
${1:name} = open("${2:file}","a+")
$1.write(${2:string})
$1.close()
endsnippet

# options im-> inword
snippet forFolder "iteration of files over folder"
# iterate over files in
# that directory
for filename in os.listdir(directory):
	f = os.path.join(directory, filename)
# checking if it is a file
	if os.path.isfile(f):
		print(f)
endsnippet
# options im-> inword
snippet recurseFolder "to recurse a folder iteractively"
def recurseFolder(path):
	for filename in os.listdir(path):
		f = os.path.join(path,filename)
		if os.path.isfile(f):
			print(f)
		else:
			recurseFolder(f)
endsnippet
# options im-> inword
snippet fmkdir "for creating a directory and it's parent directories"
# import pathlib
pathlib.Path(${1:abs_path}).mkdir(parents=True,exist_ok=True)
endsnippet
# options im-> inword
snippet move_file "for moving a file into other location"
# import shutil
shutil.move(${1:path_original},${2:path_target})
endsnippet
# options im-> inword
snippet copy_file "for copying a file into a new location"
# import shutil
shutil.copy(${1:path_original},${2:path_target})
endsnippet

# options im-> inword
snippet relPath "create a relative path"

# import pathlib


# create the directory creating the parents
relPath = str(pathlib.Path().absolute()) + "/"

# pathlib.Path(relPath + "/holaa").mkdir(parents=True, exist_ok=True)

# abrir
ruta = relPath  +"${1:relarivePath}"
endsnippet


# ========== POO ======

snippet POOHelp "for help information on POO with python"
# create files with a single class inside of them and import them into the main file
# example:
# create the class Car inside the car.py file
# import into the main file using 
# from car import Car
#
# and use it
# car = Car()
endsnippet

snippet inheritanceHelp "how to use inheritance in python"
# class name(fatherName)
# inside the __init__(self)
# super().__init__()
endsnippet


# ========== exception handling ======
# options im-> inword
snippet try "try catch generic"
try:
	$1
except Exception as e:
	$2
fianlly:
	$3
endsnippet
# options im-> inword
snippet raise "for raising custom exception"
if $1:
	raise $2("${3:text for exception}")
endsnippet

# ==========================
# ========== pandas ======
# ==========================


snippet encode "for encoding a string"
# encode it to a different encoding, replacing characters that raise errors
after = before.encode("utf-8", errors="replace")
endsnippet


# options im-> inword
snippet lookAndFix "for looking and fixing the needed inconsistencies"
# get all the unique values in the 'Country' column
countries = professors['Country'].unique()

# sort them alphabetically and then take a closer look
countries.sort()
countries

# convert to lower case
professors['Country'] = professors['Country'].str.lower()
# remove trailing white spaces
professors['Country'] = professors['Country'].str.strip()

endsnippet

# options im-> inword
snippet fuzzyMatching "for searching and replacing inconsistencies "
# get the top 10 closest matches to "south korea"
matches = fuzzywuzzy.process.extract("south korea", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)

# take a look at them
matches
###################################
######################


# function to replace rows in the provided column of the provided dataframe
# that match the provided string above the provided ratio with the provided string
def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):
# get a list of unique strings
	strings = df[column].unique()

# get the top 10 closest matches to our input string
matches = fuzzywuzzy.process.extract(string_to_match, strings, 
		limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)

# only get matches with a ratio > 90
	close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]

# get the rows of all the close matches in our dataframe
rows_with_matches = df[column].isin(close_matches)

# replace all rows with close matches with the input matches 
	df.loc[rows_with_matches, column] = string_to_match

# let us know the function's done
	print("All done!")

# use the function we just wrote to replace close matches to "south korea" with "south korea"
replace_matches_in_column(df=professors, column='Country', string_to_match="south korea")

endsnippet

snippet fileEncodingProblems "for finding the file encoding"
# get the byte size of the file to predict the needed file encoding
def getSize(fileobject):
	fileobject.seek(0,2) # move the cursor to the end of the file
	size = fileobject.tell()
	return size

file = open('../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv', 'rb')
print( getSize(file))


# now replace with the number of bytes needed
with open("../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv", 'rb') as rawdata:
	result = chardet.detect(rawdata.read(${1:number_of_bytes}))

# check what the character encoding might be
print(result)

# now replace the needed file encoding
police_killings = pd.read_csv("../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv",encoding='Windows-1252')
endsnippet

snippet parseDate "for parsing a date"
# where pd is for pandas and landslides['date'] is the name of the column
landslides['date_parsed'] = pd.to_datetime(landslides['date'], format="%m/%d/%y")
endsnippet

snippet normalizeData "for normalizing the given data for a machine learning algoritm"
# normalize the exponential data with boxcox
normalized_data = stats.boxcox(original_data)
endsnippet

snippet scalingData "for scaling the needed data"
## this is used to place the data in a matter of 0 - 1
# generate 1000 data points randomly drawn from an exponential distribution
original_data = np.random.exponential(size=1000)

# mix-max scale the data between 0 and 1
# where the columns are the list of columns to be normalized
scaled_data = minmax_scaling(original_data, columns=[0])
endsnippet

snippet contractions "for creating a dictionary with english common word contractions"

contractions = { 
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he'll've": "he will have",
"he's": "he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how does",
"i'd": "i would",
"i'd've": "i would have",
"i'll": "i will",
"i'll've": "i will have",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'd've": "it would have",
"it'll": "it will",
"it'll've": "it will have",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she would",
"she'd've": "she would have",
"she'll": "she will",
"she'll've": "she will have",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"so's": "so is",
"that'd": "that would",
"that'd've": "that would have",
"that's": "that is",
"there'd": "there would",
"there'd've": "there would have",
"there's": "there is",
"they'd": "they would",
"they'd've": "they would have",
"they'll": "they will",
"they'll've": "they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
" u ": " you ",
" ur ": " your ",
" n ": " and "}

endsnippet

snippet fillWithNextToItVal "for fill NaN values with the value that comes directly after it"
sf_permits_with_na_imputed = sf_permits.fillna(method='bfill', axis=0)
endsnippet

snippet combineDataframe "for combining a dataframe"
# when you have different dataframes with the same columns
pd.concat([canadian_youtube, british_youtube])


# combine dataframes that have an index in common
# where MeetID is the index in common and can be different
powerlifting_combined = powerlifting_meets.set_index("MeetID").join(powerlifting_competitors.set_index("MeetID"))
endsnippet

snippet rename "for renaming the dataset"
reviews.rename(columns={'points': 'score'})
reviews.rename(index={0: 'firstEntry', 1: 'secondEntry'})
reviews.rename_axis("wines", axis='rows').rename_axis("fields", axis='columns')
endsnippet


snippet NaN "for managing Null values or Not a Number values"
reviews[pd.isnull(reviews.country)]
reviews[pd.notnull(reviews.country)]
reviews.region_2.fillna("Unknown")

# also for replacing the values
reviews.taster_twitter_handle.replace("@kerinokeefe", "@kerino")
endsnippet

snippet convertType "for converting a data type of a column pandas"
reviews.points.astype('float64')
endsnippet

snippet datatype "for getting the data type of a column"
reviews.price.dtype

# for getting all columns
reviews.dtypes
endsnippet

snippet multiFunction "to apply multiple funcitons to the dataframe"
# example

price_extremes = reviews.groupby('variety').price.agg([min, max])
endsnippet

snippet sort_values "for sorting a pandas dataframe"
countries_reviewed.sort_values(by=['country', 'len'])
endsnippet

# options im-> inword
snippet group_by "for grouping a dataframe by a column name"
reviews.groupby(['col_name','col_name2']).apply(function,axis='columns')

# reset to normal index
# reviews.reset_index()
endsnippet

snippet applyFunction "for applying a function to all the rows in a dataset"
def function(row):
	row.col_name = row.col_name - 5
	return row

dataFrame.apply(function, axis='columns')
endsnippet

snippet dataDrop
${1:name_of_db}.drop(
labels=["name","example"],
axis=1, # 0 -> rows 1 -> columns
inplace=False # alter the dataFrame
)
endsnippet

# options im-> inword
snippet describe "for describing data"
${1:pandas_dataframe}.describe
endsnippet

# options im-> inword
snippet columns "for getting the column names in pandas"
${1:name}.columns
endsnippet

snippet pcombinecsv
${1:name_of_files} = glob.glob("${2:PATH/TO/FILES/file*_.csv}")
${1}_dt = pd.concat)[pd.read_csv(f) for f in ${1} ])
endsnippet


# options im-> inword
snippet loc_iloc "for information on loc and iloc"
# the iloc if for using the integer position of the dataframe
# the loc is for using the label position on the dataframe
endsnippet


# options im-> inword
snippet create_dataframe "for creating a pandas dataframe"
# optionales
# index=['idx1','idx2']
pd.DataFrame({'${1:col_name}':[${2:vals,separated,by,comman|series}]})
endsnippet

# options im-> inword
snippet create_series "for creating a pandas series ( list of values)"
# optionals
# index=['index1','index2', ... , 'index5'] -- to assigning a name to the indices
# name = "name of series" -- for assigning a name to the whole series
pd.Series([1, 2, 3, 4, 5])
endsnippet


snippet read_csv "for reading a pandas csv"
# optionals
# set the index col manually
# index_col=num
dt_name = pd.read_csv("path/to/file.csv")


# for specific encoding
# kickstarter_2016 = pd.read_csv("../input/kickstarter-projects/ks-projects-201612.csv", encoding='Windows-1252')

# for loading dates as index
# museum_data = pd.read_csv(museum_filepath,index_col="Date",parse_dates=True)

endsnippet

# options im-> inword

# options im-> inword
snippet csv_len "for getting the length of the csv"
dt_name.shape
endsnippet

# options im-> inword
snippet csv_head "for getting the top 5 elements of the csv"
dt_name.head()
endsnippet

# options im-> inword
snippet get_column "for getting the column of a dataframe"
# dataframe.title
# dataframe["title"]
endsnippet

# options im-> inword
snippet get_val "for getting the value of the column and row"
# dataframe["title"][num]
endsnippet

# options im-> inword
snippet get_row "for getting the row of the pandas dataframe"
#dataframe.iloc[index_num]
endsnippet


# options im-> inword
snippet getRange_col "for getting a range in a column"
# dataframe.loc[row_start:row_end,column]
# you can pass a list instead to get only the needed indices
# dataframe.loc[[1,2,3,4],[col1,col2]]
endsnippet

# options im-> inword
snippet getRange_row "for getting a range in a row"
# dataframe.iloc[start:end-1]

# also you can do
# museum_data.loc[first_slice_q:second_slice_q]
# because for dates it tends to work because you use a specific index and cannot be -1
endsnippet

# options im-> inword
snippet getRange_row_index "for getting a range of rows using the index"
# remember that loc takes the last element of the range unlike python
dataframe.loc[row_start:row_end,'index_name_1','index_name_2']
# you can use a list [1,2,5] instead of a range 1:2
endsnippet

# options im-> inword
snippet set_index "for setting the index column for the dataframe"
dataframe.set_index("title")
endsnippet

# options im-> inword
snippet conditional_selection "for getting a dataframe with a condition"
dataframe.loc[dataframe.title == "Italy"]
# optional
# dataframe.loc[dataframe.title.isin(['Italy','France'])]
# dataframe.loc[dataframe.title.notnull())]
# use always parenthesis since it tends to fail
# use & and | instead of and or
endsnippet

# options im-> inword
snippet assignToColumn "for assigning a value to a column"
dataframe['col_title'] = 'constant'
dataframe['col_title'] = range(len(dataframe),0,-1)
endsnippet

# options im-> inword
snippet columnDescription "for getting the column description"
# for getting general information on the dataframe
# dt_dataframe['col_name'].describe()
# for getting the mean needed
# dt_dataframe['col_name'].mean()
# for getting the unique values
# dt_dataframe['col_name'].unique()
# for getting the times each element appears in the list
# dt_dataframe['col_name'].value_counts()

endsnippet


snippet mapFunction "for applying a basic function to all cells in a column"
# column_mean = dataFrame.col_name.mean()
# also you can put a function instead of a lambda function
# dataFrame.col_name.map(lambda p: p - column_mean)

# also you can apply a operand like
# and it will be the same
# dataFrame.col_name - column_mean

# also < > ==  etc
# example:
# dataFrame.col_name + " - " + dataFrame.second_col_name
endsnippet



# ==========================
# ========== matplotlib ======
# ==========================

# options im-> inword
snippet matplotlib "for matplotlib information"

plt.xlabel('\${1:x_description}')
plt.xlabel('\${2:y_description}')


plt.title('\$1')


#pass a list and respect the order of the plots
# format (['f(x)','g(x)'])

plt.legend(['\${1:function_mathematical_representation}\$2'])




plt.xlim(\${1:left_lim},\${2:right_lim})
plt.ylim(\${3:left_lim},\${4:right_lim})


plt.grid(True)



def getClosestPointToLine(p1,p2,p3):
	x1, y1 = p1
	x2, y2 = p2
	x3, y3 = p3
	dx, dy = x2-x1, y2-y1
	det = dx*dx + dy*dy
	a = (dy*(y3-y1)+dx*(x3-x1))/det
	return x1+a*dx, y1+a*dy



\${1:x_var} = np.linspace(\${2:x_lower_bound},\${3:x_upper_bound},\${4:step})
\${5:y_var} = np.linspace(\${6:y_lower_bound},\${7:y_upper_bound},\${8:step})
X_, Y_ = np.meshgrid(\$1, \$5)
Z_ = mainFunction(X_, Y_)
plt.contour(X_,Y_,Z_,\${9:number_of_curves})


plt.scatter(\${1:x},\${2:y})




\${1:x_var} = np.arange(\${2:left_range},\${3:right_range},\${4:step})

plt.plot(\$1,\${5:f(x)})


# here you have to identify the lower function and the bigger function
# you can use something like below
# botton = np.maximum(y1,0)

plt.fill_between(\${1:function_1},\${2:function_2},\${3:function_3},where=\${4:x_conditions} )


endsnippet


snippet folium "for folium information"

m = folium.Map(location=[0,0], tiles = 'cartodbpositron',min_zoom = 1, max_zoom = 4, zoom_start=1)

for i in range(0,len(data)):
	tooltip = '<li><bold> Country:' + str(data.iloc[i]['Country']) +
	'<li><bold> Year:' + str(data.iloc[i]['Year'])
	folium.Circle(
			location=[data.iloc[i]['lat'], data.iloc[i]['lon']],
			popup=data.iloc[i]['name'],
			radius=100000,
			color='crimson',
			fill=True,
			fill_color='crimson'
			).add_to(m)

endsnippet

# ==========================
# ========== Kaggle ======
# ==========================

# options im-> inword
snippet pltFormat "for configureing the format of the plot"
%config InlineBackend.figure_formats = ['${1:svg|png}']
endsnippet

# options im-> inword
snippet ignoreWarnings "for ignoring warnings"
with warnings.catch_warnings():
	warnings.simplefilter('ignore')
endsnippet

snippet install_kaggle
# upgrade pip
!pip install --upgrade pip
# dependencies
!pip install --upgrade chart_studio
# necessary
# !pip install --upgrade numpy
# !pip install --upgrade pandas
# !pip install --upgrade matplotlib
# !pip install --upgrade seaborn
# for visualization tools
# !pip install --upgrade cufflinks
# !pip install --upgrade plotly
# !pip install --upgrade numpy
# for machine learning
# !pip install --upgrade sklearn
# !pip install --upgrade xgboost
# !pip install --upgrade lightgbm
# !pip install --upgrade catboost
# !pip install --upgrade keras
# !pip install --upgrade tensorflow
# for data cleaning
!pip install --upgrade textblob
# for importing pickle files
!pip install --upgrade pickle5
# for plotting maps
!pip install --upgrade folium
endsnippet

# options im-> inword
snippet import_kaggle "for importing the necessary for machine learning plotting and other things"
%config InlineBackend.figure_formats = ['svg']
# from sklearn.ensemble import RandomForestRegressor # random forest regressor
# from sklearn.model_selection import cross_val_score # the cross validation module
# Setup feedback system
# from learntools.core import binder # the binder
# binder.bind(globals())
# from learntools.feature_engineering_new.ex2 import * # for importing all in feature engineering
# import matplotlib.pyplot as plt # for importing matplotlib
# import numpy as np # for importing numpy
# import pandas as pd # for importing pandas
# import seaborn as sns # seaborn is used to plot
# from numpy.random import randint
# import random
# from matplotlib.patches import Polygon
# from matplotlib.animation import FuncAnimation
# import pickle as pkl
# import plotly.offline as py # for importing plotly
# import cufflinks as cf
# import plotly.graph_objects as go
# import plotly.express as px
# import plotly.figure_factory as ff
# from plotly.subplots import make_subplots
# plot maps
import folium
# for data cleaning
from textblob import TextBlob
# offline interactive plotly
# cf.go_offline()
# themes
# import mplcyberpunk
# set matplotlib formats
%matplotlib inline
import time
endsnippet


# ==========================
# ========== machine learning ======
# ==========================

# options im-> inword
snippet split_validation "for setting the split validation"
from sklearn.model_selection import train_test_split

# split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
# X is the parameters given and y is the target
# random state is the seed for the train -- use the same seed for same results

train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)
# Define model
melbourne_model = DecisionTreeRegressor()
# Fit model
melbourne_model.fit(train_X, train_y)

# get predicted prices on validation data
val_predictions = melbourne_model.predict(val_X)
print(mean_absolute_error(val_y, val_predictions))
endsnippet

# options im-> inword
snippet model_descition_tree "for a basic model"
from sklearn.tree import DecisionTreeRegressor

# Define model. Specify a number for random_state to ensure same results each run
melbourne_model = DecisionTreeRegressor(random_state=1)
# X is the parameters and y is the target
# Fit model
melbourne_model.fit(X, y)

print("Making predictions for the following 5 houses:")
print(X.head())
print("The predictions are")
print(melbourne_model.predict(X.head()))
endsnippet

# options im-> inword
snippet model_random_forest "is the descition tree with randomness"
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# X is the parameters and y is the target
forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
endsnippet

# options im-> inword
snippet getMAE "for getting the mean absolute error of a basic model"
# this is to get the error testing different size of tree (tree nodes)
# is meant to be used in a loop

def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
	model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
	model.fit(train_X, train_y)
	preds_val = model.predict(val_X)
	mae = mean_absolute_error(val_y, preds_val)
	return(mae)
endsnippet

# options im-> inword
snippet getBestTreeSize "for getting the best tree size"
# these are just random numbers used for finding the correct tree size
# maybe a binary search can be apropiate

candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]
# Write loop to find the ideal tree size from candidate_max_leaf_nodes
prev = float('inf')
tree_size = candidate_max_leaf_nodes[0]

for leaf_nodes in candidate_max_leaf_nodes:
	mae = get_mae(leaf_nodes,train_X,val_X,train_y,val_y)
	if(prev > mae):
		tree_size = leaf_nodes
		prev = mae

# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)
best_tree_size = tree_size

endsnippet

# options im-> inword
snippet export_csv "for exporting a csv file"
# Run the code to save predictions in the format used for competition scoring

output = pd.DataFrame({'Id': test_data.Id,
	'SalePrice': test_preds})
output.to_csv('submission.csv', index=False)
endsnippet

# ================================
# ========== graphs ==============
# ================================

snippet graphs "for importing the necessary for machine learning plotting and other things"
sns.lineplot(data=fifa_data)
sns.barplot(x=flight_data.index, y=flight_data['NK'])
# must of the arguments here apply to all the graphs
sns.heatmap(
			df.corr(),
			cmap = colormap,
			square=True,
			cbar_kws={'shrink':.9},
			ax=ax,
			annot=True,
			linewidths=0.1,vmax=1.0, linecolor='white',
			annot_kws={'fontsize':12 },
			col = 'value',
			col_wrap = 3
)
sns.scatterplot(x=insurance_data['bmi'], y=insurance_data['charges'])
sns.regplot(x=insurance_data['bmi'], y=insurance_data['charges'])
sns.distplot(a=iris_vir_data['Petal Length (cm)'], label="Iris-virginica", kde=False)
sns.kdeplot(data=iris_ver_data['Petal Length (cm)'], label="Iris-versicolor", shade=True)
sns.jointplot(x=iris_data['Petal Length (cm)'], y=iris_data['Sepal Width (cm)'], kind="kde")
sns.set_style("dark")
sns.countplot(x = 'pclass', data = titanic, hue = 'survived')

np.random.seed(0)
N = 20
theta = np.linspace(0.0, np.pi * 2 , N , endpoint = False)
radii = 10 * np.random.rand(N)
width = np.pi / 4 * np.random.rand(N)
colors = plt.cm.plasma(radii / 10)

plt.figure(dpi = 100)
ax = plt.subplot(1,1,1, projection = 'polar')
ax.bar(theta,radii,width = width, bottom = 0, color = colors, alpha = 0.7)
plt.show()
endsnippet

# options im-> inword
snippet graphsSubplots "for plotting subplots"
rows = 4
cols = 4

fig, ax = plt.subplots(rows, cols, figsize = (20,20))

	col = data.columns
	index = 0
	limit = 14

	for i in range(rows):
		for j in range(cols):

			if index >= limit:
			continue
			sns.scatterplot(x = 'Price', y = col[index], data = data, ax = ax[i][j])
			ax[i][j].tick_params('x', labelrotation=45)
# change font size of y axis label
ax[i][j].yaxis.label.set_size(20)
# change font size of x axis label
ax[i][j].xaxis.label.set_size(20)
	index = index + 1


	plt.tight_layout()
plt.show()

# =========================
# ====== graphs over graphs ===========
# =========================


# %%
# create a subplot with a small subplot inside it
fig, ax = plt.subplots(1,1,figsize = (10,10))
sns.lineplot( y = 'arr1', x = df.index, data = df, ax = ax)
ax2 = fig.add_axes([0.8,0.8,0.2,0.2])
sns.scatterplot(x = 'arr2', y = 'arr3', data = df, ax = ax2)
plt.show()

# ==== overlap joinplot
g = sns.jointplot(x = data['RM'], y = data['Price'], kind = 'kde', color = 'm')
g.plot_joint(plt.scatter, c = 'r', s = 40, linewidth = 1, marker = '+')
g.ax_joint.collections[0].set_alpha(0.3)


endsnippet

# options im-> inword
snippet igraphs "for plotting interactive graphs"
# ============================ treemap and example for plotting
fig = px.treemap(full_latest.sort_values(by = 'Confirmed', ascending = False).reset_index(drop = True),
		path = ['Country', 'Province/State'], values = 'Confirmed', height = 700,
		title = 'Number of Confirmed Cases',
		color_discrete_sequence = px.colors.qualitative.Dark2)

#  ===================== line graph with hue as a columns
fig = px.line(country_daywise, x = 'Date', y = 'Confirmed', color = 'Country', height = 600,
		title='Confirmed', color_discrete_sequence = px.colors.cyclical.mygbm)
fig.show()

#  ======================= choropleth map animation
	fig = px.choropleth(country_daywise, locations= 'Country', locationmode='country names', color = np.log(country_daywise['Confirmed']),
			hover_name = 'Country', animation_frame=country_daywise['Date'].dt.strftime('%Y-%m-%d'),
			title='Cases over time', color_continuous_scale=px.colors.sequential.Inferno)

fig.show()


# ======================== heatmap
	fig = px.density_mapbox(df, lat = 'Lat', lon = 'Long', hover_name = 'Country', hover_data = ['Confirmed', 'Recovered', 'Deaths'], animation_frame='Date', color_continuous_scale='Portland', radius = 7, zoom = 0, height= 700)
	fig.update_layout(title = 'Worldwide Covid-19 Cases with Time Laps')
	fig.update_layout(mapbox_style = 'open-street-map', mapbox_center_lon = 0)
fig.show()




	top = 15
	fig = px.scatter(countywise.sort_values('Deaths', ascending = False).head(top),
			x = 'Confirmed', y = 'Deaths', color = 'Country', size = 'Confirmed', height = 600,
			text = 'Country', log_x = True, log_y = True, title='Deaths vs Confirmed Cases (Cases are on log10 scale)')

	fig.update_traces(textposition = 'top center')
fig.update_layout(showlegend = False)
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()

# =================== timeline

first_date = df[df['Confirmed'] > 0 ]
first_date = first_date.groupby('Country')['Date'].agg(['min']).reset_index()
last_date = df.groupby('Country')['Date'].agg(['max']).reset_index()
last_date = last_date.drop(columns = ['Country'],axis = 1)
first_last = pd.concat([first_date,last_date],axis = 1)
first_last = first_last.rename(columns = {"max":"Finish","min":"Start","Country":"Task"})
fig = ff.create_gantt(first_last,height = 3000)
fig.show()



# =========================== folium

m = folium.Map(location=[0,0], tiles = 'cartodbpositron',min_zoom = 1, max_zoom = 4, zoom_start=1)

for i in range(0,len(data)):
	tooltip = '<li><bold> Country:' + str(data.iloc[i]['Country']) +
	'<li><bold> Year:' + str(data.iloc[i]['Year'])
	folium.Circle(
			location=[data.iloc[i]['lat'], data.iloc[i]['lon']],
			popup=data.iloc[i]['name'],
			radius=100000,
			color='crimson',
			fill=True,
			fill_color='crimson'
			).add_to(m)

endsnippet

snippet pairplot "for creating a pair plot"

# %%



def PairGrid(data, hue = None, regplot = False):

	# measure the time it takes to run the code
	prev = time.time()
	column_names = data.columns.to_list()

	# plot the data using seaborn
	if hue == None:
		g = sns.PairGrid(data)
	else:
		g = sns.PairGrid(data, hue = hue,palette = ["blue","orange","green"])

	g.map_upper(sns.scatterplot, s = 6)
	g.map_lower(sns.kdeplot, alpha = 0.5)
	g.map_diag(sns.histplot, kde = True, alpha = 0.5)

	xlabels,ylabels = [],[]

	# get the x labels
	for ax in g.axes[-1,:]:
		xlabel = ax.xaxis.get_label_text()
		xlabels.append(xlabel)

	# get the y labels
	for ax in g.axes[:,0]:
		ylabel = ax.yaxis.get_label_text()
		ylabels.append(ylabel)

	# set all the axes
	for i in range(len(xlabels)):
		for j in range(len(ylabels)):
			g.axes[j,i].xaxis.set_label_text(xlabels[i], visible = True)
			g.axes[j,i].yaxis.set_label_text(ylabels[j],visible = True)

	for ax in g.axes.flat:
		# get the ax position
		pos = ax.get_position()
		# check if the ax is below the main diagonal
		# example
		# 0 1 2
		# 3 4 5
		# 6 7 8
		# if the ax is 3, 6, 7 then it is below the main diagonal


		xlabel = ax.xaxis.get_label_text()
		ylabel = ax.yaxis.get_label_text()
		xindex = column_names.index(xlabel)
		yindex = column_names.index(ylabel)

		below = False
		if xindex < yindex:
			below = True


		# set xlabel and y label text visible
		ax.tick_params(labelleft = True, labelbottom = True)
		ax.xaxis.label.set_visible(True)
		ax.yaxis.label.set_visible(True)
		ax.grid(True)

		color_palette = ["blue","orange","green"]
		# create a regplot for the current ax
		if hue != None and regplot == True and below == False:
			colors = {}
			for i in range(len(data[hue].unique())):
				colors[data[hue].unique()[i]] = color_palette[i%len(color_palette)]
			if ax.get_xlabel() != ax.get_ylabel():
				tcounter = 0
				for cat in data[hue].unique():
					tmp_data = data[data[hue] == cat]
					sns.regplot(x = ax.get_xlabel(), y = ax.get_ylabel(), data = tmp_data, ax = ax, color = colors[cat], scatter = False, line_kws = {"alpha":0.5})
					tcounter+=1

	g.tight_layout()
	# show hue legend
	g.add_legend()
	print( "time(s) : "  + str(time.time() - prev))

endsnippet


# ================================
# ==  feature engineering ========
# ================================

snippet mutual_info_regression "for creating a mutual info regression"
from sklearn.feature_selection import mutual_info_regression

# where X is the whole dataset and the y is the target
# the discrete features are the integer values
def make_mi_scores(X, y, discrete_features):
	mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
	mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
	mi_scores = mi_scores.sort_values(ascending=False)
	return mi_scores

mi_scores = make_mi_scores(X, y, discrete_features)
mi_scores[::3]  # show a few features with their MI scores
endsnippet


snippet clustering "for clustering information"
# Create cluster feature
kmeans = KMeans(n_clusters=6)
X["Cluster"] = kmeans.fit_predict(X)
X["Cluster"] = X["Cluster"].astype("category")

X.head()


sns.relplot(
	x="Longitude", y="Latitude", hue="Cluster", data=X, height=6,
);
endsnippet


# ==================================
# == intermediate machine learning =
# ==================================

# options im-> inword
snippet scoreModels "for scoring the models"
from sklearn.ensemble import RandomForestRegressor

# Define the models
model_1 = RandomForestRegressor(n_estimators=50, random_state=0)
model_2 = RandomForestRegressor(n_estimators=100, random_state=0)
model_3 = RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)
model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)
model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)

models = [model_1, model_2, model_3, model_4, model_5]

from sklearn.metrics import mean_absolute_error

# Function for comparing different models
def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):
	model.fit(X_t, y_t)
	preds = model.predict(X_v)
	return mean_absolute_error(y_v, preds)

for i in range(0, len(models)):
	mae = score_model(models[i])
	print("Model %d MAE: %d" % (i+1, mae))


endsnippet

# options im-> inword
snippet imputter "for replacing missing values with a simple regression"
from sklearn.impute import SimpleImputer

# Imputation
my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

# Imputation removed column names; put them back
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns

print("MAE from Approach 2 (Imputation):")
print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))
endsnippet

# options im-> inword
snippet completeImputter "for using imputer and specifying that the imputter was performed in a column"
# Make copy to avoid changing original data (when imputing)
X_train_plus = X_train.copy()
X_valid_plus = X_valid.copy()

# Make new columns indicating what will be imputed
for col in cols_with_missing:
	X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()
	X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()

# Imputation
my_imputer = SimpleImputer()
imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))
imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))

# Imputation removed column names; put them back
imputed_X_train_plus.columns = X_train_plus.columns
imputed_X_valid_plus.columns = X_valid_plus.columns

print("MAE from Approach 3 (An Extension to Imputation):")
print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))
endsnippet

# options im-> inword
snippet ordinalEncoder "for categorical values to encode them in integer colum"
from sklearn.preprocessing import OrdinalEncoder

# Make copy to avoid changing original data 
label_X_train = X_train.copy()
label_X_valid = X_valid.copy()

# Apply ordinal encoder to each column with categorical data
ordinal_encoder = OrdinalEncoder()
label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])

print("MAE from Approach 2 (Ordinal Encoding):") 
print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))
endsnippet


# options im-> inword
snippet oneHotEncoding "like the above but particullarly usefull when you have a column that encapsulates mo categories"
from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = X_train.drop(object_cols, axis=1)
num_X_valid = X_valid.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)

print("MAE from Approach 3 (One-Hot Encoding):") 
print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))
endsnippet

# options im-> inword
snippet splitOneHotEncoding "for using ordinalEncoder on columns that have a lot of unique values"
## for separating the high unique values columns
# Columns that will be one-hot encoded
low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]

# Columns that will be dropped from the dataset
high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))

print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)
print('\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)

# now one hot encoding and cardinality encoding


from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = X_train.index
OH_cols_valid.index = X_valid.index

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = X_train.drop(object_cols, axis=1)
num_X_valid = X_valid.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)
endsnippet

# options im-> inword
snippet pipeLine "for using a pipieline for better performance and less bugs"
# Step 1: Define Preprocessing Steps
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Preprocessing for numerical data
numerical_transformer = SimpleImputer(strategy='constant')

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
	('imputer', SimpleImputer(strategy='most_frequent')),
	('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
	transformers=[
		('num', numerical_transformer, numerical_cols),
		('cat', categorical_transformer, categorical_cols)
	])
# Step 2: Define the Model

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=0)




# Step 3: Create and Evaluate the Pipeline
from sklearn.metrics import mean_absolute_error

# Bundle preprocessing and modeling code in a pipeline
my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', model)])

# Preprocessing of training data, fit model 
my_pipeline.fit(X_train, y_train)

# Preprocessing of validation data, get predictions
preds = my_pipeline.predict(X_valid)

# Evaluate the model
score = mean_absolute_error(y_valid, preds)
print('MAE:', score)
endsnippet


# options im-> inword
snippet crossValidation "for using crossvalidation with a pipeline and a regressionForest"
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),('model',RandomForestRegressor(n_estimators=50,random_state=0))])

from sklearn.model_selection import cross_val_score

# Multiply by -1 since sklearn calculates *negative* MAE
scores = -1 * cross_val_score(my_pipeline, X, y,cv=5,scoring='neg_mean_absolute_error')

print("MAE scores:\n", scores)

endsnippet

# options im-> inword
snippet xgboost "for using the better xgboost"
from xgboost import XGBRegressor

my_model = XGBRegressor()
my_model.fit(X_train, y_train)

from sklearn.metrics import mean_absolute_error

predictions = my_model.predict(X_valid)
print("Mean Absolute Error: " + str(mean_absolute_error(predictions, y_valid)))

endsnippet






# options im-> inword
snippet dataLeackage "for looking and dropping data leackage"


## first we test the model and see that it has 98% of accuracy
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!)
my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))
cv_scores = cross_val_score(my_pipeline, X, y, cv=5,scoring='accuracy')

print("Cross-validation accuracy: %f" % cv_scores.mean())

## then we can see that the expenditure is creating the leackage
expenditures_cardholders = X.expenditure[y]
expenditures_noncardholders = X.expenditure[~y]

print('Fraction of those who did not receive a card and had no expenditures: %.2f' \
	%((expenditures_noncardholders == 0).mean()))
print('Fraction of those who received a card and had no expenditures: %.2f' \
	%(( expenditures_cardholders == 0).mean()))


## so we fix the leackage
# Drop leaky predictors from dataset
potential_leaks = ['expenditure', 'share', 'active', 'majorcards']
X2 = X.drop(potential_leaks, axis=1)

# Evaluate the model with leaky predictors removed
cv_scores = cross_val_score(my_pipeline, X2, y, cv=5,scoring='accuracy')

print("Cross-val accuracy: %f" % cv_scores.mean())
endsnippet




# options im-> inword
snippet loadBigQuery "for loading the python big query module"
from google.cloud import bigquery


# Create a "Client" object
client = bigquery.Client()


# Construct a reference to the "hacker_news" dataset
dataset_ref = client.dataset("hacker_news", project="bigquery-public-data")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

# List all the tables in the "hacker_news" dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there are four!)
for table in tables:  
	print(table.table_id)


# Construct a reference to the "full" table
table_ref = dataset_ref.table("full")

# API request - fetch the table
table = client.get_table(table_ref)

# Print information on all the columns in the "full" table in the "hacker_news" dataset
table.schema


# Preview the first five lines of the "full" table
client.list_rows(table, max_results=5).to_dataframe()

# Preview the first five entries in the "by" column of the "full" table
client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()

endsnippet

# options im-> inword
snippet BQ_query "for executing a sql command in big query"

from google.cloud import bigquery

# Create a "Client" object
client = bigquery.Client()

# Construct a reference to the "openaq" dataset
dataset_ref = client.dataset("openaq", project="bigquery-public-data")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

# List all the tables in the "openaq" dataset
tables = list(client.list_tables(dataset))

# Print names of all tables in the dataset (there's only one!)
for table in tables:  
	print(table.table_id)

# Construct a reference to the "global_air_quality" table
table_ref = dataset_ref.table("global_air_quality")

# API request - fetch the table
table = client.get_table(table_ref)

# Preview the first five lines of the "global_air_quality" table
client.list_rows(table, max_results=5).to_dataframe()



#### now actually the query

# Query to select all the items from the "city" column where the "country" column is 'US'
query = """
	SELECT city
	FROM `bigquery-public-data.openaq.global_air_quality`
	WHERE country = 'US'
	"""

# Create a "Client" object
client = bigquery.Client()

# Set up the query
query_job = client.query(query)


# API request - run the query, and return a pandas DataFrame
us_cities = query_job.to_dataframe()

# What five cities have the most measurements?
us_cities.city.value_counts().head()




endsnippet

# options im-> inword
snippet BQ_big_queries "for executing a big query without exceding the limit"
### you can test wheter the query is going to be very huge


# Query to get the score column from every row where the type column has value "job"
query = """
	SELECT score, title
	FROM `bigquery-public-data.hacker_news.full`
	WHERE type = "job" 
	"""

# Create a QueryJobConfig object to estimate size of query without running it
dry_run_config = bigquery.QueryJobConfig(dry_run=True)

# API request - dry run query to estimate costs
dry_run_query_job = client.query(query, job_config=dry_run_config)

print("This query will process {} bytes.".format(dry_run_query_job.total_bytes_processed))



### especify the limit and execut the query
# Only run the query if it's less than 1 MB
ONE_MB = 1000*1000
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)

# Set up the query (will only run if it's less than 1 MB)
safe_query_job = client.query(query, job_config=safe_config)

# API request - try to run the query, and return a pandas DataFrame
safe_query_job.to_dataframe()

endsnippet


# options im-> inword
snippet BQ_groupBy_having_count ""
from google.cloud import bigquery

# Create a "Client" object
client = bigquery.Client()

# Construct a reference to the "hacker_news" dataset
dataset_ref = client.dataset("hacker_news", project="bigquery-public-data")

# API request - fetch the dataset
dataset = client.get_dataset(dataset_ref)

# Construct a reference to the "comments" table
table_ref = dataset_ref.table("comments")

# API request - fetch the table
table = client.get_table(table_ref)

# Preview the first five lines of the "comments" table
client.list_rows(table, max_results=5).to_dataframe()


### this is where the code starts


# Query to select comments that received more than 10 replies
query_popular = """
	SELECT parent, COUNT(id)
	FROM `bigquery-public-data.hacker_news.comments`
	GROUP BY parent
	HAVING COUNT(id) > 10
	"""

# Set up the query (cancel the query if it would use too much of 
# your quota, with the limit set to 10 GB)
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
query_job = client.query(query_popular, job_config=safe_config)

# API request - run the query, and convert the results to a pandas DataFrame
popular_comments = query_job.to_dataframe()

# Print the first five rows of the DataFrame
popular_comments.head()



endsnippet


# options im-> inword
snippet BQ_orderBy "for working with the order by"
query = """
select id,name,animal
from `bigquery-public-data.pet_records.pets`
order by id
"""
endsnippet

# options im-> inword
snippet BQ_dates "for working with dates on big query"
# Query to find out the number of accidents for each day of the week
query = """
	SELECT COUNT(consecutive_number) AS num_accidents, 
		EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week
	FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`
	GROUP BY day_of_week
	ORDER BY num_accidents DESC
	"""
# also check the documentation for extract method

#########
##### convert the datetime to date
######
query_with_CTE = """ 
	WITH time AS 
	(
		SELECT DATE(block_timestamp) AS trans_date
		FROM `bigquery-public-data.crypto_bitcoin.transactions`
	)
	SELECT COUNT(1) AS transactions,
		trans_date
	FROM time
	GROUP BY trans_date
	ORDER BY trans_date
	"""

# Set up the query (cancel the query if it would use too much of 
# your quota, with the limit set to 10 GB)
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
query_job = client.query(query_with_CTE, job_config=safe_config)

# API request - run the query, and convert the results to a pandas DataFrame
transactions_by_date = query_job.to_dataframe()

# Print the first five rows
transactions_by_date.head()
endsnippet


# options im-> inword
snippet BQ_with_as "for nested queries better readability"

query = """
with seniors as 
(
	select id,name
	from `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`
	where years_old > 5
)
"""

select id
from seniors
endsnippet

# options im-> inword
snippet BQ_join "for join in big query"
# Query to determine the number of files per license, sorted by number of files
query = """
	SELECT L.license, COUNT(1) AS number_of_files
	FROM `bigquery-public-data.github_repos.sample_files` AS sf
	INNER JOIN `bigquery-public-data.github_repos.licenses` AS L 
		ON sf.repo_name = L.repo_name
	GROUP BY L.license
	ORDER BY number_of_files DESC
	"""

# Set up the query (cancel the query if it would use too much of 
# your quota, with the limit set to 10 GB)
safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)
query_job = client.query(query, job_config=safe_config)

# API request - run the query, and convert the results to a pandas DataFrame
file_count_by_license = query_job.to_dataframe()
endsnippet



# == advanced BQ
# options im-> inword
snippet BQ_left_join "for doing the big query left join"
# Query to select all stories posted on January 1, 2012, with number of comments
join_query = """
	WITH c AS
	(
	SELECT parent, COUNT(*) as num_comments
	FROM `bigquery-public-data.hacker_news.comments` 
	GROUP BY parent
	)
	SELECT s.id as story_id, s.by, s.title, c.num_comments
	FROM `bigquery-public-data.hacker_news.stories` AS s
	LEFT JOIN c
	ON s.id = c.parent
	WHERE EXTRACT(DATE FROM s.time_ts) = '2012-01-01'
	ORDER BY c.num_comments DESC
	"""

# Run the query, and return a pandas DataFrame
join_result = client.query(join_query).result().to_dataframe()
join_result.head()
endsnippet


# options im-> inword
snippet BQ_over_clause ""
# Query to count the (cumulative) number of trips per day
num_trips_query = """
	WITH trips_by_day AS
	(
	SELECT DATE(start_date) AS trip_date,
		COUNT(*) as num_trips
	FROM `bigquery-public-data.san_francisco.bikeshare_trips`
	WHERE EXTRACT(YEAR FROM start_date) = 2015
	GROUP BY trip_date
	)
	SELECT *,
		SUM(num_trips) 
			OVER (
				ORDER BY trip_date
				ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
				) AS cumulative_trips
		FROM trips_by_day
	"""

# Run the query, and return a pandas DataFrame
num_trips_result = client.query(num_trips_query).result().to_dataframe()
num_trips_result.head()


endsnippet

# options im-> inword
snippet  partition_by "for using the partition clause"
# Query to track beginning and ending stations on October 25, 2015, for each bike
start_end_query = """
	SELECT bike_number,
		TIME(start_date) AS trip_time,
		FIRST_VALUE(start_station_id)
			OVER (
				PARTITION BY bike_number
				ORDER BY start_date
				ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
				) AS first_station_id,
				E(end_station_id)
				(
				PARTITION BY bike_number
				ORDER BY start_date
				ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
				) AS last_station_id,
		start_station_id,
		end_station_id
	FROM `bigquery-public-data.san_francisco.bikeshare_trips`
	WHERE DATE(start_date) = '2015-10-25' 
	"""

# Run the query, and return a pandas DataFrame
start_end_result = client.query(start_end_query).result().to_dataframe()
start_end_result.head()
endsnippet


# options im-> inword
snippet BQ_unest "for unesting nested data or repeated data"

# Your code here
all_langs_query = """
	SELECT l.name, l.bytes
	FROM `bigquery-public-data.github_repos.languages`,
		UNNEST(language) as l
	WHERE repo_name = 'polyrabbit/polyglot'
	ORDER BY l.bytes DESC
	"""
endsnippet





# options im-> inword
snippet BQ_speed_functions "usable functions to see the efficiency of the queries"

from google.cloud import bigquery
from time import time

client = bigquery.Client()

def show_amount_of_data_scanned(query):
	# dry_run lets us see how much data the query uses without running it
	dry_run_config = bigquery.QueryJobConfig(dry_run=True)
	query_job = client.query(query, job_config=dry_run_config)
	print('Data processed: {} GB'.format(round(query_job.total_bytes_processed / 10**9, 3)))

def show_time_to_run(query):
	time_config = bigquery.QueryJobConfig(use_query_cache=False)
	start = time()
	query_result = client.query(query, job_config=time_config).result()
	end = time()
	print('Time to run: {} seconds'.format(round(end-start, 3)))

endsnippet

# === intro to deep learning

# options im-> inword
snippet model1LinearUnit "for creating a model with one linear unit"
from tensorflow import keras
from tensorflow.keras import layers

# Create a network with 1 linear unit
model = keras.Sequential([
	layers.Dense(units=1, input_shape=[3])
])
endsnippet


# options im-> inword
snippet getWeightsAndBiases "for getting weights and biases"
w, b = model.weights

print("Weights\n{}\n\nBias\n{}".format(w, b)
endsnippet

# options im-> inword
snippet plotOutputOfUntrained "for plotting the output of an untrained linear model"
import tensorflow as tf
import matplotlib.pyplot as plt

model = keras.Sequential([
	layers.Dense(1, input_shape=[1]),
])

x = tf.linspace(-1.0, 1.0, 100)
y = model.predict(x)

plt.figure(dpi=100)
plt.plot(x, y, 'k')
plt.xlim(-1, 1)
plt.ylim(-1, 1)
plt.xlabel("Input: x")
plt.ylabel("Target y")
w, b = model.weights # you could also use model.get_weights() here
plt.title("Weight: {:0.2f}\nBias: {:0.2f}".format(w[0][0], b[0]))
plt.show()
endsnippet

# options im-> inword
snippet buildSequencialModel "this is for creating a secuencial model"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	# the hidden ReLU layers
	layers.Dense(units=4, activation='relu', input_shape=[2]),
	layers.Dense(units=3, activation='relu'),
	# the linear output layer 
	layers.Dense(units=1),
])


endsnippet

# options im-> inword
snippet loss_optimizer "for adding a loss and the optimizer"
model.compile(
	optimizer="adam",
	loss="mae",
)

endsnippet

# options im-> inword
snippet train_dl "for training a deep learning model"
history = model.fit(
	X_train, y_train,
	validation_data=(X_valid, y_valid),
	batch_size=256,
	epochs=10,
)

endsnippet

# options im-> inword
snippet plotLosstrainingData "for plotting loss training data"
import pandas as pd

# convert the training history to a dataframe
history_df = pd.DataFrame(history.history)
# use Pandas native plot method
history_df['loss'].plot();
endsnippet

# options im-> inword
snippet animateDeepLearning "for animating a deep learnig project"
from learntools.deep_learning_intro.dltools import animate_sgd


# YOUR CODE HERE: Experiment with different values for the learning rate, batch size, and number of examples
learning_rate = 0.05
batch_size = 32
num_examples = 256

animate_sgd(
	learning_rate=learning_rate,
	batch_size=batch_size,
	num_examples=num_examples,
	# You can also change these, if you like
	steps=50, # total training steps (batches seen)
	true_w=3.0, # the slope of the data
	true_b=2.0, # the bias of the data
)
endsnippet


# options im-> inword
snippet earlyStopping "for getting an early stopping"

from tensorflow import keras
from tensorflow.keras import layers, callbacks

early_stopping = callbacks.EarlyStopping(
	min_delta=0.001, # minimium amount of change to count as an improvement
	patience=20, # how many epochs to wait before stopping
	restore_best_weights=True,
	
	
	l = keras.Sequential([
	layers.Dense(512, activation='relu', input_shape=[11]),
	layers.Dense(512, activation='relu'),
	layers.Dense(512, activation='relu'),
	layers.Dense(1),
	
	l.compile(
	optimizer='adam',
	loss='mae',
)


history = model.fit(
	X_train, y_train,
	validation_data=(X_valid, y_valid),
	batch_size=256,
	epochs=500,
	callbacks=[early_stopping], # put your callbacks in a list
	verbose=0,  # turn off training log
)

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
print("Minimum validation loss: {}".format(history_df['val_loss'].min()))

endsnippet




# options im-> inword
snippet dropout_batchNormalization "for using dropout and batch normalization"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Dense(1024, activation='relu', input_shape=[11]),
	layers.Dropout(0.3),
	layers.BatchNormalization(),
	layers.Dense(1024, activation='relu'),
	layers.Dropout(0.3),
	layers.BatchNormalization(),
	layers.Dense(1024, activation='relu'),
	layers.Dropout(0.3),
	layers.BatchNormalization(),
	layers.Dense(1),
])
endsnippet



# options im-> inword
snippet binaryClassification "for classifying in bynary"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Dense(4, activation='relu', input_shape=[33]),
	layers.Dense(4, activation='relu'),    
	layers.Dense(1, activation='sigmoid'),
])

model.compile(
	optimizer='adam',
	loss='binary_crossentropy',
	metrics=['binary_accuracy'],
)
# include early stopping
early_stopping = keras.callbacks.EarlyStopping(
	patience=10,
	min_delta=0.001,
	restore_best_weights=True,
)

history = model.fit(
	X_train, y_train,
	validation_data=(X_valid, y_valid),
	batch_size=512,
	epochs=1000,
	callbacks=[early_stopping],
	verbose=0, # hide the output because we have so many epochs
)



endsnippet


# options im-> inword
snippet define_pretrained_base "for defining the pre trained base"
pretrained_base = tf.keras.models.load_model(
	'../input/cv-course-models/cv-course-models/vgg16-pretrained-base',
)
pretrained_base.trainable = False
endsnippet

# options im-> inword
snippet attachHeAD "for attaching the head"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	pretrained_base,
	layers.Flatten(),
	layers.Dense(6, activation='relu'),
	layers.Dense(1, activation='sigmoid'),
])
endsnippet


# options im-> inword
snippet CV_train "for training a simple computer vision algoritm"
model.compile(
	optimizer='adam',
	loss='binary_crossentropy',
	metrics=['binary_accuracy'],
)

history = model.fit(
	ds_train,
	validation_data=ds_valid,
	epochs=30,
	verbose=0,
)
endsnippet

# options im-> inword
snippet CV_plot_acc "for plotting the binary accuracy"
import pandas as pd

history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();
endsnippet



# options im-> inword
snippet filterWithConvulsion "for filtering with convulsion"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Conv2D(filters=64, kernel_size=3), # activation is None
	# More layers follow
])
endsnippet


# options im-> inword
snippet relu_activation "for activating the relu with te conv2d"
model = keras.Sequential([
	layers.Conv2D(filters=64, kernel_size=3, activation='relu')
	# More layers follow
])
endsnippet

# options im-> inword
snippet CV_tfKernel "for defining a basic tensorflow kernel"
kernel = tf.constant([
	[-2, -1, 0],
	[-1, 1, 1],
	[0, 1, 2],
])
endsnippet

# options im-> inword
snippet condenseWithMaximumPooling "adding condense with maximum pooling"
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Conv2D(filters=64, kernel_size=3), # activation is None
	layers.MaxPool2D(pool_size=2),
	# More layers follow
])
endsnippet


# options im-> inword
snippet imageCondensePool "for image condensing a pool"
import tensorflow as tf

image_condense = tf.nn.pool(
	input=image_detect, # image in the Detect step above
	window_shape=(2, 2),
	pooling_type='MAX',
	# we'll see what these do in the next lesson!
	strides=(2, 2),
	padding='SAME',
)

plt.figure(figsize=(6, 6))
plt.imshow(tf.squeeze(image_condense))
plt.axis('off')
plt.show();
endsnippet


# options im-> inword
snippet globalAveragePooling ""
feature_maps = [visiontools.random_map([5, 5], scale=0.1, decay_power=4) for _ in range(8)]

gs = gridspec.GridSpec(1, 8, wspace=0.01, hspace=0.01)
plt.figure(figsize=(18, 2))
for i, feature_map in enumerate(feature_maps):
	plt.subplot(gs[i])
	plt.imshow(feature_map, vmin=0, vmax=1)
	plt.axis('off')
plt.suptitle('Feature Maps', size=18, weight='bold', y=1.1)
plt.show()

# reformat for TensorFlow
feature_maps_tf = [tf.reshape(feature_map, [1, *feature_map.shape, 1])
	for feature_map in feature_maps]

global_avg_pool = tf.keras.layers.GlobalAvgPool2D()
pooled_maps = [global_avg_pool(feature_map) for feature_map in feature_maps_tf]
img = np.array(pooled_maps)[:,:,0].T

plt.imshow(img, vmin=0, vmax=1)
plt.axis('off')
plt.title('Pooled Feature Maps')
plt.show();
endsnippet


# options im-> inword
snippet sliding_window "for the sliding window tecniques padding and stride"
show_extraction(
	image, kernel,
	
	# Window parameters
	conv_stride=1,
	pool_size=2,
	pool_stride=2,
	
	subplot_shape=(1, 4),
	figsize=(14, 6),
)
endsnippet

# options im-> inword
snippet customConvents "for defining a custom convent"
### define the model
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([

	# First Convolutional Block
	layers.Conv2D(filters=32, kernel_size=5, activation="relu", padding='same',
		# give the input dimensions in the first layer
		# [height, width, color channels(RGB)]
		input_shape=[128, 128, 3]),
	layers.MaxPool2D(),
	
	# Second Convolutional Block
	layers.Conv2D(filters=64, kernel_size=3, activation="relu", padding='same'),
	layers.MaxPool2D(),
	
	# Third Convolutional Block
	layers.Conv2D(filters=128, kernel_size=3, activation="relu", padding='same'),
	layers.MaxPool2D(),
	
	# Classifier Head
	layers.Flatten(),
	layers.Dense(units=6, activation="relu"),
	layers.Dense(units=1, activation="sigmoid"),
])
model.summary()

#### train the model
model.compile(
	optimizer=tf.keras.optimizers.Adam(epsilon=0.01),
	loss='binary_crossentropy',
	metrics=['binary_accuracy']
)

history = model.fit(
	ds_train,
	validation_data=ds_valid,
	epochs=40,
	verbose=0,
)

#### test the model
import pandas as pd

history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();
endsnippet



# options im-> inword
snippet dataAugmentation "for adding features to the cv model"
from tensorflow import keras
from tensorflow.keras import layers
# these are a new feature in TF 2.2
from tensorflow.keras.layers.experimental import preprocessing


pretrained_base = tf.keras.models.load_model(
	'../input/cv-course-models/cv-course-models/vgg16-pretrained-base',
)
pretrained_base.trainable = False

model = keras.Sequential([
	# Preprocessing
	preprocessing.RandomFlip('horizontal'), # flip left-to-right
	preprocessing.RandomContrast(0.5), # contrast change by up to 50%
	# Base
	pretrained_base,
	# Head
	layers.Flatten(),
	layers.Dense(6, activation='relu'),
	layers.Dense(1, activation='sigmoid'),
])
endsnippet


# options im-> inword
snippet RL_setup "for the setup on reinforcement learnig"
from kaggle_environments import make, evaluate

# Create the game environment
# Set debug=True to see the errors if your agent refuses to run
env = make("connectx", debug=True)

# List of available default agents
print(list(env.agents))
endsnippet

# options im-> inword
snippet definingAgents "for defining agents for the competition"
# Selects random valid column
def agent_random(obs, config):
	valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]
	return random.choice(valid_moves)

# Selects middle column
def agent_middle(obs, config):
	return config.columns//2

# Selects leftmost valid column
def agent_leftmost(obs, config):
	valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]
	return valid_moves[0]
endsnippet



# options im-> inword
snippet fitLinearRegression "for fitting a linear regression model from a forecasting task"
from sklearn.linear_model import LinearRegression

# Training data
X = df.loc[:, ['Time']]  # features
y = df.loc[:, 'NumVehicles']  # target

# Train the model
model = LinearRegression()
model.fit(X, y)

# Store the fitted values as a time series with the same time index as
# the training data
y_pred = pd.Series(model.predict(X), index=X.index)
endsnippet

# options im-> inword
snippet lagFeature "for adding a lag feature to a pandas dataframe"
df['Lag_1'] = df['NumVehicles'].shift(1)
df.head()
endsnippet

# options im-> inword
snippet moving_average_plot "for a moving average plot"
moving_average = tunnel.rolling(
	window=365,       # 365-day window
	center=True,      # puts the average at the center of the window
	min_periods=183,  # choose about half the window size
).mean()              # compute the mean (could also do median, std, min, max, ...)

ax = tunnel.plot(style=".", color="0.5")
moving_average.plot(
	ax=ax, linewidth=3, title="Tunnel Traffic - 365-Day Moving Average", legend=False,
);
endsnippet

# options im-> inword
snippet splines "an alternative to polinomials that is better"
from pyearth import Earth

# Target and features are the same as before
y = average_sales.copy()
dp = DeterministicProcess(index=y.index, order=1)
X = dp.in_sample()

# Fit a MARS model with `Earth`
model = Earth()
model.fit(X, y)

y_pred = pd.Series(model.predict(X), index=X.index)

ax = y.plot(**plot_params, title="Average Sales", ylabel="items sold")
ax = y_pred.plot(ax=ax, linewidth=3, label="Trend")
endsnippet

# options im-> inword
snippet plotSeasonability "for plotting seasonability"
X = tunnel.copy()

# days within a week
X["day"] = X.index.dayofweek  # the x-axis (freq)
X["week"] = X.index.week  # the seasonal period (period)

# days within a year
X["dayofyear"] = X.index.dayofyear
X["year"] = X.index.year
fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))
seasonal_plot(X, y="NumVehicles", period="week", freq="day", ax=ax0)
seasonal_plot(X, y="NumVehicles", period="year", freq="dayofyear", ax=ax1);

plot_periodogram(tunnel.NumVehicles);

endsnippet




# options im-> inword
snippet seasonalFeatures "for creating seasonal features"
from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess

fourier = CalendarFourier(freq="A", order=10)  # 10 sin/cos pairs for "A"nnual seasonality

dp = DeterministicProcess(
	index=tunnel.index,
	constant=True,               # dummy feature for bias (y-intercept)
	order=1,                     # trend (order 1 means linear)
	seasonal=True,               # weekly seasonality (indicators)
	additional_terms=[fourier],  # annual seasonality (fourier)
	drop=True,                   # drop terms to avoid collinearity
)

X = dp.in_sample()  # create features for dates in tunnel.index

endsnippet


# options im-> inword
snippet hibrid_forecasting "for creating hibrid models"
# 1. Train and predict with first model
model_1.fit(X_train_1, y_train)
y_pred_1 = model_1.predict(X_train)

# 2. Train and predict with second model on residuals
model_2.fit(X_train_2, y_train - y_pred_1)
y_pred_2 = model_2.predict(X_train_2)

# 3. Add to get overall predictions
y_pred = y_pred_1 + y_pred_2


# The `stack` method converts column labels to row labels, pivoting from wide format to long
X = retail.stack()  # pivot dataset wide to long
display(X.head())
y = X.pop('Sales')  # grab target series


# Turn row labels into categorical feature columns with a label encoding
X = X.reset_index('Industries')
# Label encoding for 'Industries' feature
for colname in X.select_dtypes(["object", "category"]):
	X[colname], _ = X[colname].factorize()

# Label encoding for annual seasonality
X["Month"] = X.index.month  # values are 1, 2, ..., 12

# Create splits
X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]
y_train, y_test = y.loc[idx_train], y.loc[idx_test]


# Pivot wide to long (stack) and convert DataFrame to Series (squeeze)
y_fit = y_fit.stack().squeeze()    # trend from training set
y_pred = y_pred.stack().squeeze()  # trend from test set

# Create residuals (the collection of detrended series) from the training set
y_resid = y_train - y_fit

# Train XGBoost on the residuals
xgb = XGBRegressor()
xgb.fit(X_train, y_resid)

# Add the predicted residuals onto the predicted trends
y_fit_boosted = xgb.predict(X_train) + y_fit
y_pred_boosted = xgb.predict(X_test) + y_pred
endsnippet

# options im-> inword
snippet GP_setup "for setting up a basic geopandas instance"
# Read in the data
full_data = gpd.read_file("../input/geospatial-learn-course-data/DEC_lands/DEC_lands/DEC_lands.shp")

# View the first five rows of the data
full_data.head()

type(full_data)

data = full_data.loc[:, ["CLASS", "COUNTY", "geometry"]].copy()

# How many lands of each type are there?
data.CLASS.value_counts()

# Select lands that fall under the "WILD FOREST" or "WILDERNESS" category
wild_lands = data.loc[data.CLASS.isin(['WILD FOREST', 'WILDERNESS'])].copy()
wild_lands.head()


wild_lands.plot()


# View the first five entries in the "geometry" column
wild_lands.geometry.head()


# Campsites in New York state (Point)
POI_data = gpd.read_file("../input/geospatial-learn-course-data/DEC_pointsinterest/DEC_pointsinterest/Decptsofinterest.shp")
campsites = POI_data.loc[POI_data.ASSET=='PRIMITIVE CAMPSITE'].copy()

# Foot trails in New York state (LineString)
roads_trails = gpd.read_file("../input/geospatial-learn-course-data/DEC_roadstrails/DEC_roadstrails/Decroadstrails.shp")
trails = roads_trails.loc[roads_trails.ASSET=='FOOT TRAIL'].copy()

# County boundaries in New York state (Polygon)
counties = gpd.read_file("../input/geospatial-learn-course-data/NY_county_boundaries/NY_county_boundaries/NY_county_boundaries.shp")

# Define a base map with county boundaries
ax = counties.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)

# Add wild lands, campsites, and foot trails to the base map
wild_lands.plot(color='lightgreen', ax=ax)
campsites.plot(color='maroon', markersize=2, ax=ax)
trails.plot(color='black', markersize=1, ax=ax)

endsnippet

# options im-> inword
snippet interactive_map "for creating an interactive map"
import folium
from folium import Choropleth, Circle, Marker
from folium.plugins import HeatMap, MarkerCluster

# Create a map
m_1 = folium.Map(location=[42.32,-71.0589], tiles='openstreetmap', zoom_start=10)

# Display the map
m_1

## plotting points

daytime_robberies = crimes[((crimes.OFFENSE_CODE_GROUP == 'Robbery') & \(crimes.HOUR.isin(range(9,18))))]




# add the marquers
# Create a map
m_2 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)

# Add points to the map
for idx, row in daytime_robberies.iterrows():
	Marker([row['Lat'], row['Long']]).add_to(m_2)

# Display the map
m_2

###
## for adding a marker cluster instead
##
# Create the map
m_3 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)

# Add points to the map
mc = MarkerCluster()
for idx, row in daytime_robberies.iterrows():
	if not math.isnan(row['Long']) and not math.isnan(row['Lat']):
		mc.add_child(Marker([row['Lat'], row['Long']]))
m_3.add_child(mc)

# Display the map
m_3

##
# for adding classic points
##

# Create a base map
m_4 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=13)

def color_producer(val):
	if val <= 12:
		return 'forestgreen'
	else:
		return 'darkred'

# Add a bubble map to the base map
for i in range(0,len(daytime_robberies)):
	Circle(
		location=[daytime_robberies.iloc[i]['Lat'], daytime_robberies.iloc[i]['Long']],
		radius=20,
		color=color_producer(daytime_robberies.iloc[i]['HOUR'])).add_to(m_4)

# Display the map
m_4


##
# for adding a heat map
##

# Create a base map
m_5 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)

# Add a heatmap to the base map
HeatMap(data=crimes[['Lat', 'Long']], radius=10).add_to(m_5)

# Display the map
m_5



endsnippet


# options im-> inword
snippet choropeth_i_map "for creating a choroplet map"
# GeoDataFrame with geographical boundaries of Boston police districts
districts_full = gpd.read_file('../input/geospatial-learn-course-data/Police_Districts/Police_Districts/Police_Districts.shp')
districts = districts_full[["DISTRICT", "geometry"]].set_index("DISTRICT")
districts.head()


# Number of crimes in each police district
plot_dict = crimes.DISTRICT.value_counts()
plot_dict.head()

# Create a base map
m_6 = folium.Map(location=[42.32,-71.0589], tiles='cartodbpositron', zoom_start=12)

# Add a choropleth map to the base map
Choropleth(geo_data=districts.__geo_interface__, 
	data=plot_dict, 
	key_on="feature.id", 
	fill_color='YlGnBu', 
	legend_name='Major criminal incidents (Jan-Aug 2018)'
	).add_to(m_6)

# Display the map
m_6
endsnippet



# options im-> inword
snippet circles_choropeth_i_map "for creating a choroplet map that has empty circles of differenct sizes pointing at density results"
# Create a base map
m_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)

# Create a map
def color_producer(magnitude):
	if magnitude > 6.5:
		return 'red'
	else:
		return 'green'

Choropleth(
	geo_data=prefectures['geometry'].__geo_interface__,
	data=stats['density'],
	key_on="feature.id",
	fill_color='BuPu',
	legend_name='Population density (per square kilometer)').add_to(m_4)

for i in range(0,len(earthquakes)):
	folium.Circle(
		location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],
		popup=("{} ({})").format(
			earthquakes.iloc[i]['Magnitude'],
			earthquakes.iloc[i]['DateTime'].year),
		radius=earthquakes.iloc[i]['Magnitude']**5.5,
		color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)

# Uncomment to see a hint
#q_4.a.hint()

# View the map
embed_map(m_4, 'q_4.html')
endsnippet



# options im-> inword
snippet geoencoder_map "for creating locations in a map"
from geopy.geocoders import Nominatim


geolocator = Nominatim(user_agent="kaggle_learn")
location = geolocator.geocode("Pyramid of Khufu")

print(location.point)
print(location.address)

point = location.point
print("Latitude:", point.latitude)
print("Longitude:", point.longitude)

universities = pd.read_csv("../input/geospatial-learn-course-data/top_universities.csv")
universities.head()

def my_geocoder(row):
	try:
		point = geolocator.geocode(row).point
		return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})
	except:
		return None

universities[['Latitude', 'Longitude']] = universities.apply(lambda x: my_geocoder(x['Name']), axis=1)

print("{}% of addresses were geocoded!".format(
	(1 - sum(np.isnan(universities["Latitude"])) / len(universities)) * 100))

# Drop universities that were not successfully geocoded
universities = universities.loc[~np.isnan(universities["Latitude"])]
universities = gpd.GeoDataFrame(
	universities, geometry=gpd.points_from_xy(universities.Longitude, universities.Latitude))
universities.crs = {'init': 'epsg:4326'}
universities.head()



##
# Create a map
##
m = folium.Map(location=[54, 15], tiles='openstreetmap', zoom_start=2)

# Add points to the map
for idx, row in universities.iterrows():
	Marker([row['Latitude'], row['Longitude']], popup=row['Name']).add_to(m)

# Display the map
m
endsnippet

# options im-> inword
snippet measure_distance "for measuring a distance in a map"
# measure the distance incident and every station
# Select one release incident in particular
recent_release = releases.iloc[360]

# Measure distance from release to each station
distances = stations.geometry.distance(recent_release.geometry)
distances
endsnippet


# options im-> inword
snippet create_a_buffer "for creating a buffer in geopandas"
two_mile_buffer = stations.geometry.buffer(2*5280)
two_mile_buffer.head()

# Create map with release incidents and monitoring stations
m = folium.Map(location=[39.9526,-75.1652], zoom_start=11)
HeatMap(data=releases[['LATITUDE', 'LONGITUDE']], radius=15).add_to(m)
for idx, row in stations.iterrows():
	Marker([row['LATITUDE'], row['LONGITUDE']]).add_to(m)

# Plot each polygon on the map
GeoJson(two_mile_buffer.to_crs(epsg=4326)).add_to(m)

# Show the map
m
endsnippet




# options im-> inword
snippet test_score_features "for testing the score of the features in your model"
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')
y = (data['Man of the Match'] == "Yes")  # Convert from string "Yes"/"No" to binary
feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]
X = data[feature_names]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
my_model = RandomForestClassifier(n_estimators=100,random_state=0).fit(train_X, train_y)

import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)
eli5.show_weights(perm, feature_names = val_X.columns.tolist())



endsnippet



# options im-> inword
snippet sk_tree_lc "for leetcode trees"

# Date: `date +%d/%B/%Y\ -\ %A`
# Author: Virgilio Murillo Ochoa
# personal github: Virgilio-AI
# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
# contact: virgiliomurilloochoa1@gmail.com
# web: virgiliomurillo.com

from typing import *
import heapq as hp
from collections import deque
from collections import defaultdict
import sys

# dic = defaultdict(int)
# set = set() # .remove(val),.add(val)
# dic = {} # .remove(id),dic[key] = val
# arr = []

class TreeNode:
	def __init__(self, x):
		self.val = x
		self.left = None
		self.right = None
# main part


# build the tree
def tree_builder(values):
	if not values:
		return None
	root = TreeNode(values[0])
	queue = collections.deque([root])
	leng = len(values)
	nums = 1
	while nums < leng:
		node = queue.popleft()
		if node:
			node.left = TreeNode(values[nums]) if values[nums] else None
			queue.append(node.left)
			if nums + 1 < leng:
				node.right = TreeNode(values[nums+1]) if values[nums+1] else None
				queue.append(node.right)
				nums += 1
			nums += 1
	return root

# def input(f=open(".`!p snip.rv = snip.basename`_In1.txt")): return f.readline().rstrip() # uncomment for debugging


def main():
	arr = [1,2,3,None]
	tree = tree_builder(arr)
	# this lines are for testing the solution locally
	# Example:
	# a = Solution()
	# b = a.Function(tree)
	# print(b)


	$1


if __name__ == '__main__':
	main()
endsnippet

snippet info_firenvim



# Date: `date +%d/%B/%Y\ -\ %A`
# Author: Virgilio Murillo Ochoa
# personal github: Virgilio-AI
# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
# contact: data_scientist@virgiliomurillo.com
# web: virgiliomurillo.com


# dic = defaultdict(int)
# set = set() # .remove(val),.add(val),.discard(val),.pop(),.clear()
# dic = {} # .remove(id),dic[key] = val, dic.get(key,0)
# arr = [] # .append(val),.pop(),.remove(val),.sort(),.reverse(),.insert(pos,val),.clear()


endsnippet

snippet info
# Date: `date +%d/%B/%Y\ -\ %A`
# Author: Virgilio Murillo Ochoa
# personal github: Virgilio-AI
# linkedin: https://www.linkedin.com/in/virgilio-murillo-ochoa-b29b59203
# contact: data_scientist@virgiliomurillo.com
# web: virgiliomurillo.com
endsnippet



# ==========================
# ========== python bases de datos======
# ==========================
# options im-> inword
snippet sqlite3 "for connection into a sqlite3 database"
conexion = sqlite3.connect('pruebas.db')
endsnippet
# options im-> inword
snippet sqlite3c "for executing a command in sqlite3"

cursor = conexion.cursor()
command = """
"""
# execute command

cursor.execute(command)
# if query
# rows = cursor.fetchall()
# for row in rows():
#	print(row)
endsnippet


# options im-> inword
snippet mysql "for creating a connection to sql"
import mysql.connector

# conexion

database =  mysql.connector.connect(
		host="${1:localhost}",
		user = "$2",
		passwd="$3",
		database="${4:automation_suite}",
		)

endsnippet
# options im-> inword
snippet mysqlc "for executing a command"
cursor = database.cursor(buffered,True)

cursor.execute(${1:command})
# cursor.execute("string for formatted %s",tuple of values)
endsnippet
# options im-> inword
snippet mysqlq "for a query in mysql"
cursor.execute("${1:select * from Event ;}")
${2:rows} = cursor.fetchall()
endsnippet
# options im-> inword
snippet rowcount "for counting the affected rows by the commit"
#database.commit()
num = cursor.rowcount()
endsnippet

# options im-> inword
snippet encryption "for encripting a password"
# import hashlib
cifrado = hashlib.sha256()
cifrado.update(${1:string}.encode('utf8'))
endsnippet




# ==========================
# ========== python django ======
# ==========================

# options im-> inword
snippet createProject "for creating a django project"
# django-admin startproject mysite

endsnippet
# options im-> inword
snippet migrate "for creating a migration with databases"
# python manage.py makemigrations
# python manage.py sqlmigrate ${1:appName} ${2:migration_number ex:0001}
# python manage.py migrate

endsnippet
# options im-> inword
snippet commonPackages "for a list of common package"
django-ckeditor
mysqlclient
pillow
endsnippet
# options im-> inword
snippet createApp "for creating a django app"
# type in the terminal
python manage.py startapp polls

# in the main file settings.py add the name of the app in installed apps
# if apps.py is modified use the name in default_auto_field instead
INSTALLED_APPS = [
'$1'
]
endsnippet


# options im-> inword
snippet useMysql "for using mysql as default in django"
DATABASES = {
		'default': {
			'ENGINE' : 'django.db.backends.mysql',
			'NAME' : "django_course_second_project",
			'USER' : 'test_django',
			'PASSWORD' : '' ,
			'HOST' : 'localhost',
			'PORT' : ''
			}
		}
endsnippet

# options im-> inword
snippet changeLanguage "for changing the default language for the app"
# in the project settings
# LANGUAGE_CODE = 'en-en'
# LANGUAGE_CODE = 'en-es'

endsnippet

# options im-> inword
snippet addContextProcessor "these are used for global funcitons inside the html files"
# inside the main folder add in the TEMPLATES=[
'${1:appName}.${2:nameOfContext}.${3:name_of_function}

# and example would be

# def get_pages(request):
# 
# 	temp = Page.objects.values_list('id','title','slug','visible').order_by('order')
# 	pages = []
# 	for i in range(len(temp)):
# 		if temp[i][3] == True:
# 			pages.append(temp[i])
# 
# 
# 	return {
# 			'pages' : pages
# 			}

endsnippet




# options im-> inword
snippet importUrl "for importing the url file from an other app into the main app"
# from django.contrib import admin
# from django.urls import path,include
# from django.conf import settings
# 
# urlpatterns = [
# 	path('',include('${1:appName}.urls')),
# ]
endsnippet


# options im-> inword
snippet viewsHelp "for help in the views file"

# # for rendering html and to redirect into an other page
# from django.shortcuts import render,redirect
# # for poping messages
# from django.contrib import messages
# # for creating a user fast and easy
# from django.contrib.auth.forms import UserCreationForm
# # for creating a register form
# from MainApp.forms import RegisterForm
# # for loggin in and logging out
# from django.contrib.auth import authenticate,login,logout

# to create a view just add
def name_of_pages(request):
param1 = "param1"
return render(request,'path/inside/templates/folder',
{
	'param1_title':param1,
	'param2_title':"param2"
})

# for python tag interpolation use {{ var|safe }}
endsnippet

# options im-> inword
snippet login_page "for creating a basic login page"

# check the log in snippet for html

#from django.shortcuts import render, redirect
#from django.contrib.auth.forms import UserCreationForm,AuthenticationForm
#
#from django.contrib.auth.models import User
#from django.contrib.auth import login,logout,authenticate
#
#from django.db import IntegrityError
#
#def loginuser(request):
#	dic = {}
#	dic['form'] = AuthenticationForm()
#	if request.method == 'GET':
#		return render(request,'todo/loginuser.html',dic)
#	else:
#		user = authenticate(request,username =request.POST['username'],password=request.POST['password'])
#		if user is None:
#			dic['error'] = "username and password did not match"
#		else:
#			login(request,user)
#			return redirect('home')
#
#		return render(request,'todo/loginuser.html',dic)


endsnippet


# options im-> inword
snippet log_out "for logging out from a page"

#from django.shortcuts import render, redirect
#from django.contrib.auth.forms import UserCreationForm,AuthenticationForm
#
#from django.contrib.auth.models import User
#from django.contrib.auth import login,logout,authenticate
#
#from django.db import IntegrityError

#def logoutuser(request):
#	if request.method == 'POST':
#		logout(request)
#		return redirect('home')

endsnippet

# options im-> inword
snippet appsHelp "for help on the MainAppFolder"
# from django.apps import AppConfig
# 
# 
# class MainappConfig(AppConfig):
# 	default_auto_field = 'django.db.models.BigAutoField'
# 	name = 'MainApp'
# then on the INSTALLED_APPS use the default_auto_field name
endsnippet

# options im-> inword
snippet urlsHelp "for help in importing directories"
# first create the file urls in the app

# import the views from the same directorie

# from . import views
# from django.urls import path
#
# urlpatterns = [
#		path('inicio/',views.index,name='index'),
#		]

endsnippet

# options im-> inword
snippet forms_help "for registering a user in the database"
# from django import forms
# 
# from django.core import validators
# 
# from django.contrib.auth.forms import UserCreationForm
# from django.contrib.auth.models import User
# 
# class RegisterForm(UserCreationForm):
# 	class Meta:
# 		model = User
# 		fields = ['username','email','first_name','last_name','password1','password2']
endsnippet

# options im-> inword
snippet html_template "for creating an html template"
# inside the MainAppFolder
# create the folder templates/layouts/
# create the file layout.html and use the snippet html_template
# create the file static/css for the right design in the page


# create the pages app and inside it create the context_processors.py file
inside it place 

# from pages.models import Page
# 
# def get_pages(request):
# 
# 	temp = Page.objects.values_list('id','title','slug','visible').order_by('order')
# 	pages = []
# 	for i in range(len(temp)):
# 		if temp[i][3] == True:
# 			pages.append(temp[i])
# 
# 
# 	return {
# 			'pages' : pages
# 			}

to get the pages glbally for the whole project inside html files
endsnippet

# options im-> inword
snippet register "for registering "

##signupuser Create your views here.
#def signupuser(request):
#	dic = {}
#	dic['form'] = UserCreationForm()
#	if request.method == 'GET':
#		return render(request,'todo/signupuser.html',dic)
#	else:
#		if request.POST['password1'] == request.POST['password2']:
#			try:
#				user = User.objects.create_user(request.POST['username'],password=request.POST['password1'])
#				user.save()
#				login(request,user)
#				return redirect('currenttodos')
#
#			except IntegrityError:
#				dic['error'] = "this user already exists in the database"
#				return render(request,'todo/signupuser.html',dic)
#		else:
#			dic['error'] = "Passwords didn't match"
#			return render(request,'todo/signupuser.html',dic)
#
# use the html snippet for registering
endsnippet

# options im-> inword
snippet getSqlObject "for getting an sql object"

# Create your views here.
from .models import ${1:the_table_class}

# inside a view
	${2:objectName} = $1.objects.get(${3:condition})
endsnippet

# options im-> inword
snippet django_createTable "for creating a table"

from ckeditor.fields import RichTextField
# import the user table
from django.contrib.auth.models import User

# # table named Category 
# class Category(models.Model):
# 	name = models.CharField(max_length=100,verbose_name="Nombre")
# 	description = models.CharField(max_length=255,verbose_name = "Description")
# 	created_at = models.DateTimeField(auto_now_add=True,verbose_name = "Descripción")
# 
# 	# the displayed information in django admin page
# 	class Meta:
# 		verbose_name = "Categoria"
# 		verbose_name_plural = "categorias"
# 	
# 	# the name to be printed
# 	def __str__(self):
# 		return self.name
# 
# class Article(models.Model):
# 	title = models.CharField(max_length=150,verbose_name="Título")
# 	content = RichTextField(verbose_name="Contenido")
# 	image = models.ImageField(default='null',verbose_name="Imagen",upload_to="articles")
# 	public = models.BooleanField(verbose_name="publicado?")
# 	# foreign key to create relationship many to one
# 	user = models.ForeignKey(User,editable=False,verbose_name="Usuario",on_delete=models.CASCADE)
# 	# many to many relationship between Category and Article
# 	categories = models.ManyToManyField(Category,verbose_name='Categorias',blank=True)
# 
# 	created_at = models.DateTimeField(auto_now_add=True,verbose_name="Creado el")
# 	updated_at = models.DateTimeField(auto_now=True,verbose_name="última actualización")
# 
# 
# 	class Meta:
# 		verbose_name = "Articulo"
# 		verbose_name_plural = "Artículos"
# 	
# 	def __str__(self):
# 		return self.title
endsnippet

# options im-> inword
snippet loginRequired "require a log in for a page"

from django.contrib.auth.decorators import login_required

@login_required(login_url="login")
# def page(request,slug):
# 	page = Page.objects.get(slug=slug)
# 	return render(request,"pages/page.html",{
# 		"page": page
# 		}
# )

endsnippet

# options im-> inword
snippet modClassDisplay "for modifications on the django panel search and many others"
# from .models import Category
# class CategoryAdmin(admin.ModelAdmin):
# 	readonly_fieds = ('created_at','updated_at')
# 	search_fields = ('name','description')
# 	list_display = ('name','created_at')
# class ArticleAdmin(admin.ModelAdmin):
# 	readonly_fieds = ('user','created_at','updated_at')
# 	search_fields = ('title','content','user__username','categories__name')
# 	list_display = ('title','user','public','created_at')
# 	list_filter = ('public','user__username','categories__name')

#

# 	def save_model(self,request,obj,form,change):
# 		if not obj.user_id:
# 			obj.user_id = request.user.id
# 		obj.save()

#

# admin.site.register(Category,CategoryAdmin)
# admin.site.register(Article,ArticleAdmin)



endsnippet



# ==========================
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ========== BasesDeDatos ======
# ==========================

snippet insertIfne
-- insert query
insert into ${1:tableName}(${2:name,actionDescription}) values(${3:"alert","gives a personalized voice alert for the system"})
SELECT ${4:row_to_compare}
FROM $1
WHERE NOT($4 IN (SELECT $4 FROM $1));
endsnippet







snippet describe
describe $1 ;
endsnippet







# ==========================
# ========== sql basic syntax ======
# ==========================
snippet compHelp
-- <,>,<=,>=,=,
-- != equal <>
-- and,or
endsnippet

snippet wildCards
-- % -> A substitude for 0 or more characters
-- _ -> a substitude for a single character
-- [charlist] -> sets and ranges for characters to match
endsnippet

snippet bjoin
select ${1:table.col,table.col}
from table
join ${2:second_table}
on employee.id == branch.id;
-- left join if you want to see all
endsnippet
# ==========================
# ========== queries ======
# ==========================

snippet showTables
show tables ;
endsnippet
snippet showUsers
SELECT User FROM mysql.user ;
endsnippet

snippet showAll
select * from $1 ;
endsnippet

# options im-> inword
snippet showCurUser ""
select user() ;
endsnippet
# ==========================
# ========== databases ======
# ==========================

# options im-> inword
snippet createDatabase "for creating a database"
create database ${1:name_of_database} ;
endsnippet
# options im-> inword
snippet deleteDatabase "for deleting a database"
drop database ${1:is exists} ${2:name_of_database}
endsnippet
# options im-> inword
snippet grantAll "grant all privileges to user over database"
-- use it on the terminal as sudo
-- sudo mariadb
grant all privileges on ${1:database_name}.* to '${2:username}'@${3:localhost} ;
endsnippet
# options im-> inword
snippet showPrivileges "to show the privileges"
show grants for '${1:username}'@'${2:localhost} ;
endsnippet
# options im-> inword
snippet showColumn "for showing the specified column"
select ${1:list,of,columns}
from student
${2:[ order by (list of columns)]}
;
endsnippet

snippet columnConditions
-- select * 
-- from ${column}
-- where name in ('name1','name2')
-- limit 5;
endsnippet

snippet countVal
select count(${1:column})
from ${2:table}
-- [where conditions]
;
endsnippet

snippet average
select avg(${1:column})
from ${2:table}
-- [ where conditions]
-- [ group by column ]
endsnippet


# options im-> inword
snippet bunion "merge two queries into one"
-- select column
-- from employee
-- union
-- select branch_name
-- from branch
endsnippet
# options im-> inword
snippet helpUsers "for getting help on the users"
# the user that is been used when login normally is the '' user
endsnippet
# ==========================
# ========== table modifications ======
# ==========================

snippet foreign
foreign key (${1:name_of_key_in_this_table}) references ${2:foreign_table}(${3:value_in_foreing_table})
endsnippet

snippet createTable

create table if not exists ${1:table_name}(
	$2
);
endsnippet
# options im-> inword
snippet helpTable "for help on the table creating and deletion"

-- data types -> varchar(n),Int,DECIMAL(M,N),BLOB,DATE,TIMESTAMP,float
-- float is variable and decimal is fixed
-- use float always
-- constrains -> not null,unique,auto_increment,default 'val'
-- on delete -> on delete set null, on delete cascade
-- snippets -> foreign,

-- EXAMPLE
-- student_id INT,
-- name VARCHAR(20) not null,
-- major VARCHAR(20) unique,
-- primary key(student_id)
-- foreign key(student_id) references branch(branch_id) on delete cascade
endsnippet

snippet deltable
drop table ${1:table_name}
endsnippet

snippet addRow
alter table ${1:name_of_table} add ${2:name varchar(100)}
endsnippet

snippet modTable
-- alter table name_of_table
-- add foreign key(branch_id)
-- references branch(branch_id)
-- on delete set null;
endsnippet
# ==========================
# ========== data modification ======
# ==========================

snippet insert
insert into ${1:table_name}(${2:info,action,name,weeks,action_time,hour,minute}) values(${3:"test info","test action","test name",1,10,12,30}) ;
endsnippet

# options im-> inword
snippet update "update the data" 
-- see compHelp
update ${1:table_name}
set ${2:column_name} = ${3:value}
-- you can separate this by commas
-- ex: name='tom',name='jon'
where ${4:condition} ;
endsnippet

snippet deleteRow
-- to delete a row from the table
delete from ${1:name_of_table} where ${2:condition} ;
endsnippet
# ==========================
# ========== triggers ======
# ==========================

snippet trigger
create table ${1:trigger test} (
	message varchar(100)
	);
endsnippet


